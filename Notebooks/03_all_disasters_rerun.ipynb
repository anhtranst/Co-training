{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LG-CoTrain: All Disasters Re-Run\n",
    "\n",
    "This notebook re-runs the **LG-CoTrain** co-training pipeline across **all 10 disaster\n",
    "events** with a **configurable pseudo-label source** and stores results in a\n",
    "**named sub-folder** under `results/`.\n",
    "\n",
    "### Why a separate notebook?\n",
    "\n",
    "We may run experiments multiple times with different pseudo-label sets (e.g.,\n",
    "gpt-4o vs llama-3, or multiple runs of the same model). Each run is stored in\n",
    "its own sub-folder so results are never overwritten.\n",
    "\n",
    "### Configuration (Cell 2)\n",
    "\n",
    "Edit the following variables in the **Configuration** cell before running:\n",
    "\n",
    "| Variable | Description |\n",
    "|---|---|\n",
    "| `PSEUDO_LABEL_SOURCE` | Name of the pseudo-label directory under `data/pseudo-labelled/` |\n",
    "| `RUN_NAME` | Sub-folder name under `results/` for this run |\n",
    "\n",
    "### Resume Support\n",
    "\n",
    "Same two-level resume as notebook 02:\n",
    "\n",
    "1. **Event level**: Events with all 12 `metrics.json` files are skipped entirely.\n",
    "2. **Experiment level**: Individual `(budget, seed_set)` combinations with existing\n",
    "   results are skipped within each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import statistics\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_repo_root(marker: str = \"lg_cotrain\") -> Path:\n",
    "    for candidate in [Path().resolve()] + list(Path().resolve().parents):\n",
    "        if (candidate / marker).is_dir():\n",
    "            return candidate\n",
    "    raise RuntimeError(\n",
    "        f\"Cannot find repo root: no ancestor directory contains '{marker}/'. \"\n",
    "        \"Run the notebook from inside the repository.\"\n",
    "    )\n",
    "\n",
    "repo_root = _find_repo_root()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lg_cotrain.run_all import BUDGETS, SEED_SETS, run_all_experiments, format_summary_table\n",
    "\n",
    "print(f\"Repo root: {repo_root}\")\n",
    "print(f\"Budgets: {BUDGETS}\")\n",
    "print(f\"Seed sets: {SEED_SETS}\")\n",
    "print(f\"Experiments per event: {len(BUDGETS) * len(SEED_SETS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- User-editable configuration ----\n",
    "PSEUDO_LABEL_SOURCE = \"gpt-4o\"      # Change to use different pseudo-labels (e.g. \"llama-3\")\n",
    "RUN_NAME = \"gpt-4o-run1\"            # Sub-folder name under results/\n",
    "\n",
    "DATA_ROOT = str(repo_root / \"data\")\n",
    "RESULTS_ROOT = str(repo_root / \"results\" / RUN_NAME)\n",
    "\n",
    "print(f\"Pseudo-label source: {PSEUDO_LABEL_SOURCE}\")\n",
    "print(f\"Run name: {RUN_NAME}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Results root: {RESULTS_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_event_complete(event, results_root):\n",
    "    \"\"\"Check if all 12 metrics.json files exist for an event.\"\"\"\n",
    "    for budget in BUDGETS:\n",
    "        for seed_set in SEED_SETS:\n",
    "            path = Path(results_root) / event / f\"{budget}_set{seed_set}\" / \"metrics.json\"\n",
    "            if not path.exists():\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# Discover events from data directory\n",
    "data_dir = Path(DATA_ROOT) / \"original\"\n",
    "all_events = sorted(p.name for p in data_dir.iterdir() if p.is_dir())\n",
    "\n",
    "completed_events = [e for e in all_events if is_event_complete(e, RESULTS_ROOT)]\n",
    "pending_events = [e for e in all_events if e not in completed_events]\n",
    "\n",
    "print(f\"Found {len(all_events)} events total\")\n",
    "print(f\"  Completed: {len(completed_events)} ({len(completed_events) * 12} experiments)\")\n",
    "print(f\"  Pending:   {len(pending_events)} (up to {len(pending_events) * 12} experiments)\")\n",
    "\n",
    "if completed_events:\n",
    "    print(f\"\\nCompleted events (will be skipped):\")\n",
    "    for e in completed_events:\n",
    "        print(f\"  - {e}\")\n",
    "\n",
    "if pending_events:\n",
    "    print(f\"\\nPending events (will be run):\")\n",
    "    for e in pending_events:\n",
    "        print(f\"  - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Experiments\n",
    "\n",
    "For each pending event, we call `run_all_experiments` with the configured\n",
    "`pseudo_label_source` and `results_root` pointing to our named sub-folder.\n",
    "\n",
    "Individual experiments that already have `metrics.json` are automatically\n",
    "skipped (useful if the notebook crashed mid-event)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressTracker:\n",
    "    \"\"\"Track global progress across all experiments.\"\"\"\n",
    "\n",
    "    def __init__(self, total, already_done, start_time):\n",
    "        self.total = total\n",
    "        self.done = already_done\n",
    "        self.start_time = start_time\n",
    "\n",
    "    def update(self, event, budget, seed_set, status):\n",
    "        self.done += 1\n",
    "        elapsed = time.time() - self.start_time\n",
    "        pct = 100.0 * self.done / self.total\n",
    "        elapsed_h = elapsed / 3600\n",
    "\n",
    "        remaining = self.total - self.done\n",
    "        if elapsed > 0 and self.done > 0:\n",
    "            eta_h = (elapsed / self.done) * remaining / 3600\n",
    "        else:\n",
    "            eta_h = 0\n",
    "\n",
    "        print(\n",
    "            f\"[PROGRESS] {self.done}/{self.total} ({pct:.1f}%)\"\n",
    "            f\" | Elapsed: {elapsed_h:.2f}h | ETA: {eta_h:.2f}h\"\n",
    "        )\n",
    "\n",
    "# Count already-completed experiments (from previous runs)\n",
    "already_done = sum(\n",
    "    1\n",
    "    for e in all_events\n",
    "    for b in BUDGETS\n",
    "    for s in SEED_SETS\n",
    "    if (Path(RESULTS_ROOT) / e / f\"{b}_set{s}\" / \"metrics.json\").exists()\n",
    ")\n",
    "total_experiments = len(all_events) * len(BUDGETS) * len(SEED_SETS)\n",
    "\n",
    "print(f\"Total experiments: {total_experiments}\")\n",
    "print(f\"Already completed: {already_done}\")\n",
    "print(f\"Remaining: {total_experiments - already_done}\")\n",
    "\n",
    "all_event_results = {}\n",
    "overall_start = time.time()\n",
    "tracker = ProgressTracker(total_experiments, already_done, overall_start)\n",
    "\n",
    "# Run pending events\n",
    "for i, event in enumerate(pending_events, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Event {i}/{len(pending_events)}: {event}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    results = run_all_experiments(\n",
    "        event,\n",
    "        pseudo_label_source=PSEUDO_LABEL_SOURCE,\n",
    "        data_root=DATA_ROOT,\n",
    "        results_root=RESULTS_ROOT,\n",
    "        _on_experiment_done=tracker.update,\n",
    "    )\n",
    "    all_event_results[event] = results\n",
    "\n",
    "    print()\n",
    "    print(format_summary_table(results, event))\n",
    "\n",
    "# Load results for already-completed events\n",
    "for event in completed_events:\n",
    "    results = []\n",
    "    for budget in BUDGETS:\n",
    "        for seed_set in SEED_SETS:\n",
    "            path = Path(RESULTS_ROOT) / event / f\"{budget}_set{seed_set}\" / \"metrics.json\"\n",
    "            with open(path) as f:\n",
    "                results.append(json.load(f))\n",
    "    all_event_results[event] = results\n",
    "\n",
    "overall_elapsed = time.time() - overall_start\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"All events done in {overall_elapsed / 3600:.2f}h\")\n",
    "print(f\"Total events with results: {len(all_event_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Disaster Results\n",
    "\n",
    "We now aggregate results across all events to compare how the pipeline\n",
    "performs on different disaster types and how performance scales with the\n",
    "labeled data budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cross-disaster summary: event -> budget -> mean macro-F1\n",
    "summary = {}\n",
    "for event in sorted(all_event_results.keys()):\n",
    "    results = all_event_results[event]\n",
    "    by_budget = {b: [] for b in BUDGETS}\n",
    "    for r in results:\n",
    "        if r is not None:\n",
    "            by_budget[r[\"budget\"]].append(r)\n",
    "    summary[event] = {}\n",
    "    for b in BUDGETS:\n",
    "        f1s = [r[\"test_macro_f1\"] for r in by_budget[b]]\n",
    "        errs = [r[\"test_error_rate\"] for r in by_budget[b]]\n",
    "        summary[event][b] = {\n",
    "            \"f1_mean\": statistics.mean(f1s) if f1s else None,\n",
    "            \"f1_std\": statistics.stdev(f1s) if len(f1s) >= 2 else None,\n",
    "            \"err_mean\": statistics.mean(errs) if errs else None,\n",
    "            \"err_std\": statistics.stdev(errs) if len(errs) >= 2 else None,\n",
    "            \"n_seeds\": len(f1s),\n",
    "        }\n",
    "\n",
    "# Print grand summary table\n",
    "header = f\"{'Event':<35}\"\n",
    "for b in BUDGETS:\n",
    "    header += f\" | B={b:<11}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for event in sorted(summary.keys()):\n",
    "    row = f\"{event:<35}\"\n",
    "    for b in BUDGETS:\n",
    "        s = summary[event][b]\n",
    "        if s[\"f1_mean\"] is not None and s[\"f1_std\"] is not None:\n",
    "            row += f\" | {s['f1_mean']:.3f}+/-{s['f1_std']:.3f}\"\n",
    "        elif s[\"f1_mean\"] is not None:\n",
    "            row += f\" | {s['f1_mean']:.3f}      \"\n",
    "        else:\n",
    "            row += f\" | {'N/A':<11}\"\n",
    "    print(row)\n",
    "\n",
    "# Line plot: Macro-F1 by budget, one line per event\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for event in sorted(summary.keys()):\n",
    "    means = [summary[event][b][\"f1_mean\"] or 0 for b in BUDGETS]\n",
    "    stds = [summary[event][b][\"f1_std\"] or 0 for b in BUDGETS]\n",
    "    ax.errorbar(BUDGETS, means, yerr=stds, marker=\"o\", capsize=3, label=event)\n",
    "\n",
    "ax.set_xlabel(\"Budget (labeled samples per class)\")\n",
    "ax.set_ylabel(\"Test Macro-F1 (mean +/- std across seeds)\")\n",
    "ax.set_title(f\"LG-CoTrain Performance — {PSEUDO_LABEL_SOURCE}\")\n",
    "ax.set_xticks(BUDGETS)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap: events (rows) x budgets (columns), colored by mean macro-F1\n",
    "events_sorted = sorted(summary.keys())\n",
    "heatmap_data = np.zeros((len(events_sorted), len(BUDGETS)))\n",
    "\n",
    "for i, event in enumerate(events_sorted):\n",
    "    for j, b in enumerate(BUDGETS):\n",
    "        val = summary[event][b][\"f1_mean\"]\n",
    "        heatmap_data[i, j] = val if val is not None else 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "im = ax.imshow(heatmap_data, cmap=\"YlOrRd\", aspect=\"auto\")\n",
    "\n",
    "ax.set_xticks(range(len(BUDGETS)))\n",
    "ax.set_xticklabels([f\"B={b}\" for b in BUDGETS])\n",
    "ax.set_yticks(range(len(events_sorted)))\n",
    "ax.set_yticklabels(events_sorted, fontsize=9)\n",
    "ax.set_title(f\"Mean Test Macro-F1 — {PSEUDO_LABEL_SOURCE}\")\n",
    "\n",
    "for i in range(len(events_sorted)):\n",
    "    for j in range(len(BUDGETS)):\n",
    "        val = heatmap_data[i, j]\n",
    "        color = \"white\" if val > 0.6 else \"black\"\n",
    "        ax.text(j, i, f\"{val:.3f}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "\n",
    "fig.colorbar(im, ax=ax, label=\"Macro-F1\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lg_cotrain.dashboard import collect_all_metrics, generate_html\n",
    "\n",
    "metrics = collect_all_metrics(RESULTS_ROOT)\n",
    "html = generate_html(metrics, RESULTS_ROOT)\n",
    "dashboard_path = Path(RESULTS_ROOT) / \"dashboard.html\"\n",
    "dashboard_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "dashboard_path.write_text(html)\n",
    "print(f\"Dashboard written to: {dashboard_path}\")\n",
    "print(f\"Metrics loaded: {len(metrics)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook ran experiments for all disaster events using:\n",
    "- **Pseudo-label source**: configured via `PSEUDO_LABEL_SOURCE`\n",
    "- **Results folder**: `results/{RUN_NAME}/`\n",
    "\n",
    "Results are stored separately from previous runs, enabling side-by-side\n",
    "comparison via the multi-tab dashboard.\n",
    "\n",
    "### CLI equivalent\n",
    "```bash\n",
    "# Run all experiments for all events with custom pseudo-label source and output folder\n",
    "python -m lg_cotrain.run_experiment \\\n",
    "    --events california_wildfires_2018 canada_wildfires_2016 cyclone_idai_2019 \\\n",
    "            hurricane_dorian_2019 hurricane_florence_2018 hurricane_harvey_2017 \\\n",
    "            hurricane_irma_2017 hurricane_maria_2017 kaikoura_earthquake_2016 \\\n",
    "            kerala_floods_2018 \\\n",
    "    --pseudo-label-source gpt-4o \\\n",
    "    --output-folder results/gpt-4o-run1\n",
    "```\n",
    "\n",
    "### Generate multi-tab dashboard from the CLI\n",
    "```bash\n",
    "python -m lg_cotrain.dashboard --results-root results/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
