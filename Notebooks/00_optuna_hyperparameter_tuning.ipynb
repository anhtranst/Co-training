{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LG-CoTrain: Global Optuna Hyperparameter Tuning\n",
    "\n",
    "This notebook finds the **optimal hyperparameters** for the LG-CoTrain pipeline\n",
    "using a single global [Optuna](https://optuna.org/) study. Each trial runs the\n",
    "full 3-phase pipeline across all 10 disaster events (budget=50, seed=1) and\n",
    "optimizes the **mean dev macro-F1**.\n",
    "\n",
    "### Why global tuning?\n",
    "\n",
    "- **No test-set leakage**: The objective uses `dev_macro_f1`, not test F1.\n",
    "- **Generalizable**: One set of hyperparameters that works across all events,\n",
    "  rather than overfitting to a single event.\n",
    "- **Efficient**: Optuna's TPE sampler + MedianPruner skips unpromising trials early.\n",
    "\n",
    "### Search space\n",
    "\n",
    "| Parameter | Range | Scale |\n",
    "|-----------|-------|-------|\n",
    "| `lr` | 1e-5 to 1e-3 | Log-uniform |\n",
    "| `batch_size` | [8, 16, 32, 64] | Categorical |\n",
    "| `cotrain_epochs` | 5 to 20 | Uniform integer |\n",
    "| `finetune_patience` | 4 to 10 | Uniform integer |\n",
    "\n",
    "### Paper deviation note\n",
    "\n",
    "The original paper (Cornelia et al. 2025) uses **fixed** hyperparameters:\n",
    "lr=2e-5, batch_size=32, cotrain_epochs=10, patience=5. This notebook explores\n",
    "whether tuning these improves performance.\n",
    "\n",
    "### Usage\n",
    "\n",
    "1. Run cells 1-3 to configure and launch the Optuna study\n",
    "2. After the study completes, results are saved as a JSON file\n",
    "3. Cells 4-7 visualize and analyze the results\n",
    "4. Cell 8 shows the CLI command to apply the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: D:\\Workspace\\Co-Training\n",
      "Optuna version: 4.7.0\n",
      "Events (10): ['california_wildfires_2018', 'canada_wildfires_2016', 'cyclone_idai_2019', 'hurricane_dorian_2019', 'hurricane_florence_2018', 'hurricane_harvey_2017', 'hurricane_irma_2017', 'hurricane_maria_2017', 'kaikoura_earthquake_2016', 'kerala_floods_2018']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _find_repo_root(marker: str = \"lg_cotrain\") -> Path:\n",
    "    for candidate in [Path().resolve()] + list(Path().resolve().parents):\n",
    "        if (candidate / marker).is_dir():\n",
    "            return candidate\n",
    "    raise RuntimeError(\n",
    "        f\"Cannot find repo root: no ancestor directory contains '{marker}/'. \"\n",
    "        \"Run the notebook from inside the repository.\"\n",
    "    )\n",
    "\n",
    "\n",
    "repo_root = _find_repo_root()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "import optuna\n",
    "\n",
    "import lg_cotrain.optuna_tuner\n",
    "importlib.reload(lg_cotrain.optuna_tuner)\n",
    "from lg_cotrain.optuna_tuner import ALL_EVENTS, create_objective, run_study\n",
    "\n",
    "print(f\"Repo root: {repo_root}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"Events ({len(ALL_EVENTS)}): {ALL_EVENTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set the number of trials, budget, and seed for the tuning study.\n",
    "\n",
    "- **`N_TRIALS`**: More trials = better hyperparameters, but each trial runs\n",
    "  the full pipeline across all 10 events. Start with 10-20 for a quick scan,\n",
    "  use 50+ for thorough tuning.\n",
    "- **`TUNING_BUDGET`**: Budget level used during tuning (50 = most labeled data).\n",
    "- **`TUNING_SEED`**: Seed set used during tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study name   : lg_cotrain_global\n",
      "Trials       : 20\n",
      "Events/trial : 10\n",
      "Total runs   : 200 (upper bound, pruning may reduce this)\n",
      "Budget       : 50\n",
      "Seed set     : 1\n",
      "Results root : D:\\Workspace\\Co-Training\\results\\optuna\n",
      "\n",
      "Search space:\n",
      "  lr               : 1e-5 to 1e-3  (log-uniform)\n",
      "  batch_size       : [8, 16, 32, 64]\n",
      "  cotrain_epochs   : 5 to 20\n",
      "  finetune_patience: 4 to 10\n"
     ]
    }
   ],
   "source": [
    "# ---- Tuning Configuration ----\n",
    "\n",
    "N_TRIALS      = 20        # Number of Optuna trials\n",
    "TUNING_BUDGET = 50        # Budget level for tuning\n",
    "TUNING_SEED   = 1         # Seed set for tuning\n",
    "STUDY_NAME    = \"lg_cotrain_global\"  # Optuna study name\n",
    "\n",
    "DATA_ROOT    = str(repo_root / \"data\")\n",
    "RESULTS_ROOT = str(repo_root / \"results\" / \"optuna\")\n",
    "\n",
    "# Optionally restrict to a subset of events for faster iteration\n",
    "# Set to None to use all 10 events\n",
    "EVENTS = None  # or e.g. [\"hurricane_harvey_2017\", \"kerala_floods_2018\"]\n",
    "\n",
    "events_to_use = EVENTS or ALL_EVENTS\n",
    "runs_per_trial = len(events_to_use)\n",
    "total_runs = N_TRIALS * runs_per_trial\n",
    "\n",
    "print(f\"Study name   : {STUDY_NAME}\")\n",
    "print(f\"Trials       : {N_TRIALS}\")\n",
    "print(f\"Events/trial : {runs_per_trial}\")\n",
    "print(f\"Total runs   : {total_runs} (upper bound, pruning may reduce this)\")\n",
    "print(f\"Budget       : {TUNING_BUDGET}\")\n",
    "print(f\"Seed set     : {TUNING_SEED}\")\n",
    "print(f\"Results root : {RESULTS_ROOT}\")\n",
    "print()\n",
    "print(\"Search space:\")\n",
    "print(\"  lr               : 1e-5 to 1e-3  (log-uniform)\")\n",
    "print(\"  batch_size       : [8, 16, 32, 64]\")\n",
    "print(\"  cotrain_epochs   : 5 to 20\")\n",
    "print(\"  finetune_patience: 4 to 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run the Optuna Study\n",
    "\n",
    "This cell launches the study. Each trial:\n",
    "\n",
    "1. Optuna's TPE sampler picks a set of hyperparameters\n",
    "2. The full 3-phase pipeline runs for each event\n",
    "3. After each event, the running mean dev F1 is reported to the pruner\n",
    "4. If the trial looks unpromising (below median), it's pruned early\n",
    "5. The final objective = mean dev macro-F1 across all events\n",
    "\n",
    "**Progress tracking**: After each event completes, you'll see the current trial,\n",
    "event, dev F1, elapsed time, and estimated time remaining.\n",
    "\n",
    "**This will take a long time** (hours to days depending on GPU and N_TRIALS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-20 18:37:54,594]\u001b[0m A new study created in memory with name: lg_cotrain_global\u001b[0m\n",
      "2026-02-20 18:37:59,381 - lg_cotrain - INFO - Starting LG-CoTrain: event=california_wildfires_2018, budget=50, seed_set=1\n",
      "2026-02-20 18:37:59,410 - lg_cotrain - INFO - Detected 10 classes for event california_wildfires_2018: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'missing_or_found_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 18:37:59,539 - lg_cotrain - INFO - D_l1: 250, D_l2: 250, D_LG: 4663\n",
      "2026-02-20 18:37:59,545 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1051.21it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1075.41it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 18:38:19,012 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1061, mean_prob2=0.0956\n",
      "2026-02-20 18:38:37,154 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1051, mean_prob2=0.1265\n",
      "2026-02-20 18:38:55,439 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1166, mean_prob2=0.1455\n",
      "2026-02-20 18:39:13,730 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1479, mean_prob2=0.2016\n",
      "2026-02-20 18:39:32,342 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1830, mean_prob2=0.2590\n",
      "2026-02-20 18:39:50,770 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.2297, mean_prob2=0.3219\n",
      "2026-02-20 18:40:09,121 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.2970, mean_prob2=0.3748\n",
      "2026-02-20 18:40:09,122 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.2455, range=[0.0503, 0.5028]\n",
      "2026-02-20 18:40:09,123 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1046, range=[0.0000, 0.2325]\n",
      "2026-02-20 18:40:09,124 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1014.62it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1104.66it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 18:41:14,823 - lg_cotrain - INFO - Phase 2 epoch 1/12: loss1=0.0918, loss2=0.2180, dev_macro_f1=0.5169, dev_err=29.52%\n",
      "2026-02-20 18:42:19,504 - lg_cotrain - INFO - Phase 2 epoch 2/12: loss1=0.0667, loss2=0.1919, dev_macro_f1=0.5920, dev_err=26.06%\n",
      "2026-02-20 18:43:23,943 - lg_cotrain - INFO - Phase 2 epoch 3/12: loss1=0.0373, loss2=0.1224, dev_macro_f1=0.6000, dev_err=26.60%\n",
      "2026-02-20 18:44:28,393 - lg_cotrain - INFO - Phase 2 epoch 4/12: loss1=0.0272, loss2=0.0830, dev_macro_f1=0.5985, dev_err=27.26%\n",
      "2026-02-20 18:45:32,802 - lg_cotrain - INFO - Phase 2 epoch 5/12: loss1=0.0272, loss2=0.0662, dev_macro_f1=0.6126, dev_err=26.06%\n",
      "2026-02-20 18:46:37,288 - lg_cotrain - INFO - Phase 2 epoch 6/12: loss1=0.0207, loss2=0.0385, dev_macro_f1=0.6221, dev_err=25.53%\n",
      "2026-02-20 18:47:41,743 - lg_cotrain - INFO - Phase 2 epoch 7/12: loss1=0.0281, loss2=0.0433, dev_macro_f1=0.5899, dev_err=26.86%\n",
      "2026-02-20 18:48:46,133 - lg_cotrain - INFO - Phase 2 epoch 8/12: loss1=0.0301, loss2=0.0478, dev_macro_f1=0.5842, dev_err=27.53%\n",
      "2026-02-20 18:49:50,569 - lg_cotrain - INFO - Phase 2 epoch 9/12: loss1=0.0201, loss2=0.0473, dev_macro_f1=0.5855, dev_err=28.19%\n",
      "2026-02-20 18:50:55,810 - lg_cotrain - INFO - Phase 2 epoch 10/12: loss1=0.0192, loss2=0.0398, dev_macro_f1=0.5859, dev_err=27.53%\n",
      "2026-02-20 18:52:03,776 - lg_cotrain - INFO - Phase 2 epoch 11/12: loss1=0.0126, loss2=0.0410, dev_macro_f1=0.5824, dev_err=27.13%\n",
      "2026-02-20 18:53:08,297 - lg_cotrain - INFO - Phase 2 epoch 12/12: loss1=0.0122, loss2=0.0336, dev_macro_f1=0.6165, dev_err=26.20%\n",
      "2026-02-20 18:53:08,298 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 18:53:13,211 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.4851, dev_err=49.07%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 18:53:18,145 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.5825, dev_err=33.51%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 18:53:23,052 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6189, dev_err=30.72%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 18:53:27,969 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6329, dev_err=30.05%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 18:53:32,882 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.6149, dev_err=32.58%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 18:53:37,786 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.5974, dev_err=33.78%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 18:53:42,742 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6158, dev_err=32.45%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 18:53:47,705 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6185, dev_err=32.18%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 18:53:52,623 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6133, dev_err=32.18%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 18:53:57,517 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6073, dev_err=32.58%, es_counter1=6, es_counter2=6\n",
      "2026-02-20 18:54:02,490 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.6109, dev_err=32.31%, es_counter1=7, es_counter2=7\n",
      "2026-02-20 18:54:02,490 - lg_cotrain - INFO - Early stopping at epoch 11\n",
      "2026-02-20 18:54:02,500 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 18:54:09,774 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\california_wildfires_2018\\50_set1\\metrics.json\n",
      "2026-02-20 18:54:09,775 - lg_cotrain - INFO - Test error rate: 28.41%, Test macro-F1: 0.6429, Test ECE: 0.0720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1/20 | Event 1/10 (california_wildfires_2018) | dev_F1=0.6329 (mean=0.6329) | Elapsed: 0.27h | ETA: 53.91h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 18:54:10,337 - lg_cotrain - INFO - Starting LG-CoTrain: event=canada_wildfires_2016, budget=50, seed_set=1\n",
      "2026-02-20 18:54:10,350 - lg_cotrain - INFO - Detected 8 classes for event canada_wildfires_2016: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 18:54:10,356 - lg_cotrain - INFO - D_l1: 182, D_l2: 182, D_LG: 1205\n",
      "2026-02-20 18:54:10,357 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1145.55it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1148.52it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 18:54:17,417 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1241, mean_prob2=0.1528\n",
      "2026-02-20 18:54:23,260 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1922, mean_prob2=0.1651\n",
      "2026-02-20 18:54:29,135 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.2150, mean_prob2=0.1858\n",
      "2026-02-20 18:54:35,004 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.2888, mean_prob2=0.2364\n",
      "2026-02-20 18:54:40,895 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.3219, mean_prob2=0.2950\n",
      "2026-02-20 18:54:47,014 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.3746, mean_prob2=0.3382\n",
      "2026-02-20 18:54:53,055 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.4549, mean_prob2=0.4336\n",
      "2026-02-20 18:54:53,056 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.4028, range=[0.0709, 0.7677]\n",
      "2026-02-20 18:54:53,057 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1439, range=[0.0087, 0.2344]\n",
      "2026-02-20 18:54:53,057 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|████████████████| 199/199 [00:00<00:00, 998.21it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1132.85it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 18:55:10,850 - lg_cotrain - INFO - Phase 2 epoch 1/12: loss1=0.1931, loss2=0.4784, dev_macro_f1=0.3575, dev_err=33.33%\n",
      "2026-02-20 18:55:27,462 - lg_cotrain - INFO - Phase 2 epoch 2/12: loss1=0.1133, loss2=0.2090, dev_macro_f1=0.3766, dev_err=28.51%\n",
      "2026-02-20 18:55:44,088 - lg_cotrain - INFO - Phase 2 epoch 3/12: loss1=0.0493, loss2=0.1342, dev_macro_f1=0.4443, dev_err=24.56%\n",
      "2026-02-20 18:56:00,753 - lg_cotrain - INFO - Phase 2 epoch 4/12: loss1=0.0328, loss2=0.0971, dev_macro_f1=0.4450, dev_err=25.44%\n",
      "2026-02-20 18:56:17,423 - lg_cotrain - INFO - Phase 2 epoch 5/12: loss1=0.0232, loss2=0.0686, dev_macro_f1=0.5302, dev_err=24.56%\n",
      "2026-02-20 18:56:34,105 - lg_cotrain - INFO - Phase 2 epoch 6/12: loss1=0.0261, loss2=0.0601, dev_macro_f1=0.5139, dev_err=22.81%\n",
      "2026-02-20 18:56:50,772 - lg_cotrain - INFO - Phase 2 epoch 7/12: loss1=0.0207, loss2=0.0564, dev_macro_f1=0.4656, dev_err=25.88%\n",
      "2026-02-20 18:57:07,504 - lg_cotrain - INFO - Phase 2 epoch 8/12: loss1=0.0154, loss2=0.0445, dev_macro_f1=0.4738, dev_err=25.88%\n",
      "2026-02-20 18:57:24,143 - lg_cotrain - INFO - Phase 2 epoch 9/12: loss1=0.0143, loss2=0.0358, dev_macro_f1=0.5259, dev_err=23.25%\n",
      "2026-02-20 18:57:40,784 - lg_cotrain - INFO - Phase 2 epoch 10/12: loss1=0.0272, loss2=0.0319, dev_macro_f1=0.5428, dev_err=22.81%\n",
      "2026-02-20 18:57:57,597 - lg_cotrain - INFO - Phase 2 epoch 11/12: loss1=0.0169, loss2=0.0581, dev_macro_f1=0.5351, dev_err=24.12%\n",
      "2026-02-20 18:58:14,317 - lg_cotrain - INFO - Phase 2 epoch 12/12: loss1=0.0197, loss2=0.0477, dev_macro_f1=0.5556, dev_err=24.12%\n",
      "2026-02-20 18:58:14,317 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 18:58:16,870 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.5160, dev_err=30.26%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 18:58:19,417 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6105, dev_err=22.37%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 18:58:21,944 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.5798, dev_err=22.37%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 18:58:24,485 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6315, dev_err=19.74%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 18:58:27,031 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.6464, dev_err=17.98%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 18:58:29,582 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.7065, dev_err=19.30%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 18:58:32,131 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.7123, dev_err=19.30%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 18:58:34,658 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.7006, dev_err=20.18%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 18:58:37,187 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6990, dev_err=19.74%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 18:58:39,735 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6935, dev_err=20.18%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 18:58:42,369 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.7011, dev_err=19.74%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 18:58:44,965 - lg_cotrain - INFO - Phase 3 epoch 12: dev_macro_f1=0.7074, dev_err=19.30%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 18:58:47,528 - lg_cotrain - INFO - Phase 3 epoch 13: dev_macro_f1=0.7072, dev_err=19.30%, es_counter1=6, es_counter2=6\n",
      "2026-02-20 18:58:50,061 - lg_cotrain - INFO - Phase 3 epoch 14: dev_macro_f1=0.7072, dev_err=19.30%, es_counter1=7, es_counter2=7\n",
      "2026-02-20 18:58:50,062 - lg_cotrain - INFO - Early stopping at epoch 14\n",
      "2026-02-20 18:58:50,074 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 18:58:52,322 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\canada_wildfires_2016\\50_set1\\metrics.json\n",
      "2026-02-20 18:58:52,322 - lg_cotrain - INFO - Test error rate: 25.17%, Test macro-F1: 0.5990, Test ECE: 0.0846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1/20 | Event 2/10 (canada_wildfires_2016) | dev_F1=0.7123 (mean=0.6726) | Elapsed: 0.35h | ETA: 34.59h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 18:58:53,010 - lg_cotrain - INFO - Starting LG-CoTrain: event=cyclone_idai_2019, budget=50, seed_set=1\n",
      "2026-02-20 18:58:53,036 - lg_cotrain - INFO - Detected 10 classes for event cyclone_idai_2019: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'missing_or_found_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 18:58:53,043 - lg_cotrain - INFO - D_l1: 227, D_l2: 226, D_LG: 2300\n",
      "2026-02-20 18:58:53,045 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1052.53it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1130.07it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 18:59:04,738 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1061, mean_prob2=0.1277\n",
      "2026-02-20 18:59:14,914 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1125, mean_prob2=0.1305\n",
      "2026-02-20 18:59:25,262 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1221, mean_prob2=0.1309\n",
      "2026-02-20 18:59:35,406 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1480, mean_prob2=0.1567\n",
      "2026-02-20 18:59:45,756 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1600, mean_prob2=0.1809\n",
      "2026-02-20 18:59:55,885 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1893, mean_prob2=0.2433\n",
      "2026-02-20 19:00:06,108 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.2268, mean_prob2=0.2277\n",
      "2026-02-20 19:00:06,108 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.2040, range=[0.0587, 0.3994]\n",
      "2026-02-20 19:00:06,108 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1118, range=[0.0122, 0.3099]\n",
      "2026-02-20 19:00:06,114 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1090.90it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1201.17it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 19:00:39,380 - lg_cotrain - INFO - Phase 2 epoch 1/12: loss1=0.1318, loss2=0.2302, dev_macro_f1=0.3042, dev_err=29.43%\n",
      "2026-02-20 19:01:11,405 - lg_cotrain - INFO - Phase 2 epoch 2/12: loss1=0.0582, loss2=0.1733, dev_macro_f1=0.3432, dev_err=27.18%\n",
      "2026-02-20 19:01:43,453 - lg_cotrain - INFO - Phase 2 epoch 3/12: loss1=0.0370, loss2=0.1421, dev_macro_f1=0.3332, dev_err=27.68%\n",
      "2026-02-20 19:02:15,334 - lg_cotrain - INFO - Phase 2 epoch 4/12: loss1=0.0243, loss2=0.0884, dev_macro_f1=0.3236, dev_err=29.68%\n",
      "2026-02-20 19:02:47,354 - lg_cotrain - INFO - Phase 2 epoch 5/12: loss1=0.0178, loss2=0.0762, dev_macro_f1=0.3253, dev_err=29.43%\n",
      "2026-02-20 19:03:19,301 - lg_cotrain - INFO - Phase 2 epoch 6/12: loss1=0.0347, loss2=0.0506, dev_macro_f1=0.3521, dev_err=26.93%\n",
      "2026-02-20 19:03:51,247 - lg_cotrain - INFO - Phase 2 epoch 7/12: loss1=0.0209, loss2=0.0480, dev_macro_f1=0.3576, dev_err=27.43%\n",
      "2026-02-20 19:04:23,215 - lg_cotrain - INFO - Phase 2 epoch 8/12: loss1=0.0145, loss2=0.0367, dev_macro_f1=0.3479, dev_err=27.93%\n",
      "2026-02-20 19:04:55,324 - lg_cotrain - INFO - Phase 2 epoch 9/12: loss1=0.0301, loss2=0.0510, dev_macro_f1=0.3562, dev_err=28.93%\n",
      "2026-02-20 19:05:27,287 - lg_cotrain - INFO - Phase 2 epoch 10/12: loss1=0.0108, loss2=0.0380, dev_macro_f1=0.3485, dev_err=28.93%\n",
      "2026-02-20 19:05:59,178 - lg_cotrain - INFO - Phase 2 epoch 11/12: loss1=0.0078, loss2=0.0335, dev_macro_f1=0.3627, dev_err=27.68%\n",
      "2026-02-20 19:06:31,145 - lg_cotrain - INFO - Phase 2 epoch 12/12: loss1=0.0072, loss2=0.0117, dev_macro_f1=0.3503, dev_err=28.43%\n",
      "2026-02-20 19:06:31,146 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 19:06:34,730 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.3684, dev_err=56.86%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:06:38,353 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.5229, dev_err=29.18%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:06:41,932 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.5109, dev_err=31.92%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:06:45,508 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.4911, dev_err=31.67%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 19:06:49,090 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.4942, dev_err=34.66%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 19:06:52,681 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.5269, dev_err=28.93%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:06:56,343 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.5286, dev_err=30.17%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:06:59,934 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.5431, dev_err=27.93%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:07:03,519 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.5463, dev_err=27.93%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:07:07,094 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.5334, dev_err=29.68%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:07:10,663 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.5366, dev_err=28.93%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 19:07:14,241 - lg_cotrain - INFO - Phase 3 epoch 12: dev_macro_f1=0.5391, dev_err=28.68%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 19:07:17,820 - lg_cotrain - INFO - Phase 3 epoch 13: dev_macro_f1=0.5393, dev_err=28.18%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 19:07:21,404 - lg_cotrain - INFO - Phase 3 epoch 14: dev_macro_f1=0.5477, dev_err=27.43%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:07:24,977 - lg_cotrain - INFO - Phase 3 epoch 15: dev_macro_f1=0.5419, dev_err=26.68%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:07:28,568 - lg_cotrain - INFO - Phase 3 epoch 16: dev_macro_f1=0.5486, dev_err=26.68%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:07:32,167 - lg_cotrain - INFO - Phase 3 epoch 17: dev_macro_f1=0.5382, dev_err=27.93%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:07:35,742 - lg_cotrain - INFO - Phase 3 epoch 18: dev_macro_f1=0.5457, dev_err=27.43%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 19:07:39,317 - lg_cotrain - INFO - Phase 3 epoch 19: dev_macro_f1=0.5445, dev_err=27.68%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 19:07:42,898 - lg_cotrain - INFO - Phase 3 epoch 20: dev_macro_f1=0.5423, dev_err=27.68%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 19:07:46,511 - lg_cotrain - INFO - Phase 3 epoch 21: dev_macro_f1=0.5433, dev_err=27.68%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 19:07:50,082 - lg_cotrain - INFO - Phase 3 epoch 22: dev_macro_f1=0.5388, dev_err=28.18%, es_counter1=6, es_counter2=6\n",
      "2026-02-20 19:07:53,660 - lg_cotrain - INFO - Phase 3 epoch 23: dev_macro_f1=0.5422, dev_err=27.93%, es_counter1=7, es_counter2=7\n",
      "2026-02-20 19:07:53,661 - lg_cotrain - INFO - Early stopping at epoch 23\n",
      "2026-02-20 19:07:53,674 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 19:07:57,579 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\cyclone_idai_2019\\50_set1\\metrics.json\n",
      "2026-02-20 19:07:57,580 - lg_cotrain - INFO - Test error rate: 29.65%, Test macro-F1: 0.5823, Test ECE: 0.1130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1/20 | Event 3/10 (cyclone_idai_2019) | dev_F1=0.5486 (mean=0.6313) | Elapsed: 0.50h | ETA: 32.89h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 19:07:58,170 - lg_cotrain - INFO - Starting LG-CoTrain: event=hurricane_dorian_2019, budget=50, seed_set=1\n",
      "2026-02-20 19:07:58,205 - lg_cotrain - INFO - Detected 9 classes for event hurricane_dorian_2019: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 19:07:58,214 - lg_cotrain - INFO - D_l1: 221, D_l2: 221, D_LG: 4887\n",
      "2026-02-20 19:07:58,215 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1054.04it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1088.23it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 19:08:18,636 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1169, mean_prob2=0.1131\n",
      "2026-02-20 19:08:37,569 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1183, mean_prob2=0.1246\n",
      "2026-02-20 19:08:56,551 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1357, mean_prob2=0.1457\n",
      "2026-02-20 19:09:15,737 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1775, mean_prob2=0.1688\n",
      "2026-02-20 19:09:35,116 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.2161, mean_prob2=0.1896\n",
      "2026-02-20 19:09:54,038 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.2605, mean_prob2=0.2381\n",
      "2026-02-20 19:10:13,093 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.3262, mean_prob2=0.3072\n",
      "2026-02-20 19:10:13,095 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.2779, range=[0.0628, 0.6948]\n",
      "2026-02-20 19:10:13,095 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1054, range=[0.0050, 0.2556]\n",
      "2026-02-20 19:10:13,096 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1085.88it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|████████████████| 199/199 [00:00<00:00, 992.24it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 19:11:22,175 - lg_cotrain - INFO - Phase 2 epoch 1/12: loss1=0.0955, loss2=0.2130, dev_macro_f1=0.4708, dev_err=38.14%\n",
      "2026-02-20 19:12:30,015 - lg_cotrain - INFO - Phase 2 epoch 2/12: loss1=0.0577, loss2=0.2187, dev_macro_f1=0.4512, dev_err=40.72%\n",
      "2026-02-20 19:13:37,415 - lg_cotrain - INFO - Phase 2 epoch 3/12: loss1=0.0344, loss2=0.1557, dev_macro_f1=0.4764, dev_err=38.14%\n",
      "2026-02-20 19:14:45,268 - lg_cotrain - INFO - Phase 2 epoch 4/12: loss1=0.0237, loss2=0.0784, dev_macro_f1=0.4772, dev_err=39.05%\n",
      "2026-02-20 19:15:52,660 - lg_cotrain - INFO - Phase 2 epoch 5/12: loss1=0.0185, loss2=0.0640, dev_macro_f1=0.4677, dev_err=39.69%\n",
      "2026-02-20 19:17:01,033 - lg_cotrain - INFO - Phase 2 epoch 6/12: loss1=0.0148, loss2=0.0676, dev_macro_f1=0.4815, dev_err=38.40%\n",
      "2026-02-20 19:18:10,019 - lg_cotrain - INFO - Phase 2 epoch 7/12: loss1=0.0269, loss2=0.0248, dev_macro_f1=0.4948, dev_err=39.56%\n",
      "2026-02-20 19:19:18,479 - lg_cotrain - INFO - Phase 2 epoch 8/12: loss1=0.0234, loss2=0.0277, dev_macro_f1=0.4707, dev_err=40.08%\n",
      "2026-02-20 19:20:27,335 - lg_cotrain - INFO - Phase 2 epoch 9/12: loss1=0.0170, loss2=0.0291, dev_macro_f1=0.4898, dev_err=39.43%\n",
      "2026-02-20 19:21:35,818 - lg_cotrain - INFO - Phase 2 epoch 10/12: loss1=0.0177, loss2=0.0334, dev_macro_f1=0.4964, dev_err=38.92%\n",
      "2026-02-20 19:22:43,265 - lg_cotrain - INFO - Phase 2 epoch 11/12: loss1=0.0235, loss2=0.0425, dev_macro_f1=0.5167, dev_err=39.69%\n",
      "2026-02-20 19:23:50,687 - lg_cotrain - INFO - Phase 2 epoch 12/12: loss1=0.0179, loss2=0.0478, dev_macro_f1=0.4942, dev_err=39.43%\n",
      "2026-02-20 19:23:50,687 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 19:23:55,410 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.4635, dev_err=52.45%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:24:00,138 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.5912, dev_err=36.60%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:24:04,868 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6166, dev_err=35.44%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:24:09,581 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.5959, dev_err=37.24%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:24:14,292 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.5961, dev_err=37.50%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 19:24:18,999 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.5965, dev_err=37.11%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 19:24:23,710 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.5983, dev_err=36.73%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 19:24:28,441 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.5959, dev_err=36.98%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 19:24:33,150 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.5974, dev_err=36.60%, es_counter1=6, es_counter2=6\n",
      "2026-02-20 19:24:37,943 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6017, dev_err=36.21%, es_counter1=7, es_counter2=7\n",
      "2026-02-20 19:24:37,944 - lg_cotrain - INFO - Early stopping at epoch 10\n",
      "2026-02-20 19:24:37,957 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 19:24:45,490 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\hurricane_dorian_2019\\50_set1\\metrics.json\n",
      "2026-02-20 19:24:45,491 - lg_cotrain - INFO - Test error rate: 34.62%, Test macro-F1: 0.5995, Test ECE: 0.0566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1/20 | Event 4/10 (hurricane_dorian_2019) | dev_F1=0.6166 (mean=0.6276) | Elapsed: 0.78h | ETA: 38.26h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 19:24:46,069 - lg_cotrain - INFO - Starting LG-CoTrain: event=hurricane_florence_2018, budget=50, seed_set=1\n",
      "2026-02-20 19:24:46,111 - lg_cotrain - INFO - Detected 9 classes for event hurricane_florence_2018: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 19:24:46,120 - lg_cotrain - INFO - D_l1: 219, D_l2: 219, D_LG: 3946\n",
      "2026-02-20 19:24:46,122 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1134.76it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1034.25it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 19:25:02,928 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1090, mean_prob2=0.1196\n",
      "2026-02-20 19:25:18,657 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1249, mean_prob2=0.1429\n",
      "2026-02-20 19:25:34,277 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1472, mean_prob2=0.1615\n",
      "2026-02-20 19:25:50,032 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1789, mean_prob2=0.1785\n",
      "2026-02-20 19:26:05,697 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.2206, mean_prob2=0.2076\n",
      "2026-02-20 19:26:21,396 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.2774, mean_prob2=0.2532\n",
      "2026-02-20 19:26:37,114 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.3463, mean_prob2=0.2985\n",
      "2026-02-20 19:26:37,114 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.2908, range=[0.0785, 0.6262]\n",
      "2026-02-20 19:26:37,114 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1240, range=[0.0086, 0.3097]\n",
      "2026-02-20 19:26:37,118 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1085.99it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1006.03it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 19:27:33,896 - lg_cotrain - INFO - Phase 2 epoch 1/12: loss1=0.1219, loss2=0.2571, dev_macro_f1=0.6111, dev_err=25.04%\n",
      "2026-02-20 19:28:28,871 - lg_cotrain - INFO - Phase 2 epoch 2/12: loss1=0.0609, loss2=0.1933, dev_macro_f1=0.6360, dev_err=23.32%\n",
      "2026-02-20 19:29:24,082 - lg_cotrain - INFO - Phase 2 epoch 3/12: loss1=0.0337, loss2=0.1122, dev_macro_f1=0.6488, dev_err=22.85%\n",
      "2026-02-20 19:30:19,639 - lg_cotrain - INFO - Phase 2 epoch 4/12: loss1=0.0275, loss2=0.0996, dev_macro_f1=0.6487, dev_err=22.69%\n",
      "2026-02-20 19:31:15,644 - lg_cotrain - INFO - Phase 2 epoch 5/12: loss1=0.0186, loss2=0.0836, dev_macro_f1=0.6381, dev_err=24.73%\n",
      "2026-02-20 19:32:11,845 - lg_cotrain - INFO - Phase 2 epoch 6/12: loss1=0.0283, loss2=0.0690, dev_macro_f1=0.6267, dev_err=25.35%\n",
      "2026-02-20 19:33:08,260 - lg_cotrain - INFO - Phase 2 epoch 7/12: loss1=0.0277, loss2=0.0433, dev_macro_f1=0.6374, dev_err=24.41%\n",
      "2026-02-20 19:34:04,687 - lg_cotrain - INFO - Phase 2 epoch 8/12: loss1=0.0244, loss2=0.0361, dev_macro_f1=0.6395, dev_err=23.63%\n",
      "2026-02-20 19:35:00,750 - lg_cotrain - INFO - Phase 2 epoch 9/12: loss1=0.0133, loss2=0.0202, dev_macro_f1=0.6453, dev_err=22.85%\n",
      "2026-02-20 19:35:57,996 - lg_cotrain - INFO - Phase 2 epoch 10/12: loss1=0.0150, loss2=0.0317, dev_macro_f1=0.6509, dev_err=23.00%\n",
      "2026-02-20 19:36:56,925 - lg_cotrain - INFO - Phase 2 epoch 11/12: loss1=0.0190, loss2=0.0497, dev_macro_f1=0.6556, dev_err=21.91%\n",
      "2026-02-20 19:37:51,260 - lg_cotrain - INFO - Phase 2 epoch 12/12: loss1=0.0155, loss2=0.0241, dev_macro_f1=0.6416, dev_err=23.00%\n",
      "2026-02-20 19:37:51,261 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 19:37:55,505 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.6503, dev_err=27.07%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:37:59,727 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6346, dev_err=33.02%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:38:03,960 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6748, dev_err=21.75%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:38:08,182 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6723, dev_err=25.20%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:38:12,405 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.6709, dev_err=26.45%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 19:38:16,631 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.6728, dev_err=25.04%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 19:38:20,865 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6975, dev_err=24.10%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:38:25,083 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6932, dev_err=26.13%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:38:29,320 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.7024, dev_err=22.22%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:38:33,535 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6960, dev_err=22.69%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:38:37,770 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.6835, dev_err=26.29%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 19:38:41,982 - lg_cotrain - INFO - Phase 3 epoch 12: dev_macro_f1=0.6983, dev_err=23.16%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 19:38:46,210 - lg_cotrain - INFO - Phase 3 epoch 13: dev_macro_f1=0.7006, dev_err=22.85%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 19:38:50,449 - lg_cotrain - INFO - Phase 3 epoch 14: dev_macro_f1=0.7047, dev_err=24.10%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:38:54,672 - lg_cotrain - INFO - Phase 3 epoch 15: dev_macro_f1=0.6976, dev_err=26.13%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:38:58,907 - lg_cotrain - INFO - Phase 3 epoch 16: dev_macro_f1=0.7068, dev_err=24.10%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:39:03,144 - lg_cotrain - INFO - Phase 3 epoch 17: dev_macro_f1=0.7082, dev_err=23.63%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:39:07,390 - lg_cotrain - INFO - Phase 3 epoch 18: dev_macro_f1=0.7093, dev_err=23.32%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:39:11,614 - lg_cotrain - INFO - Phase 3 epoch 19: dev_macro_f1=0.7083, dev_err=23.47%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:39:15,835 - lg_cotrain - INFO - Phase 3 epoch 20: dev_macro_f1=0.7092, dev_err=23.32%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 19:39:20,050 - lg_cotrain - INFO - Phase 3 epoch 21: dev_macro_f1=0.7092, dev_err=23.32%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 19:39:24,274 - lg_cotrain - INFO - Phase 3 epoch 22: dev_macro_f1=0.7092, dev_err=23.32%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 19:39:28,507 - lg_cotrain - INFO - Phase 3 epoch 23: dev_macro_f1=0.7064, dev_err=23.63%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 19:39:32,724 - lg_cotrain - INFO - Phase 3 epoch 24: dev_macro_f1=0.7064, dev_err=23.63%, es_counter1=6, es_counter2=6\n",
      "2026-02-20 19:39:36,957 - lg_cotrain - INFO - Phase 3 epoch 25: dev_macro_f1=0.7064, dev_err=23.63%, es_counter1=7, es_counter2=7\n",
      "2026-02-20 19:39:36,957 - lg_cotrain - INFO - Early stopping at epoch 25\n",
      "2026-02-20 19:39:36,973 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 19:39:43,160 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\hurricane_florence_2018\\50_set1\\metrics.json\n",
      "2026-02-20 19:39:43,161 - lg_cotrain - INFO - Test error rate: 27.96%, Test macro-F1: 0.6691, Test ECE: 0.1257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1/20 | Event 5/10 (hurricane_florence_2018) | dev_F1=0.7093 (mean=0.6439) | Elapsed: 1.03h | ETA: 40.18h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 19:39:43,729 - lg_cotrain - INFO - Starting LG-CoTrain: event=hurricane_harvey_2017, budget=50, seed_set=1\n",
      "2026-02-20 19:39:43,773 - lg_cotrain - INFO - Detected 9 classes for event hurricane_harvey_2017: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 19:39:43,783 - lg_cotrain - INFO - D_l1: 225, D_l2: 225, D_LG: 5928\n",
      "2026-02-20 19:39:43,784 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1157.80it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1033.60it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 19:40:07,185 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1148, mean_prob2=0.1218\n",
      "2026-02-20 19:40:29,433 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1344, mean_prob2=0.1412\n",
      "2026-02-20 19:40:51,669 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1712, mean_prob2=0.1449\n",
      "2026-02-20 19:41:14,073 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1691, mean_prob2=0.1539\n",
      "2026-02-20 19:41:36,348 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1910, mean_prob2=0.1660\n",
      "2026-02-20 19:41:58,609 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.2240, mean_prob2=0.1911\n",
      "2026-02-20 19:42:20,905 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.2597, mean_prob2=0.2123\n",
      "2026-02-20 19:42:20,910 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.2407, range=[0.0514, 0.5313]\n",
      "2026-02-20 19:42:20,910 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1138, range=[0.0000, 0.3182]\n",
      "2026-02-20 19:42:20,910 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1056.76it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1033.60it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 19:43:43,035 - lg_cotrain - INFO - Phase 2 epoch 1/12: loss1=0.1028, loss2=0.2051, dev_macro_f1=0.6051, dev_err=35.41%\n",
      "2026-02-20 19:45:03,995 - lg_cotrain - INFO - Phase 2 epoch 2/12: loss1=0.0588, loss2=0.2058, dev_macro_f1=0.6209, dev_err=32.40%\n",
      "2026-02-20 19:46:24,998 - lg_cotrain - INFO - Phase 2 epoch 3/12: loss1=0.0427, loss2=0.1590, dev_macro_f1=0.6191, dev_err=33.48%\n",
      "2026-02-20 19:47:46,040 - lg_cotrain - INFO - Phase 2 epoch 4/12: loss1=0.0242, loss2=0.1028, dev_macro_f1=0.6116, dev_err=33.69%\n",
      "2026-02-20 19:49:07,008 - lg_cotrain - INFO - Phase 2 epoch 5/12: loss1=0.0356, loss2=0.0630, dev_macro_f1=0.5995, dev_err=34.98%\n",
      "2026-02-20 19:50:27,993 - lg_cotrain - INFO - Phase 2 epoch 6/12: loss1=0.0243, loss2=0.0752, dev_macro_f1=0.6087, dev_err=33.69%\n",
      "2026-02-20 19:51:49,066 - lg_cotrain - INFO - Phase 2 epoch 7/12: loss1=0.0254, loss2=0.0618, dev_macro_f1=0.6097, dev_err=34.34%\n",
      "2026-02-20 19:53:09,868 - lg_cotrain - INFO - Phase 2 epoch 8/12: loss1=0.0236, loss2=0.0427, dev_macro_f1=0.6072, dev_err=34.66%\n",
      "2026-02-20 19:54:30,651 - lg_cotrain - INFO - Phase 2 epoch 9/12: loss1=0.0194, loss2=0.0349, dev_macro_f1=0.6183, dev_err=33.15%\n",
      "2026-02-20 19:55:51,415 - lg_cotrain - INFO - Phase 2 epoch 10/12: loss1=0.0270, loss2=0.0302, dev_macro_f1=0.6115, dev_err=34.12%\n",
      "2026-02-20 19:57:12,218 - lg_cotrain - INFO - Phase 2 epoch 11/12: loss1=0.0238, loss2=0.0293, dev_macro_f1=0.6098, dev_err=33.69%\n",
      "2026-02-20 19:58:32,995 - lg_cotrain - INFO - Phase 2 epoch 12/12: loss1=0.0096, loss2=0.0224, dev_macro_f1=0.6051, dev_err=34.02%\n",
      "2026-02-20 19:58:32,996 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 19:58:38,263 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.6064, dev_err=44.13%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:58:43,530 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6520, dev_err=32.08%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:58:48,788 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6388, dev_err=31.11%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:58:54,039 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6247, dev_err=32.51%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 19:58:59,307 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.6690, dev_err=30.25%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:59:04,580 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.6742, dev_err=30.25%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 19:59:09,854 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6730, dev_err=29.82%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 19:59:15,108 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6682, dev_err=30.03%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 19:59:20,371 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6662, dev_err=30.36%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 19:59:25,631 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6676, dev_err=30.14%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 19:59:30,892 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.6692, dev_err=29.71%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 19:59:36,147 - lg_cotrain - INFO - Phase 3 epoch 12: dev_macro_f1=0.6690, dev_err=29.60%, es_counter1=6, es_counter2=6\n",
      "2026-02-20 19:59:41,401 - lg_cotrain - INFO - Phase 3 epoch 13: dev_macro_f1=0.6687, dev_err=29.60%, es_counter1=7, es_counter2=7\n",
      "2026-02-20 19:59:41,402 - lg_cotrain - INFO - Early stopping at epoch 13\n",
      "2026-02-20 19:59:41,412 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 19:59:50,346 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\hurricane_harvey_2017\\50_set1\\metrics.json\n",
      "2026-02-20 19:59:50,347 - lg_cotrain - INFO - Test error rate: 29.25%, Test macro-F1: 0.6704, Test ECE: 0.1070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1/20 | Event 6/10 (hurricane_harvey_2017) | dev_F1=0.6742 (mean=0.6490) | Elapsed: 1.37h | ETA: 44.15h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 19:59:50,995 - lg_cotrain - INFO - Starting LG-CoTrain: event=hurricane_irma_2017, budget=50, seed_set=1\n",
      "2026-02-20 19:59:51,068 - lg_cotrain - INFO - Detected 9 classes for event hurricane_irma_2017: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 19:59:51,079 - lg_cotrain - INFO - D_l1: 225, D_l2: 225, D_LG: 6129\n",
      "2026-02-20 19:59:51,081 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1087.62it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1117.18it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 20:00:15,159 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1126, mean_prob2=0.1192\n",
      "2026-02-20 20:00:38,007 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1284, mean_prob2=0.1356\n",
      "2026-02-20 20:01:00,910 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1436, mean_prob2=0.1468\n",
      "2026-02-20 20:01:23,801 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1711, mean_prob2=0.1598\n",
      "2026-02-20 20:01:46,671 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1930, mean_prob2=0.1533\n",
      "2026-02-20 20:02:09,541 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.2182, mean_prob2=0.2006\n",
      "2026-02-20 20:02:32,416 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.2433, mean_prob2=0.2205\n",
      "2026-02-20 20:02:32,427 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.2287, range=[0.0465, 0.5818]\n",
      "2026-02-20 20:02:32,427 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1073, range=[0.0000, 0.3053]\n",
      "2026-02-20 20:02:32,428 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1016.33it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1042.08it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 20:03:57,049 - lg_cotrain - INFO - Phase 2 epoch 1/12: loss1=0.0964, loss2=0.2016, dev_macro_f1=0.5893, dev_err=37.27%\n",
      "2026-02-20 20:05:20,536 - lg_cotrain - INFO - Phase 2 epoch 2/12: loss1=0.0499, loss2=0.2200, dev_macro_f1=0.6092, dev_err=36.01%\n",
      "2026-02-20 20:06:43,993 - lg_cotrain - INFO - Phase 2 epoch 3/12: loss1=0.0338, loss2=0.1710, dev_macro_f1=0.5877, dev_err=39.35%\n",
      "2026-02-20 20:08:07,454 - lg_cotrain - INFO - Phase 2 epoch 4/12: loss1=0.0419, loss2=0.1102, dev_macro_f1=0.6053, dev_err=37.16%\n",
      "2026-02-20 20:09:30,933 - lg_cotrain - INFO - Phase 2 epoch 5/12: loss1=0.0177, loss2=0.0596, dev_macro_f1=0.6165, dev_err=36.12%\n",
      "2026-02-20 20:10:54,401 - lg_cotrain - INFO - Phase 2 epoch 6/12: loss1=0.0159, loss2=0.0508, dev_macro_f1=0.6069, dev_err=36.43%\n",
      "2026-02-20 20:12:17,863 - lg_cotrain - INFO - Phase 2 epoch 7/12: loss1=0.0185, loss2=0.0505, dev_macro_f1=0.6019, dev_err=38.10%\n",
      "2026-02-20 20:13:41,326 - lg_cotrain - INFO - Phase 2 epoch 8/12: loss1=0.0285, loss2=0.0582, dev_macro_f1=0.6014, dev_err=37.47%\n",
      "2026-02-20 20:15:04,790 - lg_cotrain - INFO - Phase 2 epoch 9/12: loss1=0.0270, loss2=0.0319, dev_macro_f1=0.5947, dev_err=37.68%\n",
      "2026-02-20 20:16:28,253 - lg_cotrain - INFO - Phase 2 epoch 10/12: loss1=0.0162, loss2=0.0237, dev_macro_f1=0.6146, dev_err=36.01%\n",
      "2026-02-20 20:17:51,774 - lg_cotrain - INFO - Phase 2 epoch 11/12: loss1=0.0200, loss2=0.0308, dev_macro_f1=0.6103, dev_err=37.68%\n",
      "2026-02-20 20:19:15,326 - lg_cotrain - INFO - Phase 2 epoch 12/12: loss1=0.0209, loss2=0.0345, dev_macro_f1=0.5908, dev_err=38.20%\n",
      "2026-02-20 20:19:15,326 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 20:19:20,697 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.5594, dev_err=45.62%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:19:26,059 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6350, dev_err=33.51%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:19:31,416 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.5949, dev_err=38.31%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 20:19:36,764 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6025, dev_err=37.58%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 20:19:42,116 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.6233, dev_err=34.86%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 20:19:47,498 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.6094, dev_err=36.12%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 20:19:52,854 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6171, dev_err=35.28%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 20:19:58,199 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6264, dev_err=35.18%, es_counter1=6, es_counter2=6\n",
      "2026-02-20 20:20:03,540 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6277, dev_err=35.28%, es_counter1=7, es_counter2=7\n",
      "2026-02-20 20:20:03,541 - lg_cotrain - INFO - Early stopping at epoch 9\n",
      "2026-02-20 20:20:03,554 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 20:20:12,769 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\hurricane_irma_2017\\50_set1\\metrics.json\n",
      "2026-02-20 20:20:12,769 - lg_cotrain - INFO - Test error rate: 32.17%, Test macro-F1: 0.6519, Test ECE: 0.0592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1/20 | Event 7/10 (hurricane_irma_2017) | dev_F1=0.6350 (mean=0.6470) | Elapsed: 1.71h | ETA: 47.01h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 20:20:13,366 - lg_cotrain - INFO - Starting LG-CoTrain: event=hurricane_maria_2017, budget=50, seed_set=1\n",
      "2026-02-20 20:20:13,425 - lg_cotrain - INFO - Detected 9 classes for event hurricane_maria_2017: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 20:20:13,433 - lg_cotrain - INFO - D_l1: 225, D_l2: 225, D_LG: 4644\n",
      "2026-02-20 20:20:13,433 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1142.99it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1059.05it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 20:20:32,741 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1069, mean_prob2=0.1285\n",
      "2026-02-20 20:20:50,646 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1458, mean_prob2=0.1607\n",
      "2026-02-20 20:21:08,511 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1730, mean_prob2=0.1700\n",
      "2026-02-20 20:21:26,372 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1944, mean_prob2=0.1886\n",
      "2026-02-20 20:21:44,256 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.2150, mean_prob2=0.1855\n",
      "2026-02-20 20:22:02,157 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.2677, mean_prob2=0.2167\n",
      "2026-02-20 20:22:20,053 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.3255, mean_prob2=0.2340\n",
      "2026-02-20 20:22:20,055 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.2868, range=[0.0619, 0.6035]\n",
      "2026-02-20 20:22:20,055 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1285, range=[0.0000, 0.2652]\n",
      "2026-02-20 20:22:20,056 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1099.34it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1081.04it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 20:23:24,647 - lg_cotrain - INFO - Phase 2 epoch 1/12: loss1=0.1129, loss2=0.2403, dev_macro_f1=0.6215, dev_err=33.42%\n",
      "2026-02-20 20:24:28,041 - lg_cotrain - INFO - Phase 2 epoch 2/12: loss1=0.0646, loss2=0.1966, dev_macro_f1=0.6351, dev_err=31.13%\n",
      "2026-02-20 20:25:31,449 - lg_cotrain - INFO - Phase 2 epoch 3/12: loss1=0.0379, loss2=0.1358, dev_macro_f1=0.6375, dev_err=31.54%\n",
      "2026-02-20 20:26:34,855 - lg_cotrain - INFO - Phase 2 epoch 4/12: loss1=0.0276, loss2=0.1181, dev_macro_f1=0.6292, dev_err=31.54%\n",
      "2026-02-20 20:27:38,219 - lg_cotrain - INFO - Phase 2 epoch 5/12: loss1=0.0263, loss2=0.0615, dev_macro_f1=0.6466, dev_err=31.81%\n",
      "2026-02-20 20:28:41,581 - lg_cotrain - INFO - Phase 2 epoch 6/12: loss1=0.0310, loss2=0.0545, dev_macro_f1=0.6472, dev_err=31.54%\n",
      "2026-02-20 20:29:44,982 - lg_cotrain - INFO - Phase 2 epoch 7/12: loss1=0.0285, loss2=0.0601, dev_macro_f1=0.6486, dev_err=30.46%\n",
      "2026-02-20 20:30:48,349 - lg_cotrain - INFO - Phase 2 epoch 8/12: loss1=0.0187, loss2=0.0366, dev_macro_f1=0.6487, dev_err=31.27%\n",
      "2026-02-20 20:31:51,756 - lg_cotrain - INFO - Phase 2 epoch 9/12: loss1=0.0135, loss2=0.0353, dev_macro_f1=0.6394, dev_err=31.27%\n",
      "2026-02-20 20:32:55,130 - lg_cotrain - INFO - Phase 2 epoch 10/12: loss1=0.0168, loss2=0.0266, dev_macro_f1=0.6387, dev_err=31.67%\n",
      "2026-02-20 20:33:58,514 - lg_cotrain - INFO - Phase 2 epoch 11/12: loss1=0.0110, loss2=0.0200, dev_macro_f1=0.6501, dev_err=31.27%\n",
      "2026-02-20 20:35:01,985 - lg_cotrain - INFO - Phase 2 epoch 12/12: loss1=0.0142, loss2=0.0260, dev_macro_f1=0.6431, dev_err=30.86%\n",
      "2026-02-20 20:35:01,986 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 20:35:06,642 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.6450, dev_err=33.02%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:35:11,305 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6565, dev_err=30.05%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:35:15,942 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6446, dev_err=31.00%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 20:35:20,586 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6534, dev_err=30.19%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 20:35:25,247 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.6825, dev_err=28.84%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:35:29,907 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.6867, dev_err=28.44%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:35:34,550 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6795, dev_err=28.71%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 20:35:39,193 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6856, dev_err=28.44%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 20:35:43,840 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6809, dev_err=28.98%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 20:35:48,480 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6812, dev_err=28.98%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 20:35:53,128 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.6689, dev_err=28.98%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 20:35:57,769 - lg_cotrain - INFO - Phase 3 epoch 12: dev_macro_f1=0.6854, dev_err=28.71%, es_counter1=6, es_counter2=6\n",
      "2026-02-20 20:36:02,427 - lg_cotrain - INFO - Phase 3 epoch 13: dev_macro_f1=0.6884, dev_err=28.44%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:36:07,069 - lg_cotrain - INFO - Phase 3 epoch 14: dev_macro_f1=0.6718, dev_err=30.19%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 20:36:11,708 - lg_cotrain - INFO - Phase 3 epoch 15: dev_macro_f1=0.6827, dev_err=28.84%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 20:36:16,354 - lg_cotrain - INFO - Phase 3 epoch 16: dev_macro_f1=0.6775, dev_err=29.11%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 20:36:21,001 - lg_cotrain - INFO - Phase 3 epoch 17: dev_macro_f1=0.6792, dev_err=29.38%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 20:36:25,651 - lg_cotrain - INFO - Phase 3 epoch 18: dev_macro_f1=0.6736, dev_err=29.92%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 20:36:30,301 - lg_cotrain - INFO - Phase 3 epoch 19: dev_macro_f1=0.6747, dev_err=29.78%, es_counter1=6, es_counter2=6\n",
      "2026-02-20 20:36:34,937 - lg_cotrain - INFO - Phase 3 epoch 20: dev_macro_f1=0.6737, dev_err=29.78%, es_counter1=7, es_counter2=7\n",
      "2026-02-20 20:36:34,937 - lg_cotrain - INFO - Early stopping at epoch 20\n",
      "2026-02-20 20:36:34,952 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 20:36:42,102 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\hurricane_maria_2017\\50_set1\\metrics.json\n",
      "2026-02-20 20:36:42,103 - lg_cotrain - INFO - Test error rate: 29.82%, Test macro-F1: 0.6654, Test ECE: 0.1468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1/20 | Event 8/10 (hurricane_maria_2017) | dev_F1=0.6884 (mean=0.6522) | Elapsed: 1.98h | ETA: 47.52h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 20:36:42,683 - lg_cotrain - INFO - Starting LG-CoTrain: event=kaikoura_earthquake_2016, budget=50, seed_set=1\n",
      "2026-02-20 20:36:42,723 - lg_cotrain - INFO - Detected 9 classes for event kaikoura_earthquake_2016: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 20:36:42,723 - lg_cotrain - INFO - D_l1: 209, D_l2: 208, D_LG: 1119\n",
      "2026-02-20 20:36:42,723 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1094.25it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1105.01it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 20:36:49,671 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1254, mean_prob2=0.1137\n",
      "2026-02-20 20:36:55,458 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1414, mean_prob2=0.1334\n",
      "2026-02-20 20:37:01,258 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1650, mean_prob2=0.1746\n",
      "2026-02-20 20:37:07,040 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.2176, mean_prob2=0.2204\n",
      "2026-02-20 20:37:12,838 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.2635, mean_prob2=0.2816\n",
      "2026-02-20 20:37:18,623 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.3350, mean_prob2=0.3374\n",
      "2026-02-20 20:37:24,416 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.4069, mean_prob2=0.4065\n",
      "2026-02-20 20:37:24,417 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.3442, range=[0.0621, 0.6742]\n",
      "2026-02-20 20:37:24,418 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1253, range=[0.0096, 0.2724]\n",
      "2026-02-20 20:37:24,419 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1113.51it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1100.69it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 20:37:40,987 - lg_cotrain - INFO - Phase 2 epoch 1/12: loss1=0.1900, loss2=0.5143, dev_macro_f1=0.2963, dev_err=44.20%\n",
      "2026-02-20 20:37:56,362 - lg_cotrain - INFO - Phase 2 epoch 2/12: loss1=0.1235, loss2=0.3045, dev_macro_f1=0.4851, dev_err=33.93%\n",
      "2026-02-20 20:38:11,763 - lg_cotrain - INFO - Phase 2 epoch 3/12: loss1=0.0483, loss2=0.1959, dev_macro_f1=0.5210, dev_err=31.70%\n",
      "2026-02-20 20:38:27,155 - lg_cotrain - INFO - Phase 2 epoch 4/12: loss1=0.0243, loss2=0.1091, dev_macro_f1=0.5168, dev_err=32.59%\n",
      "2026-02-20 20:38:42,566 - lg_cotrain - INFO - Phase 2 epoch 5/12: loss1=0.0220, loss2=0.0832, dev_macro_f1=0.5568, dev_err=31.70%\n",
      "2026-02-20 20:38:57,953 - lg_cotrain - INFO - Phase 2 epoch 6/12: loss1=0.0216, loss2=0.0698, dev_macro_f1=0.5676, dev_err=31.70%\n",
      "2026-02-20 20:39:13,358 - lg_cotrain - INFO - Phase 2 epoch 7/12: loss1=0.0121, loss2=0.0506, dev_macro_f1=0.5924, dev_err=30.80%\n",
      "2026-02-20 20:39:28,766 - lg_cotrain - INFO - Phase 2 epoch 8/12: loss1=0.0283, loss2=0.0467, dev_macro_f1=0.6184, dev_err=30.80%\n",
      "2026-02-20 20:39:44,189 - lg_cotrain - INFO - Phase 2 epoch 9/12: loss1=0.0152, loss2=0.0552, dev_macro_f1=0.6164, dev_err=29.91%\n",
      "2026-02-20 20:39:59,585 - lg_cotrain - INFO - Phase 2 epoch 10/12: loss1=0.0245, loss2=0.0525, dev_macro_f1=0.6218, dev_err=29.46%\n",
      "2026-02-20 20:40:14,983 - lg_cotrain - INFO - Phase 2 epoch 11/12: loss1=0.0398, loss2=0.0294, dev_macro_f1=0.6403, dev_err=29.46%\n",
      "2026-02-20 20:40:30,397 - lg_cotrain - INFO - Phase 2 epoch 12/12: loss1=0.0187, loss2=0.0098, dev_macro_f1=0.6353, dev_err=29.91%\n",
      "2026-02-20 20:40:30,398 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 20:40:33,173 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.6574, dev_err=27.23%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:40:35,932 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6567, dev_err=26.79%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 20:40:38,710 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6862, dev_err=24.55%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:40:41,482 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.7060, dev_err=22.32%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:40:44,260 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.7069, dev_err=22.77%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:40:47,016 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.6824, dev_err=25.89%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 20:40:49,775 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6885, dev_err=25.00%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 20:40:52,537 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6839, dev_err=25.00%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 20:40:55,295 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6755, dev_err=25.89%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 20:40:58,052 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6676, dev_err=26.79%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 20:41:00,811 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.6719, dev_err=26.79%, es_counter1=6, es_counter2=6\n",
      "2026-02-20 20:41:03,574 - lg_cotrain - INFO - Phase 3 epoch 12: dev_macro_f1=0.6786, dev_err=25.89%, es_counter1=7, es_counter2=7\n",
      "2026-02-20 20:41:03,574 - lg_cotrain - INFO - Early stopping at epoch 12\n",
      "2026-02-20 20:41:03,584 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 20:41:05,740 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\kaikoura_earthquake_2016\\50_set1\\metrics.json\n",
      "2026-02-20 20:41:05,741 - lg_cotrain - INFO - Test error rate: 26.44%, Test macro-F1: 0.6713, Test ECE: 0.0785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1/20 | Event 9/10 (kaikoura_earthquake_2016) | dev_F1=0.7069 (mean=0.6582) | Elapsed: 2.05h | ETA: 43.57h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 20:41:06,301 - lg_cotrain - INFO - Starting LG-CoTrain: event=kerala_floods_2018, budget=50, seed_set=1\n",
      "2026-02-20 20:41:06,371 - lg_cotrain - INFO - Detected 9 classes for event kerala_floods_2018: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 20:41:06,381 - lg_cotrain - INFO - D_l1: 220, D_l2: 219, D_LG: 5149\n",
      "2026-02-20 20:41:06,383 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1110.29it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1108.78it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 20:41:27,275 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1114, mean_prob2=0.1307\n",
      "2026-02-20 20:41:46,967 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1395, mean_prob2=0.1360\n",
      "2026-02-20 20:42:06,630 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1502, mean_prob2=0.1534\n",
      "2026-02-20 20:42:26,365 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1805, mean_prob2=0.1935\n",
      "2026-02-20 20:42:46,065 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.2187, mean_prob2=0.2445\n",
      "2026-02-20 20:43:05,764 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.2907, mean_prob2=0.3436\n",
      "2026-02-20 20:43:25,472 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.3170, mean_prob2=0.3735\n",
      "2026-02-20 20:43:25,473 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.2846, range=[0.0664, 0.6322]\n",
      "2026-02-20 20:43:25,473 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1161, range=[0.0107, 0.2632]\n",
      "2026-02-20 20:43:25,473 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1116.73it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1030.21it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 20:44:37,348 - lg_cotrain - INFO - Phase 2 epoch 1/12: loss1=0.1023, loss2=0.2176, dev_macro_f1=0.4791, dev_err=27.76%\n",
      "2026-02-20 20:45:48,048 - lg_cotrain - INFO - Phase 2 epoch 2/12: loss1=0.0664, loss2=0.2000, dev_macro_f1=0.5557, dev_err=27.64%\n",
      "2026-02-20 20:46:58,732 - lg_cotrain - INFO - Phase 2 epoch 3/12: loss1=0.0337, loss2=0.1356, dev_macro_f1=0.5637, dev_err=26.78%\n",
      "2026-02-20 20:48:09,430 - lg_cotrain - INFO - Phase 2 epoch 4/12: loss1=0.0303, loss2=0.0812, dev_macro_f1=0.5877, dev_err=26.54%\n",
      "2026-02-20 20:49:20,140 - lg_cotrain - INFO - Phase 2 epoch 5/12: loss1=0.0224, loss2=0.0778, dev_macro_f1=0.5741, dev_err=27.15%\n",
      "2026-02-20 20:50:30,815 - lg_cotrain - INFO - Phase 2 epoch 6/12: loss1=0.0278, loss2=0.0419, dev_macro_f1=0.5748, dev_err=28.38%\n",
      "2026-02-20 20:51:41,514 - lg_cotrain - INFO - Phase 2 epoch 7/12: loss1=0.0227, loss2=0.0419, dev_macro_f1=0.5486, dev_err=27.89%\n",
      "2026-02-20 20:52:52,197 - lg_cotrain - INFO - Phase 2 epoch 8/12: loss1=0.0094, loss2=0.0462, dev_macro_f1=0.5627, dev_err=26.04%\n",
      "2026-02-20 20:54:02,887 - lg_cotrain - INFO - Phase 2 epoch 9/12: loss1=0.0095, loss2=0.0493, dev_macro_f1=0.5751, dev_err=28.38%\n",
      "2026-02-20 20:55:13,622 - lg_cotrain - INFO - Phase 2 epoch 10/12: loss1=0.0231, loss2=0.0179, dev_macro_f1=0.5756, dev_err=28.26%\n",
      "2026-02-20 20:56:24,327 - lg_cotrain - INFO - Phase 2 epoch 11/12: loss1=0.0243, loss2=0.0235, dev_macro_f1=0.5535, dev_err=28.99%\n",
      "2026-02-20 20:57:35,023 - lg_cotrain - INFO - Phase 2 epoch 12/12: loss1=0.0289, loss2=0.0509, dev_macro_f1=0.5471, dev_err=29.24%\n",
      "2026-02-20 20:57:35,023 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 20:57:39,828 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.4730, dev_err=51.47%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:57:44,636 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.5432, dev_err=37.71%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:57:49,428 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.5500, dev_err=34.28%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:57:54,236 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.5758, dev_err=32.43%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:57:59,038 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.5915, dev_err=30.47%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:58:03,837 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.6068, dev_err=30.47%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 20:58:08,621 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.5990, dev_err=31.57%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 20:58:13,412 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.5907, dev_err=33.17%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 20:58:18,194 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.5827, dev_err=33.91%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 20:58:22,988 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.5834, dev_err=33.78%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 20:58:27,777 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.5834, dev_err=33.91%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 20:58:32,570 - lg_cotrain - INFO - Phase 3 epoch 12: dev_macro_f1=0.5826, dev_err=33.91%, es_counter1=6, es_counter2=6\n",
      "2026-02-20 20:58:37,352 - lg_cotrain - INFO - Phase 3 epoch 13: dev_macro_f1=0.5774, dev_err=34.15%, es_counter1=7, es_counter2=7\n",
      "2026-02-20 20:58:37,352 - lg_cotrain - INFO - Early stopping at epoch 13\n",
      "2026-02-20 20:58:37,364 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 20:58:45,232 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\kerala_floods_2018\\50_set1\\metrics.json\n",
      "2026-02-20 20:58:45,233 - lg_cotrain - INFO - Test error rate: 31.23%, Test macro-F1: 0.5617, Test ECE: 0.1124\n",
      "\u001b[32m[I 2026-02-20 20:58:45,244]\u001b[0m Trial 0 finished with value: 0.6530906543869577 and parameters: {'lr': 4.3793357274313764e-05, 'batch_size': 32, 'cotrain_epochs': 12, 'finetune_patience': 7}. Best is trial 0 with value: 0.6530906543869577.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 1/20 | Event 10/10 (kerala_floods_2018) | dev_F1=0.6068 (mean=0.6531) | Elapsed: 2.35h | ETA: 44.60h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 20:58:45,792 - lg_cotrain - INFO - Starting LG-CoTrain: event=california_wildfires_2018, budget=50, seed_set=1\n",
      "2026-02-20 20:58:45,871 - lg_cotrain - INFO - Detected 10 classes for event california_wildfires_2018: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'missing_or_found_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 20:58:45,882 - lg_cotrain - INFO - D_l1: 250, D_l2: 250, D_LG: 4663\n",
      "2026-02-20 20:58:45,884 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1098.00it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1050.00it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 20:59:05,270 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.0946, mean_prob2=0.1207\n",
      "2026-02-20 20:59:23,413 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1047, mean_prob2=0.0962\n",
      "2026-02-20 20:59:41,534 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.0935, mean_prob2=0.1015\n",
      "2026-02-20 20:59:59,645 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.0873, mean_prob2=0.1000\n",
      "2026-02-20 21:00:17,738 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1029, mean_prob2=0.1003\n",
      "2026-02-20 21:00:35,847 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.0928, mean_prob2=0.1049\n",
      "2026-02-20 21:00:53,992 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1099, mean_prob2=0.0948\n",
      "2026-02-20 21:00:53,993 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1162, range=[0.1014, 0.1428]\n",
      "2026-02-20 21:00:53,995 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.0817, range=[0.0684, 0.0941]\n",
      "2026-02-20 21:00:53,995 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1035.40it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1038.52it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 21:01:59,019 - lg_cotrain - INFO - Phase 2 epoch 1/18: loss1=0.1433, loss2=0.2292, dev_macro_f1=0.0897, dev_err=70.08%\n",
      "2026-02-20 21:03:02,733 - lg_cotrain - INFO - Phase 2 epoch 2/18: loss1=0.1391, loss2=0.3708, dev_macro_f1=0.1054, dev_err=66.89%\n",
      "2026-02-20 21:04:06,379 - lg_cotrain - INFO - Phase 2 epoch 3/18: loss1=0.1347, loss2=0.4754, dev_macro_f1=0.0419, dev_err=73.54%\n",
      "2026-02-20 21:05:09,992 - lg_cotrain - INFO - Phase 2 epoch 4/18: loss1=0.1310, loss2=0.4626, dev_macro_f1=0.0419, dev_err=73.54%\n",
      "2026-02-20 21:06:13,613 - lg_cotrain - INFO - Phase 2 epoch 5/18: loss1=0.1350, loss2=0.4545, dev_macro_f1=0.0419, dev_err=73.54%\n",
      "2026-02-20 21:07:18,653 - lg_cotrain - INFO - Phase 2 epoch 6/18: loss1=0.1343, loss2=0.4452, dev_macro_f1=0.0419, dev_err=73.54%\n",
      "2026-02-20 21:08:22,811 - lg_cotrain - INFO - Phase 2 epoch 7/18: loss1=0.1268, loss2=0.4399, dev_macro_f1=0.0419, dev_err=73.54%\n",
      "2026-02-20 21:09:27,239 - lg_cotrain - INFO - Phase 2 epoch 8/18: loss1=0.1316, loss2=0.4463, dev_macro_f1=0.0419, dev_err=73.54%\n",
      "2026-02-20 21:10:31,300 - lg_cotrain - INFO - Phase 2 epoch 9/18: loss1=0.1388, loss2=0.4440, dev_macro_f1=0.0798, dev_err=69.95%\n",
      "2026-02-20 21:11:36,228 - lg_cotrain - INFO - Phase 2 epoch 10/18: loss1=0.1409, loss2=0.4429, dev_macro_f1=0.0774, dev_err=71.01%\n",
      "2026-02-20 21:12:40,695 - lg_cotrain - INFO - Phase 2 epoch 11/18: loss1=0.1440, loss2=0.4397, dev_macro_f1=0.0419, dev_err=73.54%\n",
      "2026-02-20 21:13:44,901 - lg_cotrain - INFO - Phase 2 epoch 12/18: loss1=0.1432, loss2=0.4386, dev_macro_f1=0.0419, dev_err=73.54%\n",
      "2026-02-20 21:14:50,427 - lg_cotrain - INFO - Phase 2 epoch 13/18: loss1=0.1412, loss2=0.5012, dev_macro_f1=0.0419, dev_err=73.54%\n",
      "2026-02-20 21:15:54,407 - lg_cotrain - INFO - Phase 2 epoch 14/18: loss1=0.1387, loss2=0.4374, dev_macro_f1=0.1039, dev_err=65.96%\n",
      "2026-02-20 21:16:58,647 - lg_cotrain - INFO - Phase 2 epoch 15/18: loss1=0.1372, loss2=0.4502, dev_macro_f1=0.0419, dev_err=73.54%\n",
      "2026-02-20 21:18:02,648 - lg_cotrain - INFO - Phase 2 epoch 16/18: loss1=0.1383, loss2=0.4571, dev_macro_f1=0.0606, dev_err=76.86%\n",
      "2026-02-20 21:19:06,322 - lg_cotrain - INFO - Phase 2 epoch 17/18: loss1=0.1392, loss2=0.4668, dev_macro_f1=0.0419, dev_err=73.54%\n",
      "2026-02-20 21:20:09,962 - lg_cotrain - INFO - Phase 2 epoch 18/18: loss1=0.1378, loss2=0.4642, dev_macro_f1=0.1006, dev_err=69.02%\n",
      "2026-02-20 21:20:09,963 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 21:20:14,816 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.0322, dev_err=80.85%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 21:20:19,667 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.0021, dev_err=98.94%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 21:20:24,520 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.0131, dev_err=93.75%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 21:20:29,368 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.0021, dev_err=98.94%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 21:20:34,210 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.0247, dev_err=85.90%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 21:20:39,057 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.0037, dev_err=98.14%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 21:20:39,057 - lg_cotrain - INFO - Early stopping at epoch 6\n",
      "2026-02-20 21:20:39,071 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 21:20:46,230 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\california_wildfires_2018\\50_set1\\metrics.json\n",
      "2026-02-20 21:20:46,231 - lg_cotrain - INFO - Test error rate: 80.84%, Test macro-F1: 0.0322, Test ECE: 0.0406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 2/20 | Event 1/10 (california_wildfires_2018) | dev_F1=0.0322 (mean=0.0322) | Elapsed: 2.71h | ETA: 46.64h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 21:20:46,816 - lg_cotrain - INFO - Starting LG-CoTrain: event=canada_wildfires_2016, budget=50, seed_set=1\n",
      "2026-02-20 21:20:46,860 - lg_cotrain - INFO - Detected 8 classes for event canada_wildfires_2016: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 21:20:46,865 - lg_cotrain - INFO - D_l1: 182, D_l2: 182, D_LG: 1205\n",
      "2026-02-20 21:20:46,866 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1204.56it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1143.95it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 21:20:53,791 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1219, mean_prob2=0.1380\n",
      "2026-02-20 21:20:59,590 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1363, mean_prob2=0.1513\n",
      "2026-02-20 21:21:05,393 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1546, mean_prob2=0.1268\n",
      "2026-02-20 21:21:11,196 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1432, mean_prob2=0.1408\n",
      "2026-02-20 21:21:16,985 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1283, mean_prob2=0.1483\n",
      "2026-02-20 21:21:22,785 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1443, mean_prob2=0.1364\n",
      "2026-02-20 21:21:28,560 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1330, mean_prob2=0.1298\n",
      "2026-02-20 21:21:28,561 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1567, range=[0.0476, 0.1658]\n",
      "2026-02-20 21:21:28,562 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1197, range=[0.0288, 0.1310]\n",
      "2026-02-20 21:21:28,562 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1060.85it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1082.72it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 21:21:46,224 - lg_cotrain - INFO - Phase 2 epoch 1/18: loss1=0.1968, loss2=0.2575, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:22:02,681 - lg_cotrain - INFO - Phase 2 epoch 2/18: loss1=0.2048, loss2=0.3073, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:22:19,144 - lg_cotrain - INFO - Phase 2 epoch 3/18: loss1=0.1324, loss2=0.4557, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:22:35,608 - lg_cotrain - INFO - Phase 2 epoch 4/18: loss1=0.0774, loss2=0.4107, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:22:52,063 - lg_cotrain - INFO - Phase 2 epoch 5/18: loss1=0.0756, loss2=0.3436, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:23:08,511 - lg_cotrain - INFO - Phase 2 epoch 6/18: loss1=0.0781, loss2=0.3348, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:23:24,969 - lg_cotrain - INFO - Phase 2 epoch 7/18: loss1=0.0752, loss2=0.3291, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:23:41,421 - lg_cotrain - INFO - Phase 2 epoch 8/18: loss1=0.0754, loss2=0.3253, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:23:57,861 - lg_cotrain - INFO - Phase 2 epoch 9/18: loss1=0.0735, loss2=0.3176, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:24:14,317 - lg_cotrain - INFO - Phase 2 epoch 10/18: loss1=0.0662, loss2=0.3193, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:24:30,773 - lg_cotrain - INFO - Phase 2 epoch 11/18: loss1=0.0669, loss2=0.3078, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:24:47,221 - lg_cotrain - INFO - Phase 2 epoch 12/18: loss1=0.0624, loss2=0.3015, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:25:03,675 - lg_cotrain - INFO - Phase 2 epoch 13/18: loss1=0.0623, loss2=0.2961, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:25:20,122 - lg_cotrain - INFO - Phase 2 epoch 14/18: loss1=0.0605, loss2=0.2888, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:25:36,574 - lg_cotrain - INFO - Phase 2 epoch 15/18: loss1=0.0606, loss2=0.2838, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:25:53,016 - lg_cotrain - INFO - Phase 2 epoch 16/18: loss1=0.0581, loss2=0.2785, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:26:09,450 - lg_cotrain - INFO - Phase 2 epoch 17/18: loss1=0.0569, loss2=0.2739, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:26:25,911 - lg_cotrain - INFO - Phase 2 epoch 18/18: loss1=0.0554, loss2=0.2682, dev_macro_f1=0.0735, dev_err=58.33%\n",
      "2026-02-20 21:26:25,911 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 21:26:28,429 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.0247, dev_err=89.04%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 21:26:30,928 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.0085, dev_err=96.49%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 21:26:33,428 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.0247, dev_err=89.04%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 21:26:35,942 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.0365, dev_err=82.89%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 21:26:38,439 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.0164, dev_err=92.98%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 21:26:40,939 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.0308, dev_err=85.96%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 21:26:43,437 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.0115, dev_err=95.18%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 21:26:45,938 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.0365, dev_err=82.89%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 21:26:48,436 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.0308, dev_err=85.96%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 21:26:48,436 - lg_cotrain - INFO - Early stopping at epoch 9\n",
      "2026-02-20 21:26:48,451 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 21:26:50,620 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\canada_wildfires_2016\\50_set1\\metrics.json\n",
      "2026-02-20 21:26:50,621 - lg_cotrain - INFO - Test error rate: 83.15%, Test macro-F1: 0.0361, Test ECE: 0.0006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 2/20 | Event 2/10 (canada_wildfires_2016) | dev_F1=0.0365 (mean=0.0343) | Elapsed: 2.82h | ETA: 44.11h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 21:26:51,287 - lg_cotrain - INFO - Starting LG-CoTrain: event=cyclone_idai_2019, budget=50, seed_set=1\n",
      "2026-02-20 21:26:51,342 - lg_cotrain - INFO - Detected 10 classes for event cyclone_idai_2019: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'missing_or_found_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 21:26:51,348 - lg_cotrain - INFO - D_l1: 227, D_l2: 226, D_LG: 2300\n",
      "2026-02-20 21:26:51,350 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1094.67it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|████████████████| 199/199 [00:00<00:00, 994.93it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 21:27:02,682 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.0803, mean_prob2=0.0973\n",
      "2026-02-20 21:27:12,679 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1173, mean_prob2=0.1075\n",
      "2026-02-20 21:27:22,656 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1129, mean_prob2=0.0986\n",
      "2026-02-20 21:27:32,643 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1018, mean_prob2=0.1086\n",
      "2026-02-20 21:27:42,651 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1017, mean_prob2=0.1207\n",
      "2026-02-20 21:27:52,643 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1086, mean_prob2=0.1291\n",
      "2026-02-20 21:28:02,619 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1205, mean_prob2=0.0932\n",
      "2026-02-20 21:28:02,620 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1305, range=[0.0679, 0.1679]\n",
      "2026-02-20 21:28:02,621 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.0785, range=[0.0264, 0.0973]\n",
      "2026-02-20 21:28:02,621 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1151.68it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1151.66it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 21:28:35,319 - lg_cotrain - INFO - Phase 2 epoch 1/18: loss1=0.1205, loss2=0.2072, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:29:06,912 - lg_cotrain - INFO - Phase 2 epoch 2/18: loss1=0.1105, loss2=0.2386, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:29:38,479 - lg_cotrain - INFO - Phase 2 epoch 3/18: loss1=0.0586, loss2=0.2349, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:30:10,151 - lg_cotrain - INFO - Phase 2 epoch 4/18: loss1=0.0386, loss2=0.2371, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:30:41,708 - lg_cotrain - INFO - Phase 2 epoch 5/18: loss1=0.0240, loss2=0.2316, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:31:13,268 - lg_cotrain - INFO - Phase 2 epoch 6/18: loss1=0.0159, loss2=0.2197, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:31:44,814 - lg_cotrain - INFO - Phase 2 epoch 7/18: loss1=0.0130, loss2=0.2076, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:32:16,364 - lg_cotrain - INFO - Phase 2 epoch 8/18: loss1=0.0101, loss2=0.1970, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:32:47,882 - lg_cotrain - INFO - Phase 2 epoch 9/18: loss1=0.0067, loss2=0.1883, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:33:19,419 - lg_cotrain - INFO - Phase 2 epoch 10/18: loss1=0.0077, loss2=0.1801, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:33:50,864 - lg_cotrain - INFO - Phase 2 epoch 11/18: loss1=0.0058, loss2=0.1734, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:34:22,324 - lg_cotrain - INFO - Phase 2 epoch 12/18: loss1=0.0045, loss2=0.1663, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:34:53,760 - lg_cotrain - INFO - Phase 2 epoch 13/18: loss1=0.0041, loss2=0.1606, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:35:25,198 - lg_cotrain - INFO - Phase 2 epoch 14/18: loss1=0.0036, loss2=0.1540, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:35:56,638 - lg_cotrain - INFO - Phase 2 epoch 15/18: loss1=0.0029, loss2=0.1496, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:36:28,065 - lg_cotrain - INFO - Phase 2 epoch 16/18: loss1=0.0024, loss2=0.1452, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:36:59,486 - lg_cotrain - INFO - Phase 2 epoch 17/18: loss1=0.0019, loss2=0.1410, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:37:30,909 - lg_cotrain - INFO - Phase 2 epoch 18/18: loss1=0.0010, loss2=0.1376, dev_macro_f1=0.0645, dev_err=52.37%\n",
      "2026-02-20 21:37:30,911 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-20 21:37:34,440 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.0645, dev_err=52.37%, es_counter1=0, es_counter2=0\n",
      "2026-02-20 21:37:37,948 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.0165, dev_err=91.02%, es_counter1=1, es_counter2=1\n",
      "2026-02-20 21:37:41,459 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.0039, dev_err=98.00%, es_counter1=2, es_counter2=2\n",
      "2026-02-20 21:37:44,964 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.0645, dev_err=52.37%, es_counter1=3, es_counter2=3\n",
      "2026-02-20 21:37:48,476 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.0044, dev_err=97.76%, es_counter1=4, es_counter2=4\n",
      "2026-02-20 21:37:51,979 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.0645, dev_err=52.37%, es_counter1=5, es_counter2=5\n",
      "2026-02-20 21:37:51,979 - lg_cotrain - INFO - Early stopping at epoch 6\n",
      "2026-02-20 21:37:51,996 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-20 21:37:55,824 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\optuna\\cyclone_idai_2019\\50_set1\\metrics.json\n",
      "2026-02-20 21:37:55,824 - lg_cotrain - INFO - Test error rate: 52.50%, Test macro-F1: 0.0644, Test ECE: 0.1887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trial 2/20 | Event 3/10 (cyclone_idai_2019) | dev_F1=0.0645 (mean=0.0444) | Elapsed: 3.00h | ETA: 43.16h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 21:37:56,394 - lg_cotrain - INFO - Starting LG-CoTrain: event=hurricane_dorian_2019, budget=50, seed_set=1\n",
      "2026-02-20 21:37:56,461 - lg_cotrain - INFO - Detected 9 classes for event hurricane_dorian_2019: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 21:37:56,470 - lg_cotrain - INFO - D_l1: 221, D_l2: 221, D_LG: 4887\n",
      "2026-02-20 21:37:56,472 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1134.38it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1036.94it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 21:38:16,297 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1117, mean_prob2=0.1162\n",
      "2026-02-20 21:38:34,931 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1105, mean_prob2=0.1172\n",
      "2026-02-20 21:38:53,528 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1156, mean_prob2=0.1102\n",
      "2026-02-20 21:39:12,094 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1036, mean_prob2=0.1118\n",
      "2026-02-20 21:39:30,664 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1227, mean_prob2=0.1145\n",
      "2026-02-20 21:39:49,227 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1127, mean_prob2=0.1148\n",
      "2026-02-20 21:40:07,775 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1053, mean_prob2=0.1131\n",
      "2026-02-20 21:40:07,776 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1364, range=[0.1261, 0.1528]\n",
      "2026-02-20 21:40:07,777 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1035, range=[0.0673, 0.1114]\n",
      "2026-02-20 21:40:07,777 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1172.49it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1082.38it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 21:41:15,392 - lg_cotrain - INFO - Phase 2 epoch 1/18: loss1=0.2025, loss2=0.2613, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "2026-02-20 21:42:21,787 - lg_cotrain - INFO - Phase 2 epoch 2/18: loss1=0.1832, loss2=0.3033, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "2026-02-20 21:43:28,139 - lg_cotrain - INFO - Phase 2 epoch 3/18: loss1=0.1547, loss2=0.3284, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "2026-02-20 21:44:34,492 - lg_cotrain - INFO - Phase 2 epoch 4/18: loss1=0.1570, loss2=0.3406, dev_macro_f1=0.0229, dev_err=88.53%\n",
      "2026-02-20 21:45:40,858 - lg_cotrain - INFO - Phase 2 epoch 5/18: loss1=0.1597, loss2=0.3457, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "2026-02-20 21:46:47,281 - lg_cotrain - INFO - Phase 2 epoch 6/18: loss1=0.1594, loss2=0.3481, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "2026-02-20 21:47:53,653 - lg_cotrain - INFO - Phase 2 epoch 7/18: loss1=0.1600, loss2=0.3485, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "2026-02-20 21:48:59,967 - lg_cotrain - INFO - Phase 2 epoch 8/18: loss1=0.1612, loss2=0.3475, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "2026-02-20 21:50:06,275 - lg_cotrain - INFO - Phase 2 epoch 9/18: loss1=0.1607, loss2=0.3468, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "2026-02-20 21:51:12,560 - lg_cotrain - INFO - Phase 2 epoch 10/18: loss1=0.1621, loss2=0.3440, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "2026-02-20 21:52:18,845 - lg_cotrain - INFO - Phase 2 epoch 11/18: loss1=0.1609, loss2=0.3406, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "2026-02-20 21:53:25,126 - lg_cotrain - INFO - Phase 2 epoch 12/18: loss1=0.1605, loss2=0.3387, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "2026-02-20 21:54:32,710 - lg_cotrain - INFO - Phase 2 epoch 13/18: loss1=0.1612, loss2=0.3349, dev_macro_f1=0.0340, dev_err=81.96%\n",
      "\u001b[33m[W 2026-02-20 21:54:49,501]\u001b[0m Trial 1 failed with parameters: {'lr': 0.0003634293648319154, 'batch_size': 32, 'cotrain_epochs': 18, 'finetune_patience': 5} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\lg_cotrain\\optuna_tuner.py\", line 104, in objective\n",
      "    result = trainer.run()\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\lg_cotrain\\trainer.py\", line 254, in run\n",
      "    logits2 = model2(input_ids, attention_mask)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\lg_cotrain\\model.py\", line 26, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 841, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 1139, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 915, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\utils\\output_capturing.py\", line 253, in wrapper\n",
      "    outputs = func(self, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 688, in forward\n",
      "    attention_mask, encoder_attention_mask = self._create_attention_masks(\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 735, in _create_attention_masks\n",
      "    attention_mask = create_bidirectional_mask(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\utils\\deprecation.py\", line 171, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\masking_utils.py\", line 1020, in create_bidirectional_mask\n",
      "    attention_mask = mask_interface(\n",
      "                     ^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\masking_utils.py\", line 495, in sdpa_mask\n",
      "    mask_function = and_masks(mask_function, padding_mask_function(padding_mask))\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\masking_utils.py\", line 148, in padding_mask_function\n",
      "    def padding_mask_function(padding_mask: torch.Tensor) -> Callable:\n",
      "    \n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2026-02-20 21:54:49,559]\u001b[0m Trial 1 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m start_time = time.time()\n\u001b[32m     48\u001b[39m tracker = ProgressTracker(N_TRIALS, \u001b[38;5;28mlen\u001b[39m(events_to_use), start_time)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m study = \u001b[43mrun_study\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevents_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTUNING_BUDGET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTUNING_SEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDATA_ROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresults_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRESULTS_ROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSTUDY_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_on_event_done\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtracker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_event_done\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m elapsed = time.time() - start_time\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStudy completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m3600\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mh (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mmin)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\lg_cotrain\\optuna_tuner.py:166\u001b[39m, in \u001b[36mrun_study\u001b[39m\u001b[34m(n_trials, events, budget, seed_set, data_root, results_root, study_name, storage, fixed_params, _trainer_cls, _on_event_done)\u001b[39m\n\u001b[32m    144\u001b[39m study = optuna.create_study(\n\u001b[32m    145\u001b[39m     study_name=study_name,\n\u001b[32m    146\u001b[39m     direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m     ),\n\u001b[32m    153\u001b[39m )\n\u001b[32m    155\u001b[39m objective = create_objective(\n\u001b[32m    156\u001b[39m     events=events \u001b[38;5;129;01mor\u001b[39;00m ALL_EVENTS,\n\u001b[32m    157\u001b[39m     budget=budget,\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m     _on_event_done=_on_event_done,\n\u001b[32m    164\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:68\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:165\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:263\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    256\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    259\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    262\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\optuna\\study\\_optimize.py:206\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    208\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    209\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\lg_cotrain\\optuna_tuner.py:104\u001b[39m, in \u001b[36mcreate_objective.<locals>.objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    102\u001b[39m     trainer = _trainer_cls(cfg)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m dev_f1_scores.append(result[\u001b[33m\"\u001b[39m\u001b[33mdev_macro_f1\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    107\u001b[39m mean_so_far = statistics.mean(dev_f1_scores)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\lg_cotrain\\trainer.py:254\u001b[39m, in \u001b[36mLGCoTrainer.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    251\u001b[39m opt1.step()\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# Model 2 loss (uses theta1's weights = lambda1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m logits2 = \u001b[43mmodel2\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m per_sample_loss2 = F.cross_entropy(logits2, labels, reduction=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    256\u001b[39m loss2 = (w1 * per_sample_loss2).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\lg_cotrain\\model.py:26\u001b[39m, in \u001b[36mBertClassifier.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, labels)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, labels=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     25\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass returning logits.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We compute loss ourselves for weighted CE\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs.logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\utils\\generic.py:841\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    840\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    843\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1139\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, **kwargs)\u001b[39m\n\u001b[32m   1121\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m   1122\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1131\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m   1132\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor] | SequenceClassifierOutput:\n\u001b[32m   1133\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1135\u001b[39m \u001b[33;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[33;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[33;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1149\u001b[39m     pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1151\u001b[39m     pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\utils\\generic.py:915\u001b[39m, in \u001b[36mmerge_with_config_defaults.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    913\u001b[39m             output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[38;5;66;03m# Restore original config value\u001b[39;00m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_causal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\utils\\output_capturing.py:253\u001b[39m, in \u001b[36mcapture_outputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Run the forward\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Reset the states\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    256\u001b[39m     _active_collector.reset(output_token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:688\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n\u001b[32m    680\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    681\u001b[39m     input_ids=input_ids,\n\u001b[32m    682\u001b[39m     position_ids=position_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    685\u001b[39m     past_key_values_length=past_key_values_length,\n\u001b[32m    686\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m attention_mask, encoder_attention_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_attention_masks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m encoder_outputs = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m    698\u001b[39m     embedding_output,\n\u001b[32m    699\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    706\u001b[39m     **kwargs,\n\u001b[32m    707\u001b[39m )\n\u001b[32m    708\u001b[39m sequence_output = encoder_outputs.last_hidden_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:735\u001b[39m, in \u001b[36mBertModel._create_attention_masks\u001b[39m\u001b[34m(self, attention_mask, encoder_attention_mask, embedding_output, encoder_hidden_states, cache_position, past_key_values)\u001b[39m\n\u001b[32m    727\u001b[39m     attention_mask = create_causal_mask(\n\u001b[32m    728\u001b[39m         config=\u001b[38;5;28mself\u001b[39m.config,\n\u001b[32m    729\u001b[39m         inputs_embeds=embedding_output,\n\u001b[32m   (...)\u001b[39m\u001b[32m    732\u001b[39m         past_key_values=past_key_values,\n\u001b[32m    733\u001b[39m     )\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     attention_mask = \u001b[43mcreate_bidirectional_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    742\u001b[39m     encoder_attention_mask = create_bidirectional_mask(\n\u001b[32m    743\u001b[39m         config=\u001b[38;5;28mself\u001b[39m.config,\n\u001b[32m    744\u001b[39m         inputs_embeds=embedding_output,\n\u001b[32m    745\u001b[39m         attention_mask=encoder_attention_mask,\n\u001b[32m    746\u001b[39m         encoder_hidden_states=encoder_hidden_states,\n\u001b[32m    747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\utils\\deprecation.py:171\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    169\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\masking_utils.py:1020\u001b[39m, in \u001b[36mcreate_bidirectional_mask\u001b[39m\u001b[34m(config, inputs_embeds, attention_mask, encoder_hidden_states, or_mask_function, and_mask_function)\u001b[39m\n\u001b[32m   1017\u001b[39m     use_vmap = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1019\u001b[39m \u001b[38;5;66;03m# We now create the mask\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m attention_mask = \u001b[43mmask_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_factory_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Additional kwargs for sdpa\u001b[39;49;00m\n\u001b[32m   1028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_is_causal_skip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_is_bidirectional_skip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_is_bidirectional_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Additional kwarg for eager\u001b[39;49;00m\n\u001b[32m   1031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the config as well, in case someone wants to easily have their own mask_interface\u001b[39;49;00m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_vmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_vmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Short-circuit to non-vmap expansions for the mask\u001b[39;49;00m\n\u001b[32m   1033\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attention_mask\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\masking_utils.py:495\u001b[39m, in \u001b[36msdpa_mask\u001b[39m\u001b[34m(batch_size, cache_position, kv_length, kv_offset, mask_function, attention_mask, local_size, allow_is_causal_skip, allow_is_bidirectional_skip, allow_torch_fix, use_vmap, **kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# Potentially add the padding 2D mask\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     mask_function = and_masks(mask_function, \u001b[43mpadding_mask_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    497\u001b[39m batch_arange = torch.arange(batch_size, device=cache_position.device)\n\u001b[32m    498\u001b[39m head_arange = torch.arange(\u001b[32m1\u001b[39m, device=cache_position.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\masking_utils.py:148\u001b[39m, in \u001b[36mpadding_mask_function\u001b[39m\u001b[34m(padding_mask)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m    This return the mask_function function to create a chunked attention mask.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m and_masks(chunked_overlay(chunk_size, left_padding), causal_mask_function)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpadding_mask_function\u001b[39m(padding_mask: torch.Tensor) -> Callable:\n\u001b[32m    149\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[33;03m    This return the mask_function function corresponding to a 2D padding mask.\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner_mask\u001b[39m(batch_idx: \u001b[38;5;28mint\u001b[39m, head_idx: \u001b[38;5;28mint\u001b[39m, q_idx: \u001b[38;5;28mint\u001b[39m, kv_idx: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    154\u001b[39m         \u001b[38;5;66;03m# Note that here the mask should ALWAYS be at least of the max `kv_index` size in the dimension 1. This is because\u001b[39;00m\n\u001b[32m    155\u001b[39m         \u001b[38;5;66;03m# we cannot pad it here in the mask_function as we don't know the final size, and we cannot try/except, as it is not\u001b[39;00m\n\u001b[32m    156\u001b[39m         \u001b[38;5;66;03m# vectorizable on accelerator devices\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"Track per-event progress across all Optuna trials.\"\"\"\n",
    "\n",
    "    def __init__(self, n_trials: int, n_events: int, start_time: float):\n",
    "        self.n_trials = n_trials\n",
    "        self.n_events = n_events\n",
    "        self.total_runs = n_trials * n_events  # upper bound (pruning reduces this)\n",
    "        self.runs_done = 0\n",
    "        self.trials_done = 0\n",
    "        self.start_time = start_time\n",
    "        self._current_trial = -1\n",
    "\n",
    "    def on_event_done(self, trial_number, event, event_idx, n_events, dev_f1, mean_f1):\n",
    "        \"\"\"Called after each event within a trial.\"\"\"\n",
    "        self.runs_done += 1\n",
    "\n",
    "        # Detect trial transitions\n",
    "        if trial_number != self._current_trial:\n",
    "            if self._current_trial >= 0:\n",
    "                self.trials_done += 1\n",
    "            self._current_trial = trial_number\n",
    "\n",
    "        elapsed = time.time() - self.start_time\n",
    "        elapsed_h = elapsed / 3600\n",
    "        # ETA based on average time per run so far\n",
    "        if self.runs_done > 0:\n",
    "            avg_per_run = elapsed / self.runs_done\n",
    "            # Estimate remaining: remaining trials × events per trial\n",
    "            remaining_this_trial = n_events - (event_idx + 1)\n",
    "            remaining_future = (self.n_trials - self.trials_done - 1) * n_events\n",
    "            remaining_runs = remaining_this_trial + remaining_future\n",
    "            eta_h = avg_per_run * remaining_runs / 3600\n",
    "        else:\n",
    "            eta_h = 0\n",
    "\n",
    "        print(\n",
    "            f\"  Trial {trial_number + 1}/{self.n_trials} | \"\n",
    "            f\"Event {event_idx + 1}/{n_events} ({event}) | \"\n",
    "            f\"dev_F1={dev_f1:.4f} (mean={mean_f1:.4f}) | \"\n",
    "            f\"Elapsed: {elapsed_h:.2f}h | ETA: {eta_h:.2f}h\"\n",
    "        )\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "tracker = ProgressTracker(N_TRIALS, len(events_to_use), start_time)\n",
    "\n",
    "study = run_study(\n",
    "    n_trials=N_TRIALS,\n",
    "    events=events_to_use,\n",
    "    budget=TUNING_BUDGET,\n",
    "    seed_set=TUNING_SEED,\n",
    "    data_root=DATA_ROOT,\n",
    "    results_root=RESULTS_ROOT,\n",
    "    study_name=STUDY_NAME,\n",
    "    _on_event_done=tracker.on_event_done,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nStudy completed in {elapsed / 3600:.2f}h ({elapsed / 60:.1f}min)\")\n",
    "print(f\"Runs executed: {tracker.runs_done} (of {tracker.total_runs} max)\")\n",
    "print(f\"Pruned trials saved ~{tracker.total_runs - tracker.runs_done} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save Results to JSON\n",
    "\n",
    "Export the study results as a human-readable JSON file for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Build results dict\n",
    "results = {\n",
    "    \"study_name\": STUDY_NAME,\n",
    "    \"n_trials\": len(study.trials),\n",
    "    \"best_trial\": {\n",
    "        \"number\": study.best_trial.number,\n",
    "        \"mean_dev_macro_f1\": round(study.best_value, 6),\n",
    "        \"params\": study.best_params,\n",
    "    },\n",
    "    \"paper_defaults\": {\n",
    "        \"lr\": 2e-5,\n",
    "        \"batch_size\": 32,\n",
    "        \"cotrain_epochs\": 10,\n",
    "        \"finetune_patience\": 5,\n",
    "    },\n",
    "    \"search_space\": {\n",
    "        \"lr\": \"1e-5 to 1e-3 (log-uniform)\",\n",
    "        \"batch_size\": [8, 16, 32, 64],\n",
    "        \"cotrain_epochs\": \"5 to 20\",\n",
    "        \"finetune_patience\": \"4 to 10\",\n",
    "    },\n",
    "    \"trials\": [],\n",
    "}\n",
    "\n",
    "for t in study.trials:\n",
    "    trial_info = {\n",
    "        \"number\": t.number,\n",
    "        \"state\": t.state.name,\n",
    "        \"params\": t.params,\n",
    "    }\n",
    "    if t.value is not None:\n",
    "        trial_info[\"mean_dev_macro_f1\"] = round(t.value, 6)\n",
    "    if t.datetime_start and t.datetime_complete:\n",
    "        trial_info[\"duration_seconds\"] = round(\n",
    "            (t.datetime_complete - t.datetime_start).total_seconds(), 1\n",
    "        )\n",
    "    results[\"trials\"].append(trial_info)\n",
    "\n",
    "# Save to results directory\n",
    "output_dir = Path(RESULTS_ROOT)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"optuna_results.json\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "print(f\"\\nBest trial #{results['best_trial']['number']}:\")\n",
    "print(f\"  Mean dev macro-F1: {results['best_trial']['mean_dev_macro_f1']}\")\n",
    "for k, v in results['best_trial']['params'].items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization History\n",
    "\n",
    "Plot how the objective value evolved across trials. Completed trials are shown\n",
    "as blue dots; pruned trials as red X marks. The dashed line tracks the\n",
    "running best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "completed = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "pruned    = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Completed trials\n",
    "if completed:\n",
    "    ax.scatter(\n",
    "        [t.number for t in completed],\n",
    "        [t.value for t in completed],\n",
    "        color=\"tab:blue\", alpha=0.7, label=f\"Completed ({len(completed)})\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "# Pruned trials (show at last reported value)\n",
    "if pruned:\n",
    "    pruned_vals = []\n",
    "    for t in pruned:\n",
    "        if t.intermediate_values:\n",
    "            last_step = max(t.intermediate_values.keys())\n",
    "            pruned_vals.append(t.intermediate_values[last_step])\n",
    "        else:\n",
    "            pruned_vals.append(0)\n",
    "    ax.scatter(\n",
    "        [t.number for t in pruned],\n",
    "        pruned_vals,\n",
    "        color=\"tab:red\", marker=\"x\", alpha=0.5, label=f\"Pruned ({len(pruned)})\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "# Running best line\n",
    "if completed:\n",
    "    sorted_completed = sorted(completed, key=lambda t: t.number)\n",
    "    running_best = []\n",
    "    best_so_far = -1\n",
    "    for t in sorted_completed:\n",
    "        best_so_far = max(best_so_far, t.value)\n",
    "        running_best.append(best_so_far)\n",
    "    ax.plot(\n",
    "        [t.number for t in sorted_completed],\n",
    "        running_best,\n",
    "        \"--\", color=\"tab:green\", alpha=0.8, label=\"Running best\",\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Trial number\")\n",
    "ax.set_ylabel(\"Mean dev macro-F1\")\n",
    "ax.set_title(\"Optimization History\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total trials : {len(study.trials)}\")\n",
    "print(f\"Completed    : {len(completed)}\")\n",
    "print(f\"Pruned       : {len(pruned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parameter Distributions\n",
    "\n",
    "Visualize how Optuna explored the search space. For each parameter, we show:\n",
    "- A histogram of sampled values (completed trials only)\n",
    "- The best trial's value highlighted in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\"lr\", \"batch_size\", \"cotrain_epochs\", \"finetune_patience\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "for ax, param in zip(axes.flat, params):\n",
    "    values = [t.params[param] for t in completed]\n",
    "    best_val = best.params[param]\n",
    "\n",
    "    if param == \"lr\":\n",
    "        # Log scale for learning rate\n",
    "        log_values = [np.log10(v) for v in values]\n",
    "        ax.hist(log_values, bins=15, alpha=0.7, color=\"tab:blue\", edgecolor=\"white\")\n",
    "        ax.axvline(np.log10(best_val), color=\"tab:red\", linestyle=\"--\",\n",
    "                   label=f\"Best: {best_val:.2e}\")\n",
    "        ax.set_xlabel(f\"{param} (log10)\")\n",
    "    elif param == \"batch_size\":\n",
    "        # Categorical: bar chart\n",
    "        from collections import Counter\n",
    "        counts = Counter(values)\n",
    "        categories = [8, 16, 32, 64]\n",
    "        bar_counts = [counts.get(c, 0) for c in categories]\n",
    "        bar_colors = [\"tab:red\" if c == best_val else \"tab:blue\" for c in categories]\n",
    "        ax.bar([str(c) for c in categories], bar_counts, color=bar_colors, alpha=0.7,\n",
    "               edgecolor=\"white\")\n",
    "        ax.set_xlabel(param)\n",
    "    else:\n",
    "        ax.hist(values, bins=range(min(values), max(values) + 2), alpha=0.7,\n",
    "                color=\"tab:blue\", edgecolor=\"white\", align=\"left\")\n",
    "        ax.axvline(best_val, color=\"tab:red\", linestyle=\"--\",\n",
    "                   label=f\"Best: {best_val}\")\n",
    "        ax.set_xlabel(param)\n",
    "\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(param)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.suptitle(\"Parameter Distributions (Completed Trials)\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter vs Objective Scatter Plots\n",
    "\n",
    "See how each parameter correlates with the objective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "for ax, param in zip(axes.flat, params):\n",
    "    values = [t.params[param] for t in completed]\n",
    "    objectives = [t.value for t in completed]\n",
    "\n",
    "    if param == \"lr\":\n",
    "        ax.scatter(values, objectives, alpha=0.6, color=\"tab:blue\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.axvline(best.params[param], color=\"tab:red\", linestyle=\"--\", alpha=0.5)\n",
    "    elif param == \"batch_size\":\n",
    "        # Add jitter for visibility\n",
    "        jittered = [v + np.random.uniform(-1.5, 1.5) for v in values]\n",
    "        ax.scatter(jittered, objectives, alpha=0.6, color=\"tab:blue\")\n",
    "        ax.set_xticks([8, 16, 32, 64])\n",
    "    else:\n",
    "        jittered = [v + np.random.uniform(-0.3, 0.3) for v in values]\n",
    "        ax.scatter(jittered, objectives, alpha=0.6, color=\"tab:blue\")\n",
    "        ax.axvline(best.params[param], color=\"tab:red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel(param)\n",
    "    ax.set_ylabel(\"Mean dev macro-F1\")\n",
    "    ax.set_title(f\"{param} vs Objective\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Parameter vs Objective (Completed Trials)\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Trial Summary Table\n",
    "\n",
    "Show all completed trials sorted by objective value (best first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_trials = sorted(completed, key=lambda t: t.value, reverse=True)\n",
    "\n",
    "print(f\"{'#':>4}  {'Mean Dev F1':>12}  {'lr':>10}  {'batch':>6}  {'co_ep':>6}  {'pat':>4}  {'Duration':>10}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for t in sorted_trials[:20]:  # Show top 20\n",
    "    duration = (t.datetime_complete - t.datetime_start).total_seconds() if t.datetime_complete else 0\n",
    "    duration_str = f\"{duration / 60:.1f}min\" if duration < 3600 else f\"{duration / 3600:.1f}h\"\n",
    "    print(\n",
    "        f\"{t.number:>4}  {t.value:>12.4f}  {t.params['lr']:>10.2e}  \"\n",
    "        f\"{t.params['batch_size']:>6}  {t.params['cotrain_epochs']:>6}  \"\n",
    "        f\"{t.params['finetune_patience']:>4}  {duration_str:>10}\"\n",
    "    )\n",
    "\n",
    "if len(sorted_trials) > 20:\n",
    "    print(f\"  ... and {len(sorted_trials) - 20} more trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Apply Best Hyperparameters\n",
    "\n",
    "Use the best hyperparameters found by Optuna to run the full experiment grid\n",
    "(all budgets, all seed sets, all events).\n",
    "\n",
    "**Copy the CLI command below** and run it in a terminal, or use the\n",
    "`run_experiment.py` API directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = study.best_params\n",
    "\n",
    "print(\"Apply the best hyperparameters via CLI:\\n\")\n",
    "print(\"# Single event:\")\n",
    "print(\n",
    "    f\"python -m lg_cotrain.run_experiment \\\\\"\n",
    "    f\"\\n    --event kaikoura_earthquake_2016 \\\\\"\n",
    "    f\"\\n    --lr {bp['lr']:.6f} \\\\\"\n",
    "    f\"\\n    --batch-size {bp['batch_size']} \\\\\"\n",
    "    f\"\\n    --cotrain-epochs {bp['cotrain_epochs']} \\\\\"\n",
    "    f\"\\n    --finetune-patience {bp['finetune_patience']}\"\n",
    ")\n",
    "\n",
    "print(\"\\n# All events (full sweep):\")\n",
    "events_str = \" \".join(ALL_EVENTS)\n",
    "print(\n",
    "    f\"python -m lg_cotrain.run_experiment \\\\\"\n",
    "    f\"\\n    --events {events_str} \\\\\"\n",
    "    f\"\\n    --lr {bp['lr']:.6f} \\\\\"\n",
    "    f\"\\n    --batch-size {bp['batch_size']} \\\\\"\n",
    "    f\"\\n    --cotrain-epochs {bp['cotrain_epochs']} \\\\\"\n",
    "    f\"\\n    --finetune-patience {bp['finetune_patience']} \\\\\"\n",
    "    f\"\\n    --output-folder results/gpt-4o/test/optuna-tuned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook ran a **global Optuna hyperparameter study** to find optimal\n",
    "`lr`, `batch_size`, `cotrain_epochs`, and `finetune_patience` for the\n",
    "LG-CoTrain pipeline.\n",
    "\n",
    "### Methodology\n",
    "- **Objective**: Mean dev macro-F1 across all 10 disaster events (budget=50, seed=1)\n",
    "- **Sampler**: TPE (Tree-structured Parzen Estimator)\n",
    "- **Pruner**: MedianPruner (prune after 3+ events if below median)\n",
    "- **No test-set leakage**: Only dev set metrics used for optimization\n",
    "- **Output**: Results saved as JSON to `results/optuna/optuna_results.json`\n",
    "\n",
    "### CLI equivalent\n",
    "```bash\n",
    "python -m lg_cotrain.optuna_tuner --n-trials 20\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
