{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LG-CoTrain: Global Optuna Hyperparameter Tuning\n",
    "\n",
    "This notebook finds the **optimal hyperparameters** for the LG-CoTrain pipeline\n",
    "using a single global [Optuna](https://optuna.org/) study. Each trial runs the\n",
    "full 3-phase pipeline across all 10 disaster events (budget=50, seed=1) and\n",
    "optimizes the **mean dev macro-F1**.\n",
    "\n",
    "### Why global tuning?\n",
    "\n",
    "- **No test-set leakage**: The objective uses `dev_macro_f1`, not test F1.\n",
    "- **Generalizable**: One set of hyperparameters that works across all events,\n",
    "  rather than overfitting to a single event.\n",
    "- **Efficient**: Optuna's TPE sampler + MedianPruner skips unpromising trials early.\n",
    "\n",
    "### Search space\n",
    "\n",
    "| Parameter | Range | Scale |\n",
    "|-----------|-------|-------|\n",
    "| `lr` | 1e-5 to 1e-3 | Log-uniform |\n",
    "| `batch_size` | [8, 16, 32, 64] | Categorical |\n",
    "| `cotrain_epochs` | 5 to 20 | Uniform integer |\n",
    "| `finetune_patience` | 4 to 10 | Uniform integer |\n",
    "\n",
    "### Paper deviation note\n",
    "\n",
    "The original paper (Cornelia et al. 2025) uses **fixed** hyperparameters:\n",
    "lr=2e-5, batch_size=32, cotrain_epochs=10, patience=5. This notebook explores\n",
    "whether tuning these improves performance.\n",
    "\n",
    "### Usage\n",
    "\n",
    "1. Run cells 1-3 to configure and launch the Optuna study\n",
    "2. After the study completes, results are saved as a JSON file\n",
    "3. Cells 4-7 visualize and analyze the results\n",
    "4. Cell 8 shows the CLI command to apply the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: D:\\Workspace\\Co-Training\n",
      "Optuna version: 4.7.0\n",
      "Events (10): ['california_wildfires_2018', 'canada_wildfires_2016', 'cyclone_idai_2019', 'hurricane_dorian_2019', 'hurricane_florence_2018', 'hurricane_harvey_2017', 'hurricane_irma_2017', 'hurricane_maria_2017', 'kaikoura_earthquake_2016', 'kerala_floods_2018']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _find_repo_root(marker: str = \"lg_cotrain\") -> Path:\n",
    "    for candidate in [Path().resolve()] + list(Path().resolve().parents):\n",
    "        if (candidate / marker).is_dir():\n",
    "            return candidate\n",
    "    raise RuntimeError(\n",
    "        f\"Cannot find repo root: no ancestor directory contains '{marker}/'. \"\n",
    "        \"Run the notebook from inside the repository.\"\n",
    "    )\n",
    "\n",
    "\n",
    "repo_root = _find_repo_root()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "import optuna\n",
    "\n",
    "import lg_cotrain.optuna_tuner\n",
    "importlib.reload(lg_cotrain.optuna_tuner)\n",
    "from lg_cotrain.optuna_tuner import ALL_EVENTS, create_objective, run_study\n",
    "\n",
    "print(f\"Repo root: {repo_root}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"Events ({len(ALL_EVENTS)}): {ALL_EVENTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set the number of trials, budget, and seed for the tuning study.\n",
    "\n",
    "- **`N_TRIALS`**: More trials = better hyperparameters, but each trial runs\n",
    "  the full pipeline across all 10 events. Start with 10-20 for a quick scan,\n",
    "  use 50+ for thorough tuning.\n",
    "- **`TUNING_BUDGET`**: Budget level used during tuning (50 = most labeled data).\n",
    "- **`TUNING_SEED`**: Seed set used during tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study name   : lg_cotrain_global\n",
      "Trials       : 20\n",
      "Events/trial : 10\n",
      "Total runs   : 200 (upper bound, pruning may reduce this)\n",
      "Budget       : 50\n",
      "Seed set     : 1\n",
      "Results root : D:\\Workspace\\Co-Training\\results\\optuna\n",
      "\n",
      "Search space:\n",
      "  lr               : 1e-5 to 1e-3  (log-uniform)\n",
      "  batch_size       : [8, 16, 32, 64]\n",
      "  cotrain_epochs   : 5 to 20\n",
      "  finetune_patience: 4 to 10\n"
     ]
    }
   ],
   "source": [
    "# ---- Tuning Configuration ----\n",
    "\n",
    "N_TRIALS      = 20        # Number of Optuna trials\n",
    "TUNING_BUDGET = 50        # Budget level for tuning\n",
    "TUNING_SEED   = 1         # Seed set for tuning\n",
    "STUDY_NAME    = \"lg_cotrain_global\"  # Optuna study name\n",
    "\n",
    "DATA_ROOT    = str(repo_root / \"data\")\n",
    "RESULTS_ROOT = str(repo_root / \"results\" / \"optuna\")\n",
    "\n",
    "# Optionally restrict to a subset of events for faster iteration\n",
    "# Set to None to use all 10 events\n",
    "EVENTS = None  # or e.g. [\"hurricane_harvey_2017\", \"kerala_floods_2018\"]\n",
    "\n",
    "events_to_use = EVENTS or ALL_EVENTS\n",
    "runs_per_trial = len(events_to_use)\n",
    "total_runs = N_TRIALS * runs_per_trial\n",
    "\n",
    "print(f\"Study name   : {STUDY_NAME}\")\n",
    "print(f\"Trials       : {N_TRIALS}\")\n",
    "print(f\"Events/trial : {runs_per_trial}\")\n",
    "print(f\"Total runs   : {total_runs} (upper bound, pruning may reduce this)\")\n",
    "print(f\"Budget       : {TUNING_BUDGET}\")\n",
    "print(f\"Seed set     : {TUNING_SEED}\")\n",
    "print(f\"Results root : {RESULTS_ROOT}\")\n",
    "print()\n",
    "print(\"Search space:\")\n",
    "print(\"  lr               : 1e-5 to 1e-3  (log-uniform)\")\n",
    "print(\"  batch_size       : [8, 16, 32, 64]\")\n",
    "print(\"  cotrain_epochs   : 5 to 20\")\n",
    "print(\"  finetune_patience: 4 to 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run the Optuna Study\n",
    "\n",
    "This cell launches the study. Each trial:\n",
    "\n",
    "1. Optuna's TPE sampler picks a set of hyperparameters\n",
    "2. The full 3-phase pipeline runs for each event\n",
    "3. After each event, the running mean dev F1 is reported to the pruner\n",
    "4. If the trial looks unpromising (below median), it's pruned early\n",
    "5. The final objective = mean dev macro-F1 across all events\n",
    "\n",
    "**Progress tracking**: After each event completes, you'll see the current trial,\n",
    "event, dev F1, elapsed time, and estimated time remaining.\n",
    "\n",
    "**This will take a long time** (hours to days depending on GPU and N_TRIALS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-20 18:37:54,594]\u001b[0m A new study created in memory with name: lg_cotrain_global\u001b[0m\n",
      "2026-02-20 18:37:59,381 - lg_cotrain - INFO - Starting LG-CoTrain: event=california_wildfires_2018, budget=50, seed_set=1\n",
      "2026-02-20 18:37:59,410 - lg_cotrain - INFO - Detected 10 classes for event california_wildfires_2018: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'missing_or_found_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-20 18:37:59,539 - lg_cotrain - INFO - D_l1: 250, D_l2: 250, D_LG: 4663\n",
      "2026-02-20 18:37:59,545 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1051.21it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1075.41it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-20 18:38:19,012 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1061, mean_prob2=0.0956\n",
      "2026-02-20 18:38:37,154 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1051, mean_prob2=0.1265\n",
      "2026-02-20 18:38:55,439 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1166, mean_prob2=0.1455\n",
      "2026-02-20 18:39:13,730 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1479, mean_prob2=0.2016\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"Track per-event progress across all Optuna trials.\"\"\"\n",
    "\n",
    "    def __init__(self, n_trials: int, n_events: int, start_time: float):\n",
    "        self.n_trials = n_trials\n",
    "        self.n_events = n_events\n",
    "        self.total_runs = n_trials * n_events  # upper bound (pruning reduces this)\n",
    "        self.runs_done = 0\n",
    "        self.trials_done = 0\n",
    "        self.start_time = start_time\n",
    "        self._current_trial = -1\n",
    "\n",
    "    def on_event_done(self, trial_number, event, event_idx, n_events, dev_f1, mean_f1):\n",
    "        \"\"\"Called after each event within a trial.\"\"\"\n",
    "        self.runs_done += 1\n",
    "\n",
    "        # Detect trial transitions\n",
    "        if trial_number != self._current_trial:\n",
    "            if self._current_trial >= 0:\n",
    "                self.trials_done += 1\n",
    "            self._current_trial = trial_number\n",
    "\n",
    "        elapsed = time.time() - self.start_time\n",
    "        elapsed_h = elapsed / 3600\n",
    "        # ETA based on average time per run so far\n",
    "        if self.runs_done > 0:\n",
    "            avg_per_run = elapsed / self.runs_done\n",
    "            # Estimate remaining: remaining trials × events per trial\n",
    "            remaining_this_trial = n_events - (event_idx + 1)\n",
    "            remaining_future = (self.n_trials - self.trials_done - 1) * n_events\n",
    "            remaining_runs = remaining_this_trial + remaining_future\n",
    "            eta_h = avg_per_run * remaining_runs / 3600\n",
    "        else:\n",
    "            eta_h = 0\n",
    "\n",
    "        print(\n",
    "            f\"  Trial {trial_number + 1}/{self.n_trials} | \"\n",
    "            f\"Event {event_idx + 1}/{n_events} ({event}) | \"\n",
    "            f\"dev_F1={dev_f1:.4f} (mean={mean_f1:.4f}) | \"\n",
    "            f\"Elapsed: {elapsed_h:.2f}h | ETA: {eta_h:.2f}h\"\n",
    "        )\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "tracker = ProgressTracker(N_TRIALS, len(events_to_use), start_time)\n",
    "\n",
    "study = run_study(\n",
    "    n_trials=N_TRIALS,\n",
    "    events=events_to_use,\n",
    "    budget=TUNING_BUDGET,\n",
    "    seed_set=TUNING_SEED,\n",
    "    data_root=DATA_ROOT,\n",
    "    results_root=RESULTS_ROOT,\n",
    "    study_name=STUDY_NAME,\n",
    "    _on_event_done=tracker.on_event_done,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nStudy completed in {elapsed / 3600:.2f}h ({elapsed / 60:.1f}min)\")\n",
    "print(f\"Runs executed: {tracker.runs_done} (of {tracker.total_runs} max)\")\n",
    "print(f\"Pruned trials saved ~{tracker.total_runs - tracker.runs_done} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save Results to JSON\n",
    "\n",
    "Export the study results as a human-readable JSON file for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Build results dict\n",
    "results = {\n",
    "    \"study_name\": STUDY_NAME,\n",
    "    \"n_trials\": len(study.trials),\n",
    "    \"best_trial\": {\n",
    "        \"number\": study.best_trial.number,\n",
    "        \"mean_dev_macro_f1\": round(study.best_value, 6),\n",
    "        \"params\": study.best_params,\n",
    "    },\n",
    "    \"paper_defaults\": {\n",
    "        \"lr\": 2e-5,\n",
    "        \"batch_size\": 32,\n",
    "        \"cotrain_epochs\": 10,\n",
    "        \"finetune_patience\": 5,\n",
    "    },\n",
    "    \"search_space\": {\n",
    "        \"lr\": \"1e-5 to 1e-3 (log-uniform)\",\n",
    "        \"batch_size\": [8, 16, 32, 64],\n",
    "        \"cotrain_epochs\": \"5 to 20\",\n",
    "        \"finetune_patience\": \"4 to 10\",\n",
    "    },\n",
    "    \"trials\": [],\n",
    "}\n",
    "\n",
    "for t in study.trials:\n",
    "    trial_info = {\n",
    "        \"number\": t.number,\n",
    "        \"state\": t.state.name,\n",
    "        \"params\": t.params,\n",
    "    }\n",
    "    if t.value is not None:\n",
    "        trial_info[\"mean_dev_macro_f1\"] = round(t.value, 6)\n",
    "    if t.datetime_start and t.datetime_complete:\n",
    "        trial_info[\"duration_seconds\"] = round(\n",
    "            (t.datetime_complete - t.datetime_start).total_seconds(), 1\n",
    "        )\n",
    "    results[\"trials\"].append(trial_info)\n",
    "\n",
    "# Save to results directory\n",
    "output_dir = Path(RESULTS_ROOT)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"optuna_results.json\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "print(f\"\\nBest trial #{results['best_trial']['number']}:\")\n",
    "print(f\"  Mean dev macro-F1: {results['best_trial']['mean_dev_macro_f1']}\")\n",
    "for k, v in results['best_trial']['params'].items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization History\n",
    "\n",
    "Plot how the objective value evolved across trials. Completed trials are shown\n",
    "as blue dots; pruned trials as red X marks. The dashed line tracks the\n",
    "running best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "completed = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "pruned    = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Completed trials\n",
    "if completed:\n",
    "    ax.scatter(\n",
    "        [t.number for t in completed],\n",
    "        [t.value for t in completed],\n",
    "        color=\"tab:blue\", alpha=0.7, label=f\"Completed ({len(completed)})\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "# Pruned trials (show at last reported value)\n",
    "if pruned:\n",
    "    pruned_vals = []\n",
    "    for t in pruned:\n",
    "        if t.intermediate_values:\n",
    "            last_step = max(t.intermediate_values.keys())\n",
    "            pruned_vals.append(t.intermediate_values[last_step])\n",
    "        else:\n",
    "            pruned_vals.append(0)\n",
    "    ax.scatter(\n",
    "        [t.number for t in pruned],\n",
    "        pruned_vals,\n",
    "        color=\"tab:red\", marker=\"x\", alpha=0.5, label=f\"Pruned ({len(pruned)})\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "# Running best line\n",
    "if completed:\n",
    "    sorted_completed = sorted(completed, key=lambda t: t.number)\n",
    "    running_best = []\n",
    "    best_so_far = -1\n",
    "    for t in sorted_completed:\n",
    "        best_so_far = max(best_so_far, t.value)\n",
    "        running_best.append(best_so_far)\n",
    "    ax.plot(\n",
    "        [t.number for t in sorted_completed],\n",
    "        running_best,\n",
    "        \"--\", color=\"tab:green\", alpha=0.8, label=\"Running best\",\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Trial number\")\n",
    "ax.set_ylabel(\"Mean dev macro-F1\")\n",
    "ax.set_title(\"Optimization History\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total trials : {len(study.trials)}\")\n",
    "print(f\"Completed    : {len(completed)}\")\n",
    "print(f\"Pruned       : {len(pruned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parameter Distributions\n",
    "\n",
    "Visualize how Optuna explored the search space. For each parameter, we show:\n",
    "- A histogram of sampled values (completed trials only)\n",
    "- The best trial's value highlighted in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\"lr\", \"batch_size\", \"cotrain_epochs\", \"finetune_patience\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "for ax, param in zip(axes.flat, params):\n",
    "    values = [t.params[param] for t in completed]\n",
    "    best_val = best.params[param]\n",
    "\n",
    "    if param == \"lr\":\n",
    "        # Log scale for learning rate\n",
    "        log_values = [np.log10(v) for v in values]\n",
    "        ax.hist(log_values, bins=15, alpha=0.7, color=\"tab:blue\", edgecolor=\"white\")\n",
    "        ax.axvline(np.log10(best_val), color=\"tab:red\", linestyle=\"--\",\n",
    "                   label=f\"Best: {best_val:.2e}\")\n",
    "        ax.set_xlabel(f\"{param} (log10)\")\n",
    "    elif param == \"batch_size\":\n",
    "        # Categorical: bar chart\n",
    "        from collections import Counter\n",
    "        counts = Counter(values)\n",
    "        categories = [8, 16, 32, 64]\n",
    "        bar_counts = [counts.get(c, 0) for c in categories]\n",
    "        bar_colors = [\"tab:red\" if c == best_val else \"tab:blue\" for c in categories]\n",
    "        ax.bar([str(c) for c in categories], bar_counts, color=bar_colors, alpha=0.7,\n",
    "               edgecolor=\"white\")\n",
    "        ax.set_xlabel(param)\n",
    "    else:\n",
    "        ax.hist(values, bins=range(min(values), max(values) + 2), alpha=0.7,\n",
    "                color=\"tab:blue\", edgecolor=\"white\", align=\"left\")\n",
    "        ax.axvline(best_val, color=\"tab:red\", linestyle=\"--\",\n",
    "                   label=f\"Best: {best_val}\")\n",
    "        ax.set_xlabel(param)\n",
    "\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(param)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.suptitle(\"Parameter Distributions (Completed Trials)\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter vs Objective Scatter Plots\n",
    "\n",
    "See how each parameter correlates with the objective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "for ax, param in zip(axes.flat, params):\n",
    "    values = [t.params[param] for t in completed]\n",
    "    objectives = [t.value for t in completed]\n",
    "\n",
    "    if param == \"lr\":\n",
    "        ax.scatter(values, objectives, alpha=0.6, color=\"tab:blue\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.axvline(best.params[param], color=\"tab:red\", linestyle=\"--\", alpha=0.5)\n",
    "    elif param == \"batch_size\":\n",
    "        # Add jitter for visibility\n",
    "        jittered = [v + np.random.uniform(-1.5, 1.5) for v in values]\n",
    "        ax.scatter(jittered, objectives, alpha=0.6, color=\"tab:blue\")\n",
    "        ax.set_xticks([8, 16, 32, 64])\n",
    "    else:\n",
    "        jittered = [v + np.random.uniform(-0.3, 0.3) for v in values]\n",
    "        ax.scatter(jittered, objectives, alpha=0.6, color=\"tab:blue\")\n",
    "        ax.axvline(best.params[param], color=\"tab:red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel(param)\n",
    "    ax.set_ylabel(\"Mean dev macro-F1\")\n",
    "    ax.set_title(f\"{param} vs Objective\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Parameter vs Objective (Completed Trials)\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Trial Summary Table\n",
    "\n",
    "Show all completed trials sorted by objective value (best first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_trials = sorted(completed, key=lambda t: t.value, reverse=True)\n",
    "\n",
    "print(f\"{'#':>4}  {'Mean Dev F1':>12}  {'lr':>10}  {'batch':>6}  {'co_ep':>6}  {'pat':>4}  {'Duration':>10}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for t in sorted_trials[:20]:  # Show top 20\n",
    "    duration = (t.datetime_complete - t.datetime_start).total_seconds() if t.datetime_complete else 0\n",
    "    duration_str = f\"{duration / 60:.1f}min\" if duration < 3600 else f\"{duration / 3600:.1f}h\"\n",
    "    print(\n",
    "        f\"{t.number:>4}  {t.value:>12.4f}  {t.params['lr']:>10.2e}  \"\n",
    "        f\"{t.params['batch_size']:>6}  {t.params['cotrain_epochs']:>6}  \"\n",
    "        f\"{t.params['finetune_patience']:>4}  {duration_str:>10}\"\n",
    "    )\n",
    "\n",
    "if len(sorted_trials) > 20:\n",
    "    print(f\"  ... and {len(sorted_trials) - 20} more trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Apply Best Hyperparameters\n",
    "\n",
    "Use the best hyperparameters found by Optuna to run the full experiment grid\n",
    "(all budgets, all seed sets, all events).\n",
    "\n",
    "**Copy the CLI command below** and run it in a terminal, or use the\n",
    "`run_experiment.py` API directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = study.best_params\n",
    "\n",
    "print(\"Apply the best hyperparameters via CLI:\\n\")\n",
    "print(\"# Single event:\")\n",
    "print(\n",
    "    f\"python -m lg_cotrain.run_experiment \\\\\"\n",
    "    f\"\\n    --event kaikoura_earthquake_2016 \\\\\"\n",
    "    f\"\\n    --lr {bp['lr']:.6f} \\\\\"\n",
    "    f\"\\n    --batch-size {bp['batch_size']} \\\\\"\n",
    "    f\"\\n    --cotrain-epochs {bp['cotrain_epochs']} \\\\\"\n",
    "    f\"\\n    --finetune-patience {bp['finetune_patience']}\"\n",
    ")\n",
    "\n",
    "print(\"\\n# All events (full sweep):\")\n",
    "events_str = \" \".join(ALL_EVENTS)\n",
    "print(\n",
    "    f\"python -m lg_cotrain.run_experiment \\\\\"\n",
    "    f\"\\n    --events {events_str} \\\\\"\n",
    "    f\"\\n    --lr {bp['lr']:.6f} \\\\\"\n",
    "    f\"\\n    --batch-size {bp['batch_size']} \\\\\"\n",
    "    f\"\\n    --cotrain-epochs {bp['cotrain_epochs']} \\\\\"\n",
    "    f\"\\n    --finetune-patience {bp['finetune_patience']} \\\\\"\n",
    "    f\"\\n    --output-folder results/gpt-4o/test/optuna-tuned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook ran a **global Optuna hyperparameter study** to find optimal\n",
    "`lr`, `batch_size`, `cotrain_epochs`, and `finetune_patience` for the\n",
    "LG-CoTrain pipeline.\n",
    "\n",
    "### Methodology\n",
    "- **Objective**: Mean dev macro-F1 across all 10 disaster events (budget=50, seed=1)\n",
    "- **Sampler**: TPE (Tree-structured Parzen Estimator)\n",
    "- **Pruner**: MedianPruner (prune after 3+ events if below median)\n",
    "- **No test-set leakage**: Only dev set metrics used for optimization\n",
    "- **Output**: Results saved as JSON to `results/optuna/optuna_results.json`\n",
    "\n",
    "### CLI equivalent\n",
    "```bash\n",
    "python -m lg_cotrain.optuna_tuner --n-trials 20\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
