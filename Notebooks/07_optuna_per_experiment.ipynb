{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LG-CoTrain: Per-Experiment Optuna Hyperparameter Tuning\n",
    "\n",
    "This notebook runs **120 separate Optuna studies** — one for each\n",
    "(event, budget, seed_set) combination — to find experiment-specific\n",
    "optimal hyperparameters.\n",
    "\n",
    "### Why per-experiment tuning?\n",
    "\n",
    "- Different disaster events and budget levels may benefit from different\n",
    "  hyperparameters (e.g., low-budget experiments may need different LR or patience).\n",
    "- Each study optimizes `dev_macro_f1` — **no test-set leakage**.\n",
    "- Results are saved as JSON files for easy inspection and reuse.\n",
    "\n",
    "### Search space (6 hyperparameters)\n",
    "\n",
    "| Parameter | Type | Range | Default |\n",
    "|-----------|------|-------|--------|\n",
    "| `lr` | Float (log) | 1e-5 to 1e-3 | 2e-5 |\n",
    "| `batch_size` | Categorical | [8, 16, 32, 64] | 32 |\n",
    "| `cotrain_epochs` | Integer | 5 to 20 | 10 |\n",
    "| `finetune_patience` | Integer | 4 to 10 | 5 |\n",
    "| `weight_decay` | Float | 0.0 to 0.1 | 0.01 |\n",
    "| `warmup_ratio` | Float | 0.0 to 0.3 | 0.1 |\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. Run this notebook to find 120 sets of optimal hyperparameters\n",
    "2. Use notebook 08 to run final experiments with the optimized hyperparameters\n",
    "3. Compare against run-3 (default hyperparameters)\n",
    "\n",
    "### Resume support\n",
    "\n",
    "Studies whose `best_params.json` already exists are automatically skipped.\n",
    "You can interrupt and restart this notebook safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _find_repo_root(marker: str = \"lg_cotrain\") -> Path:\n",
    "    for candidate in [Path().resolve()] + list(Path().resolve().parents):\n",
    "        if (candidate / marker).is_dir():\n",
    "            return candidate\n",
    "    raise RuntimeError(\n",
    "        f\"Cannot find repo root: no ancestor directory contains '{marker}/'. \"\n",
    "        \"Run the notebook from inside the repository.\"\n",
    "    )\n",
    "\n",
    "\n",
    "repo_root = _find_repo_root()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "import lg_cotrain.optuna_per_experiment\n",
    "importlib.reload(lg_cotrain.optuna_per_experiment)\n",
    "from lg_cotrain.optuna_per_experiment import (\n",
    "    ALL_EVENTS, BUDGETS, SEED_SETS,\n",
    "    run_all_studies, load_best_params,\n",
    ")\n",
    "\n",
    "print(f\"Repo root: {repo_root}\")\n",
    "print(f\"Events ({len(ALL_EVENTS)}): {ALL_EVENTS}\")\n",
    "print(f\"Budgets: {BUDGETS}\")\n",
    "print(f\"Seed sets: {SEED_SETS}\")\n",
    "print(f\"Total studies: {len(ALL_EVENTS) * len(BUDGETS) * len(SEED_SETS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "- **`N_TRIALS`**: Number of Optuna trials per study. More trials = better\n",
    "  hyperparameters but longer runtime. Start with 10-15.\n",
    "- **`NUM_GPUS`**: Number of GPUs for parallel study execution. Each study\n",
    "  runs on one GPU; multiple studies run simultaneously.\n",
    "- **`STORAGE_DIR`**: Where to save `best_params.json` files and `summary.json`.\n",
    "\n",
    "### Runtime estimate\n",
    "\n",
    "- ~7 min per pipeline run x N_TRIALS per study x 120 studies\n",
    "- With 2 GPUs: N_TRIALS=15 -> ~105 hours (~4.4 days)\n",
    "- Resume support allows running in stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Tuning Configuration ----\n",
    "\n",
    "N_TRIALS  = 15        # Optuna trials per study\n",
    "NUM_GPUS  = 2         # Number of GPUs for parallel execution\n",
    "\n",
    "DATA_ROOT    = str(repo_root / \"data\")\n",
    "STORAGE_DIR  = str(repo_root / \"results\" / \"optuna\" / \"per_experiment\")\n",
    "\n",
    "PSEUDO_LABEL_SOURCE = \"gpt-4o\"\n",
    "\n",
    "# Optionally restrict to a subset (set to None for all)\n",
    "EVENTS    = None   # or e.g. [\"hurricane_harvey_2017\", \"kerala_floods_2018\"]\n",
    "BUDGETS_  = None   # or e.g. [5, 50]\n",
    "SEEDS_    = None   # or e.g. [1]\n",
    "\n",
    "events_to_use  = EVENTS or ALL_EVENTS\n",
    "budgets_to_use = BUDGETS_ or BUDGETS\n",
    "seeds_to_use   = SEEDS_ or SEED_SETS\n",
    "total_studies   = len(events_to_use) * len(budgets_to_use) * len(seeds_to_use)\n",
    "total_trials    = total_studies * N_TRIALS\n",
    "\n",
    "est_hours = total_trials * 7 / 60 / max(NUM_GPUS, 1)\n",
    "\n",
    "print(f\"Studies        : {total_studies}\")\n",
    "print(f\"Trials/study   : {N_TRIALS}\")\n",
    "print(f\"Total trials   : {total_trials}\")\n",
    "print(f\"GPUs           : {NUM_GPUS}\")\n",
    "print(f\"Storage dir    : {STORAGE_DIR}\")\n",
    "print(f\"Est. runtime   : ~{est_hours:.0f} hours ({est_hours/24:.1f} days)\")\n",
    "print()\n",
    "print(\"Search space:\")\n",
    "print(\"  lr               : 1e-5 to 1e-3  (log-uniform)\")\n",
    "print(\"  batch_size       : [8, 16, 32, 64]\")\n",
    "print(\"  cotrain_epochs   : 5 to 20\")\n",
    "print(\"  finetune_patience: 4 to 10\")\n",
    "print(\"  weight_decay     : 0.0 to 0.1\")\n",
    "print(\"  warmup_ratio     : 0.0 to 0.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run All Optuna Studies\n",
    "\n",
    "This cell launches all studies. With `NUM_GPUS > 1`, studies run in parallel\n",
    "across GPUs using `ProcessPoolExecutor` with spawn context.\n",
    "\n",
    "**Progress tracking**: After each study completes, you'll see the event,\n",
    "budget, seed, status, and best dev F1 found.\n",
    "\n",
    "**Resume**: If you interrupt and restart, completed studies (with existing\n",
    "`best_params.json`) are automatically skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudyProgressTracker:\n",
    "    \"\"\"Track progress across all Optuna studies.\"\"\"\n",
    "\n",
    "    def __init__(self, total_studies: int, start_time: float):\n",
    "        self.total = total_studies\n",
    "        self.done = 0\n",
    "        self.start_time = start_time\n",
    "\n",
    "    def on_study_done(self, event, budget, seed_set, status):\n",
    "        self.done += 1\n",
    "        elapsed = time.time() - self.start_time\n",
    "        elapsed_h = elapsed / 3600\n",
    "        if self.done > 0:\n",
    "            avg_per_study = elapsed / self.done\n",
    "            remaining = (self.total - self.done) * avg_per_study\n",
    "            eta_h = remaining / 3600\n",
    "        else:\n",
    "            eta_h = 0\n",
    "        print(\n",
    "            f\"  [{self.done}/{self.total}] {event} b={budget} s={seed_set}\"\n",
    "            f\" -> {status} | Elapsed: {elapsed_h:.2f}h | ETA: {eta_h:.2f}h\"\n",
    "        )\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "tracker = StudyProgressTracker(total_studies, start_time)\n",
    "\n",
    "all_results = run_all_studies(\n",
    "    events=events_to_use,\n",
    "    budgets=budgets_to_use,\n",
    "    seed_sets=seeds_to_use,\n",
    "    n_trials=N_TRIALS,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    storage_dir=STORAGE_DIR,\n",
    "    data_root=DATA_ROOT,\n",
    "    pseudo_label_source=PSEUDO_LABEL_SOURCE,\n",
    "    on_study_done=tracker.on_study_done,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTotal time: {elapsed / 3600:.2f}h ({elapsed / 60:.1f}min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results Overview\n",
    "\n",
    "Load the summary and display a table of all 120 best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load summary\n",
    "summary_path = Path(STORAGE_DIR) / \"summary.json\"\n",
    "with open(summary_path) as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print(f\"Total studies : {summary['total_studies']}\")\n",
    "print(f\"Completed     : {summary['completed']}\")\n",
    "print(f\"Failed        : {summary['failed']}\")\n",
    "print(f\"Trials/study  : {summary['n_trials_per_study']}\")\n",
    "print()\n",
    "\n",
    "# Show best results table\n",
    "done_studies = [s for s in summary['studies'] if s['status'] == 'done']\n",
    "\n",
    "print(f\"{'Event':>30}  {'Budget':>6}  {'Seed':>4}  {'Best Dev F1':>11}  \"\n",
    "      f\"{'LR':>10}  {'Batch':>5}  {'CoEp':>4}  {'Pat':>3}  {'WD':>6}  {'WR':>5}\")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "for s in done_studies:\n",
    "    bp = s['best_params']\n",
    "    if bp is None:\n",
    "        continue\n",
    "    print(\n",
    "        f\"{s['event']:>30}  {s['budget']:>6}  {s['seed_set']:>4}  \"\n",
    "        f\"{s['best_value']:>11.4f}  \"\n",
    "        f\"{bp.get('lr', 0):>10.2e}  \"\n",
    "        f\"{bp.get('batch_size', 0):>5}  \"\n",
    "        f\"{bp.get('cotrain_epochs', 0):>4}  \"\n",
    "        f\"{bp.get('finetune_patience', 0):>3}  \"\n",
    "        f\"{bp.get('weight_decay', 0):>6.4f}  \"\n",
    "        f\"{bp.get('warmup_ratio', 0):>5.3f}\"\n",
    "    )\n",
    "\n",
    "# Aggregate stats\n",
    "if done_studies:\n",
    "    values = [s['best_value'] for s in done_studies if s['best_value'] is not None]\n",
    "    import statistics\n",
    "    print(f\"\\nBest dev F1 across all studies:\")\n",
    "    print(f\"  Mean: {statistics.mean(values):.4f}\")\n",
    "    print(f\"  Std:  {statistics.stdev(values):.4f}\" if len(values) > 1 else \"\")\n",
    "    print(f\"  Min:  {min(values):.4f}\")\n",
    "    print(f\"  Max:  {max(values):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations\n",
    "\n",
    "Box plots of best dev F1 by event and budget, and parameter distribution\n",
    "analysis across all 120 studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "done_studies = [s for s in summary['studies'] if s['status'] == 'done' and s['best_value'] is not None]\n",
    "\n",
    "# --- Box plot: Best dev F1 by event ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Group by event\n",
    "event_groups = {}\n",
    "for s in done_studies:\n",
    "    event_groups.setdefault(s['event'], []).append(s['best_value'])\n",
    "\n",
    "events_sorted = sorted(event_groups.keys())\n",
    "data_by_event = [event_groups[e] for e in events_sorted]\n",
    "labels_event = [e.replace('_', '\\n') for e in events_sorted]\n",
    "\n",
    "axes[0].boxplot(data_by_event, labels=labels_event)\n",
    "axes[0].set_ylabel('Best dev macro-F1')\n",
    "axes[0].set_title('Best Dev F1 by Event')\n",
    "axes[0].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Group by budget\n",
    "budget_groups = {}\n",
    "for s in done_studies:\n",
    "    budget_groups.setdefault(s['budget'], []).append(s['best_value'])\n",
    "\n",
    "budgets_sorted = sorted(budget_groups.keys())\n",
    "data_by_budget = [budget_groups[b] for b in budgets_sorted]\n",
    "\n",
    "axes[1].boxplot(data_by_budget, labels=[str(b) for b in budgets_sorted])\n",
    "axes[1].set_xlabel('Budget')\n",
    "axes[1].set_ylabel('Best dev macro-F1')\n",
    "axes[1].set_title('Best Dev F1 by Budget')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Per-Experiment Optuna: Best Dev F1 Distribution', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Parameter distributions ---\n",
    "params = ['lr', 'batch_size', 'cotrain_epochs', 'finetune_patience', 'weight_decay', 'warmup_ratio']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for ax, param in zip(axes.flat, params):\n",
    "    values = [s['best_params'][param] for s in done_studies if param in s['best_params']]\n",
    "    if not values:\n",
    "        continue\n",
    "\n",
    "    if param == 'lr':\n",
    "        log_values = [np.log10(v) for v in values]\n",
    "        ax.hist(log_values, bins=15, alpha=0.7, color='tab:blue', edgecolor='white')\n",
    "        ax.set_xlabel(f'{param} (log10)')\n",
    "    elif param == 'batch_size':\n",
    "        from collections import Counter\n",
    "        counts = Counter(values)\n",
    "        categories = [8, 16, 32, 64]\n",
    "        bar_counts = [counts.get(c, 0) for c in categories]\n",
    "        ax.bar([str(c) for c in categories], bar_counts, alpha=0.7,\n",
    "               color='tab:blue', edgecolor='white')\n",
    "        ax.set_xlabel(param)\n",
    "    else:\n",
    "        ax.hist(values, bins=15, alpha=0.7, color='tab:blue', edgecolor='white')\n",
    "        ax.set_xlabel(param)\n",
    "\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'Best {param} across studies')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Optimal Hyperparameter Distributions (120 Studies)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "Use the optimized hyperparameters to run the final 120 experiments.\n",
    "The `load_best_params()` function loads all `best_params.json` files\n",
    "into a dict keyed by `(event, budget, seed_set)`.\n",
    "\n",
    "### CLI equivalent\n",
    "\n",
    "```bash\n",
    "# Run all 120 Optuna studies\n",
    "python -m lg_cotrain.optuna_per_experiment --n-trials 15 --num-gpus 2\n",
    "\n",
    "# Run a subset\n",
    "python -m lg_cotrain.optuna_per_experiment --n-trials 15 --num-gpus 2 \\\n",
    "    --events hurricane_harvey_2017 kerala_floods_2018 --budgets 50 --seed-sets 1\n",
    "```\n",
    "\n",
    "### Using optimized hyperparameters for final experiments\n",
    "\n",
    "```python\n",
    "from lg_cotrain.optuna_per_experiment import load_best_params\n",
    "\n",
    "best = load_best_params(\"results/optuna/per_experiment\")\n",
    "params = best[(\"hurricane_harvey_2017\", 50, 1)][\"best_params\"]\n",
    "# params = {\"lr\": 0.0003, \"batch_size\": 16, ...}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
