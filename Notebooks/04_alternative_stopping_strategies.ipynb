{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# LG-CoTrain: Alternative Early Stopping — Quick Comparison\n",
    "\n",
    "This notebook runs a **fast comparison** of all 6 stopping strategies using only\n",
    "**budget=50, seed set 1** across all 10 events — the minimum number of experiments\n",
    "needed to see whether any strategy is consistently better.\n",
    "\n",
    "| Strategy | Key Idea |\n",
    "|---|---|\n",
    "| `baseline` | Original: stop when ensemble macro-F1 plateaus for `patience` epochs |\n",
    "| `no_early_stopping` | Run all `finetune_max_epochs`; restore best-ever checkpoint (upper bound) |\n",
    "| `per_class_patience` | Stop only when **every** class F1 has individually plateaued |\n",
    "| `weighted_macro_f1` | Weight rare classes more in the stopping metric |\n",
    "| `balanced_dev` | Resample dev set to equal class sizes for the stopping signal |\n",
    "| `scaled_threshold` | Require a larger improvement delta for highly imbalanced events |\n",
    "\n",
    "**Total experiments**: 6 strategies × 10 events × 1 budget × 1 seed = **60 runs**\n",
    "(vs 360 for a full 3-seed, 4-budget sweep).\n",
    "\n",
    "Results are stored in `results/quick-stop-{strategy}/` to keep them separate from\n",
    "full-run results. See **notebook 05** for the complete sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: D:\\Workspace\\Co-Training\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _find_repo_root(marker: str = \"lg_cotrain\") -> Path:\n",
    "    for candidate in [Path().resolve()] + list(Path().resolve().parents):\n",
    "        if (candidate / marker).is_dir():\n",
    "            return candidate\n",
    "    raise RuntimeError(\n",
    "        f\"Cannot find repo root: no ancestor directory contains '{marker}/'. \"\n",
    "        \"Run the notebook from inside the repository.\"\n",
    "    )\n",
    "\n",
    "\n",
    "repo_root = _find_repo_root()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lg_cotrain.run_all import run_all_experiments\n",
    "\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"Track global progress across all strategies × events × budgets × seeds.\"\"\"\n",
    "\n",
    "    def __init__(self, total: int, already_done: int, start_time: float):\n",
    "        self.total = total\n",
    "        self.done = already_done\n",
    "        self.start_time = start_time\n",
    "\n",
    "    def update(self, event, budget, seed_set, status):\n",
    "        self.done += 1\n",
    "        elapsed = time.time() - self.start_time\n",
    "        pct = 100.0 * self.done / self.total if self.total else 0\n",
    "        elapsed_h = elapsed / 3600\n",
    "        remaining = self.total - self.done\n",
    "        eta_h = (elapsed / self.done) * remaining / 3600 if self.done > 0 else 0\n",
    "        print(\n",
    "            f\"  [PROGRESS] {self.done}/{self.total} ({pct:.1f}%)\"\n",
    "            f\"  |  Elapsed: {elapsed_h:.2f}h  |  ETA: {eta_h:.2f}h  |  {status}\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(f\"Repo root: {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategies : ['baseline', 'no_early_stopping', 'per_class_patience', 'weighted_macro_f1', 'balanced_dev', 'scaled_threshold']\n",
      "Events     : ['california_wildfires_2018', 'canada_wildfires_2016', 'cyclone_idai_2019', 'hurricane_dorian_2019', 'hurricane_florence_2018', 'hurricane_harvey_2017', 'hurricane_irma_2017', 'hurricane_maria_2017', 'kaikoura_earthquake_2016', 'kerala_floods_2018']\n",
      "Budget     : [50]  |  Seed sets: [1]\n",
      "Total runs : 60\n",
      "\n",
      "  baseline                  → D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-baseline\n",
      "  no_early_stopping         → D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-no_early_stopping\n",
      "  per_class_patience        → D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-per_class_patience\n",
      "  weighted_macro_f1         → D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-weighted_macro_f1\n",
      "  balanced_dev              → D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-balanced_dev\n",
      "  scaled_threshold          → D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-scaled_threshold\n"
     ]
    }
   ],
   "source": [
    "# ---- Configuration ----\n",
    "\n",
    "PSEUDO_LABEL_SOURCE = \"gpt-4o\"\n",
    "\n",
    "# Quick-run scope: 50 labels/class, seed set 1 only\n",
    "RUN_BUDGETS  = [50]\n",
    "RUN_SEEDS    = [1]\n",
    "\n",
    "STRATEGIES = [\n",
    "    \"baseline\",\n",
    "    \"no_early_stopping\",\n",
    "    \"per_class_patience\",\n",
    "    \"weighted_macro_f1\",\n",
    "    \"balanced_dev\",\n",
    "    \"scaled_threshold\",\n",
    "]\n",
    "\n",
    "DATA_ROOT = str(repo_root / \"data\")\n",
    "\n",
    "# Discover all events\n",
    "TARGET_EVENTS = sorted(\n",
    "    p.name for p in (Path(DATA_ROOT) / \"original\").iterdir() if p.is_dir()\n",
    ")\n",
    "\n",
    "# Each strategy gets its own results sub-folder\n",
    "STRATEGY_RESULTS_ROOTS = {\n",
    "    s: str(repo_root / \"results\" / f\"{PSEUDO_LABEL_SOURCE}-quick-stop-{s}\")\n",
    "    for s in STRATEGIES\n",
    "}\n",
    "\n",
    "total_runs = len(STRATEGIES) * len(TARGET_EVENTS) * len(RUN_BUDGETS) * len(RUN_SEEDS)\n",
    "print(f\"Strategies : {STRATEGIES}\")\n",
    "print(f\"Events     : {TARGET_EVENTS}\")\n",
    "print(f\"Budget     : {RUN_BUDGETS}  |  Seed sets: {RUN_SEEDS}\")\n",
    "print(f\"Total runs : {total_runs}\")\n",
    "print()\n",
    "for s, r in STRATEGY_RESULTS_ROOTS.items():\n",
    "    print(f\"  {s:<25} → {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Running Experiments\n",
    "\n",
    "Each cell runs all events for one budget × seed combination per strategy.\n",
    "If the cell crashes or is interrupted, re-run it — existing `metrics.json` files\n",
    "are automatically skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments : 60\n",
      "Already completed : 0\n",
      "Remaining         : 60\n",
      "\n",
      "\n",
      "=================================================================\n",
      "Strategy: baseline\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] budget=50, seed=1 -- starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 16:27:59,014 - lg_cotrain - INFO - Starting LG-CoTrain: event=california_wildfires_2018, budget=50, seed_set=1\n",
      "2026-02-19 16:27:59,047 - lg_cotrain - INFO - Detected 10 classes for event california_wildfires_2018: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'missing_or_found_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-19 16:27:59,057 - lg_cotrain - INFO - D_l1: 250, D_l2: 250, D_LG: 4663\n",
      "2026-02-19 16:27:59,059 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1120.39it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1129.41it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 16:28:18,900 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1094, mean_prob2=0.0976\n",
      "2026-02-19 16:28:36,862 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1068, mean_prob2=0.1102\n",
      "2026-02-19 16:28:54,902 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1159, mean_prob2=0.1240\n",
      "2026-02-19 16:29:12,952 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1228, mean_prob2=0.1372\n",
      "2026-02-19 16:29:31,145 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1356, mean_prob2=0.1524\n",
      "2026-02-19 16:29:49,363 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1455, mean_prob2=0.1813\n",
      "2026-02-19 16:30:07,690 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1601, mean_prob2=0.2031\n",
      "2026-02-19 16:30:07,690 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1555, range=[0.0491, 0.3370]\n",
      "2026-02-19 16:30:07,690 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1002, range=[0.0098, 0.2398]\n",
      "2026-02-19 16:30:07,690 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1180.76it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1043.80it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 16:31:13,070 - lg_cotrain - INFO - Phase 2 epoch 1/10: loss1=0.1184, loss2=0.1841, dev_macro_f1=0.4107, dev_err=31.25%\n",
      "2026-02-19 16:32:17,432 - lg_cotrain - INFO - Phase 2 epoch 2/10: loss1=0.0350, loss2=0.1152, dev_macro_f1=0.5351, dev_err=27.13%\n",
      "2026-02-19 16:33:21,750 - lg_cotrain - INFO - Phase 2 epoch 3/10: loss1=0.0144, loss2=0.0960, dev_macro_f1=0.5757, dev_err=25.80%\n",
      "2026-02-19 16:34:30,690 - lg_cotrain - INFO - Phase 2 epoch 4/10: loss1=0.0081, loss2=0.0665, dev_macro_f1=0.5930, dev_err=27.66%\n",
      "2026-02-19 16:35:35,723 - lg_cotrain - INFO - Phase 2 epoch 5/10: loss1=0.0061, loss2=0.0610, dev_macro_f1=0.5833, dev_err=26.99%\n",
      "2026-02-19 16:36:39,940 - lg_cotrain - INFO - Phase 2 epoch 6/10: loss1=0.0047, loss2=0.0436, dev_macro_f1=0.5889, dev_err=27.39%\n",
      "2026-02-19 16:37:44,229 - lg_cotrain - INFO - Phase 2 epoch 7/10: loss1=0.0054, loss2=0.0299, dev_macro_f1=0.5944, dev_err=26.60%\n",
      "2026-02-19 16:38:48,443 - lg_cotrain - INFO - Phase 2 epoch 8/10: loss1=0.0075, loss2=0.0212, dev_macro_f1=0.5881, dev_err=27.93%\n",
      "2026-02-19 16:39:52,689 - lg_cotrain - INFO - Phase 2 epoch 9/10: loss1=0.0074, loss2=0.0244, dev_macro_f1=0.5868, dev_err=27.53%\n",
      "2026-02-19 16:40:56,925 - lg_cotrain - INFO - Phase 2 epoch 10/10: loss1=0.0061, loss2=0.0207, dev_macro_f1=0.5936, dev_err=26.20%\n",
      "2026-02-19 16:40:56,926 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-19 16:41:01,846 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.5138, dev_err=47.61%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:41:06,770 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6038, dev_err=32.58%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:41:11,685 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6235, dev_err=31.38%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:41:16,613 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6237, dev_err=31.25%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:41:21,545 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.6335, dev_err=31.12%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:41:26,461 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.6367, dev_err=30.85%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:41:31,376 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6303, dev_err=30.98%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 16:41:36,284 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6307, dev_err=30.05%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 16:41:41,196 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6269, dev_err=31.12%, es_counter1=3, es_counter2=3\n",
      "2026-02-19 16:41:46,102 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6209, dev_err=31.52%, es_counter1=4, es_counter2=4\n",
      "2026-02-19 16:41:51,006 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.6221, dev_err=31.52%, es_counter1=5, es_counter2=5\n",
      "2026-02-19 16:41:51,006 - lg_cotrain - INFO - Early stopping at epoch 11\n",
      "2026-02-19 16:41:51,018 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-19 16:41:58,309 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-baseline\\california_wildfires_2018\\50_set1\\metrics.json\n",
      "2026-02-19 16:41:58,310 - lg_cotrain - INFO - Test error rate: 29.91%, Test macro-F1: 0.6379, Test ECE: 0.0991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] budget=50, seed=1 -- done (macro_f1=0.6379)\n",
      "  [PROGRESS] 1/60 (1.7%)  |  Elapsed: 0.23h  |  ETA: 13.84h  |  done\n",
      "\n",
      "Batch complete: 1 ran, 0 skipped, 0 failed (840.6s total)\n",
      "[1/1] budget=50, seed=1 -- starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 16:41:58,889 - lg_cotrain - INFO - Starting LG-CoTrain: event=canada_wildfires_2016, budget=50, seed_set=1\n",
      "2026-02-19 16:41:58,904 - lg_cotrain - INFO - Detected 8 classes for event canada_wildfires_2016: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-19 16:41:58,909 - lg_cotrain - INFO - D_l1: 182, D_l2: 182, D_LG: 1205\n",
      "2026-02-19 16:41:58,911 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1084.91it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1127.53it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 16:42:05,986 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1182, mean_prob2=0.1448\n",
      "2026-02-19 16:42:11,827 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1316, mean_prob2=0.1499\n",
      "2026-02-19 16:42:17,671 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1623, mean_prob2=0.1552\n",
      "2026-02-19 16:42:23,504 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1922, mean_prob2=0.1599\n",
      "2026-02-19 16:42:29,342 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1978, mean_prob2=0.1762\n",
      "2026-02-19 16:42:35,184 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.2282, mean_prob2=0.1943\n",
      "2026-02-19 16:42:41,044 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.2483, mean_prob2=0.2102\n",
      "2026-02-19 16:42:41,044 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.2346, range=[0.0741, 0.4128]\n",
      "2026-02-19 16:42:41,044 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1357, range=[0.0255, 0.2135]\n",
      "2026-02-19 16:42:41,044 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1166.40it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1103.92it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 16:42:58,773 - lg_cotrain - INFO - Phase 2 epoch 1/10: loss1=0.2098, loss2=0.3394, dev_macro_f1=0.1618, dev_err=45.18%\n",
      "2026-02-19 16:43:15,386 - lg_cotrain - INFO - Phase 2 epoch 2/10: loss1=0.1067, loss2=0.2221, dev_macro_f1=0.3310, dev_err=35.09%\n",
      "2026-02-19 16:43:31,991 - lg_cotrain - INFO - Phase 2 epoch 3/10: loss1=0.0581, loss2=0.1545, dev_macro_f1=0.4099, dev_err=27.19%\n",
      "2026-02-19 16:43:48,598 - lg_cotrain - INFO - Phase 2 epoch 4/10: loss1=0.0328, loss2=0.1136, dev_macro_f1=0.4618, dev_err=23.25%\n",
      "2026-02-19 16:44:05,206 - lg_cotrain - INFO - Phase 2 epoch 5/10: loss1=0.0210, loss2=0.0874, dev_macro_f1=0.4710, dev_err=25.44%\n",
      "2026-02-19 16:44:21,825 - lg_cotrain - INFO - Phase 2 epoch 6/10: loss1=0.0150, loss2=0.0709, dev_macro_f1=0.4965, dev_err=23.68%\n",
      "2026-02-19 16:44:38,424 - lg_cotrain - INFO - Phase 2 epoch 7/10: loss1=0.0101, loss2=0.0551, dev_macro_f1=0.5292, dev_err=23.68%\n",
      "2026-02-19 16:44:55,031 - lg_cotrain - INFO - Phase 2 epoch 8/10: loss1=0.0071, loss2=0.0446, dev_macro_f1=0.5181, dev_err=25.44%\n",
      "2026-02-19 16:45:11,638 - lg_cotrain - INFO - Phase 2 epoch 9/10: loss1=0.0077, loss2=0.0321, dev_macro_f1=0.5111, dev_err=24.56%\n",
      "2026-02-19 16:45:28,233 - lg_cotrain - INFO - Phase 2 epoch 10/10: loss1=0.0071, loss2=0.0290, dev_macro_f1=0.5112, dev_err=25.88%\n",
      "2026-02-19 16:45:28,233 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-19 16:45:30,769 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.4831, dev_err=32.46%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:45:33,310 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.5569, dev_err=22.81%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:45:35,847 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6065, dev_err=19.74%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:45:38,372 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.5805, dev_err=22.37%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 16:45:40,901 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.5945, dev_err=20.18%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 16:45:43,424 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.5971, dev_err=20.61%, es_counter1=3, es_counter2=3\n",
      "2026-02-19 16:45:45,963 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6524, dev_err=21.49%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:45:48,487 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6414, dev_err=21.05%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 16:45:51,032 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6544, dev_err=19.74%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:45:53,577 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6995, dev_err=18.86%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:45:56,100 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.6900, dev_err=19.30%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 16:45:58,629 - lg_cotrain - INFO - Phase 3 epoch 12: dev_macro_f1=0.6847, dev_err=19.30%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 16:46:01,161 - lg_cotrain - INFO - Phase 3 epoch 13: dev_macro_f1=0.6811, dev_err=19.74%, es_counter1=3, es_counter2=3\n",
      "2026-02-19 16:46:03,688 - lg_cotrain - INFO - Phase 3 epoch 14: dev_macro_f1=0.6765, dev_err=20.18%, es_counter1=4, es_counter2=4\n",
      "2026-02-19 16:46:06,216 - lg_cotrain - INFO - Phase 3 epoch 15: dev_macro_f1=0.6709, dev_err=20.61%, es_counter1=5, es_counter2=5\n",
      "2026-02-19 16:46:06,216 - lg_cotrain - INFO - Early stopping at epoch 15\n",
      "2026-02-19 16:46:06,226 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-19 16:46:08,443 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-baseline\\canada_wildfires_2016\\50_set1\\metrics.json\n",
      "2026-02-19 16:46:08,443 - lg_cotrain - INFO - Test error rate: 21.35%, Test macro-F1: 0.6098, Test ECE: 0.1017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] budget=50, seed=1 -- done (macro_f1=0.6098)\n",
      "  [PROGRESS] 2/60 (3.3%)  |  Elapsed: 0.30h  |  ETA: 8.82h  |  done\n",
      "\n",
      "Batch complete: 1 ran, 0 skipped, 0 failed (250.1s total)\n",
      "[1/1] budget=50, seed=1 -- starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 16:46:08,997 - lg_cotrain - INFO - Starting LG-CoTrain: event=cyclone_idai_2019, budget=50, seed_set=1\n",
      "2026-02-19 16:46:09,023 - lg_cotrain - INFO - Detected 10 classes for event cyclone_idai_2019: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'missing_or_found_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-19 16:46:09,028 - lg_cotrain - INFO - D_l1: 227, D_l2: 226, D_LG: 2300\n",
      "2026-02-19 16:46:09,028 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1129.14it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1151.38it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 16:46:20,386 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1059, mean_prob2=0.1254\n",
      "2026-02-19 16:46:30,480 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1102, mean_prob2=0.1264\n",
      "2026-02-19 16:46:40,557 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1219, mean_prob2=0.1199\n",
      "2026-02-19 16:46:50,649 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1297, mean_prob2=0.1239\n",
      "2026-02-19 16:47:00,753 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1381, mean_prob2=0.1335\n",
      "2026-02-19 16:47:10,875 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1477, mean_prob2=0.1512\n",
      "2026-02-19 16:47:20,981 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1627, mean_prob2=0.1611\n",
      "2026-02-19 16:47:20,981 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1583, range=[0.0598, 0.2795]\n",
      "2026-02-19 16:47:20,987 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1093, range=[0.0185, 0.3042]\n",
      "2026-02-19 16:47:20,988 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1179.20it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1098.49it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 16:47:53,947 - lg_cotrain - INFO - Phase 2 epoch 1/10: loss1=0.1521, loss2=0.2203, dev_macro_f1=0.2736, dev_err=32.17%\n",
      "2026-02-19 16:48:25,833 - lg_cotrain - INFO - Phase 2 epoch 2/10: loss1=0.0539, loss2=0.1335, dev_macro_f1=0.3192, dev_err=30.42%\n",
      "2026-02-19 16:48:57,501 - lg_cotrain - INFO - Phase 2 epoch 3/10: loss1=0.0282, loss2=0.1042, dev_macro_f1=0.3372, dev_err=28.18%\n",
      "2026-02-19 16:49:29,202 - lg_cotrain - INFO - Phase 2 epoch 4/10: loss1=0.0202, loss2=0.0725, dev_macro_f1=0.3460, dev_err=28.18%\n",
      "2026-02-19 16:50:00,898 - lg_cotrain - INFO - Phase 2 epoch 5/10: loss1=0.0145, loss2=0.0497, dev_macro_f1=0.3595, dev_err=27.18%\n",
      "2026-02-19 16:50:32,601 - lg_cotrain - INFO - Phase 2 epoch 6/10: loss1=0.0104, loss2=0.0380, dev_macro_f1=0.3709, dev_err=27.93%\n",
      "2026-02-19 16:51:04,346 - lg_cotrain - INFO - Phase 2 epoch 7/10: loss1=0.0089, loss2=0.0303, dev_macro_f1=0.4148, dev_err=25.94%\n",
      "2026-02-19 16:51:36,056 - lg_cotrain - INFO - Phase 2 epoch 8/10: loss1=0.0082, loss2=0.0351, dev_macro_f1=0.4041, dev_err=26.43%\n",
      "2026-02-19 16:52:07,754 - lg_cotrain - INFO - Phase 2 epoch 9/10: loss1=0.0072, loss2=0.0414, dev_macro_f1=0.4325, dev_err=25.94%\n",
      "2026-02-19 16:52:39,463 - lg_cotrain - INFO - Phase 2 epoch 10/10: loss1=0.0122, loss2=0.0275, dev_macro_f1=0.4116, dev_err=26.68%\n",
      "2026-02-19 16:52:39,463 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-19 16:52:43,037 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.3740, dev_err=50.37%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:52:46,600 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.5028, dev_err=28.43%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:52:50,165 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.5127, dev_err=26.18%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:52:53,709 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.5053, dev_err=27.68%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 16:52:57,274 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.5242, dev_err=29.18%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 16:53:00,828 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.5203, dev_err=28.43%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 16:53:04,374 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.5086, dev_err=30.42%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 16:53:07,926 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.4998, dev_err=30.67%, es_counter1=3, es_counter2=3\n",
      "2026-02-19 16:53:11,473 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.5037, dev_err=30.67%, es_counter1=4, es_counter2=4\n",
      "2026-02-19 16:53:15,024 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.5086, dev_err=30.42%, es_counter1=5, es_counter2=5\n",
      "2026-02-19 16:53:15,025 - lg_cotrain - INFO - Early stopping at epoch 10\n",
      "2026-02-19 16:53:15,034 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-19 16:53:18,925 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-baseline\\cyclone_idai_2019\\50_set1\\metrics.json\n",
      "2026-02-19 16:53:18,926 - lg_cotrain - INFO - Test error rate: 29.01%, Test macro-F1: 0.6427, Test ECE: 0.0650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] budget=50, seed=1 -- done (macro_f1=0.6427)\n",
      "  [PROGRESS] 3/60 (5.0%)  |  Elapsed: 0.42h  |  ETA: 8.05h  |  done\n",
      "\n",
      "Batch complete: 1 ran, 0 skipped, 0 failed (430.5s total)\n",
      "[1/1] budget=50, seed=1 -- starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 16:53:19,571 - lg_cotrain - INFO - Starting LG-CoTrain: event=hurricane_dorian_2019, budget=50, seed_set=1\n",
      "2026-02-19 16:53:19,606 - lg_cotrain - INFO - Detected 9 classes for event hurricane_dorian_2019: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-19 16:53:19,615 - lg_cotrain - INFO - D_l1: 221, D_l2: 221, D_LG: 4887\n",
      "2026-02-19 16:53:19,615 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|████████████████| 199/199 [00:00<00:00, 996.39it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1175.32it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 16:53:39,615 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1125, mean_prob2=0.1088\n",
      "2026-02-19 16:53:58,380 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1204, mean_prob2=0.1177\n",
      "2026-02-19 16:54:17,090 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1239, mean_prob2=0.1280\n",
      "2026-02-19 16:54:35,841 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1193, mean_prob2=0.1367\n",
      "2026-02-19 16:54:54,579 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1483, mean_prob2=0.1469\n",
      "2026-02-19 16:55:13,339 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1546, mean_prob2=0.1569\n",
      "2026-02-19 16:55:32,075 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1808, mean_prob2=0.1699\n",
      "2026-02-19 16:55:32,076 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1694, range=[0.0634, 0.3721]\n",
      "2026-02-19 16:55:32,077 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1108, range=[0.0119, 0.2390]\n",
      "2026-02-19 16:55:32,077 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1145.09it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1139.60it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 16:56:40,158 - lg_cotrain - INFO - Phase 2 epoch 1/10: loss1=0.1336, loss2=0.1892, dev_macro_f1=0.4689, dev_err=38.02%\n",
      "2026-02-19 16:57:46,905 - lg_cotrain - INFO - Phase 2 epoch 2/10: loss1=0.0317, loss2=0.1166, dev_macro_f1=0.4711, dev_err=37.50%\n",
      "2026-02-19 16:58:53,627 - lg_cotrain - INFO - Phase 2 epoch 3/10: loss1=0.0141, loss2=0.0978, dev_macro_f1=0.4672, dev_err=40.08%\n",
      "2026-02-19 17:00:00,371 - lg_cotrain - INFO - Phase 2 epoch 4/10: loss1=0.0080, loss2=0.0684, dev_macro_f1=0.4863, dev_err=38.27%\n",
      "2026-02-19 17:01:07,137 - lg_cotrain - INFO - Phase 2 epoch 5/10: loss1=0.0053, loss2=0.0446, dev_macro_f1=0.4974, dev_err=39.18%\n",
      "2026-02-19 17:02:13,915 - lg_cotrain - INFO - Phase 2 epoch 6/10: loss1=0.0044, loss2=0.0362, dev_macro_f1=0.5108, dev_err=36.98%\n",
      "2026-02-19 17:03:20,707 - lg_cotrain - INFO - Phase 2 epoch 7/10: loss1=0.0051, loss2=0.0247, dev_macro_f1=0.4790, dev_err=39.69%\n",
      "2026-02-19 17:04:27,515 - lg_cotrain - INFO - Phase 2 epoch 8/10: loss1=0.0086, loss2=0.0307, dev_macro_f1=0.4872, dev_err=37.63%\n",
      "2026-02-19 17:05:34,341 - lg_cotrain - INFO - Phase 2 epoch 9/10: loss1=0.0092, loss2=0.0163, dev_macro_f1=0.4883, dev_err=39.05%\n",
      "2026-02-19 17:06:41,132 - lg_cotrain - INFO - Phase 2 epoch 10/10: loss1=0.0088, loss2=0.0101, dev_macro_f1=0.4758, dev_err=39.43%\n",
      "2026-02-19 17:06:41,133 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-19 17:06:45,820 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.4795, dev_err=48.07%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:06:50,509 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6020, dev_err=36.73%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:06:55,191 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.5911, dev_err=38.79%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 17:06:59,863 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.5931, dev_err=37.76%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 17:07:04,543 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.5915, dev_err=36.73%, es_counter1=3, es_counter2=3\n",
      "2026-02-19 17:07:09,221 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.5867, dev_err=38.14%, es_counter1=4, es_counter2=4\n",
      "2026-02-19 17:07:13,896 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.5928, dev_err=38.02%, es_counter1=5, es_counter2=5\n",
      "2026-02-19 17:07:13,897 - lg_cotrain - INFO - Early stopping at epoch 7\n",
      "2026-02-19 17:07:13,906 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-19 17:07:21,377 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-baseline\\hurricane_dorian_2019\\50_set1\\metrics.json\n",
      "2026-02-19 17:07:21,377 - lg_cotrain - INFO - Test error rate: 36.47%, Test macro-F1: 0.5864, Test ECE: 0.0609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] budget=50, seed=1 -- done (macro_f1=0.5864)\n",
      "  [PROGRESS] 4/60 (6.7%)  |  Elapsed: 0.66h  |  ETA: 9.21h  |  done\n",
      "\n",
      "Batch complete: 1 ran, 0 skipped, 0 failed (842.5s total)\n",
      "[1/1] budget=50, seed=1 -- starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 17:07:22,077 - lg_cotrain - INFO - Starting LG-CoTrain: event=hurricane_florence_2018, budget=50, seed_set=1\n",
      "2026-02-19 17:07:22,107 - lg_cotrain - INFO - Detected 9 classes for event hurricane_florence_2018: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-19 17:07:22,115 - lg_cotrain - INFO - D_l1: 219, D_l2: 219, D_LG: 3946\n",
      "2026-02-19 17:07:22,116 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1179.48it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1201.99it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 17:07:38,908 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1223, mean_prob2=0.1138\n",
      "2026-02-19 17:07:54,431 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1249, mean_prob2=0.1327\n",
      "2026-02-19 17:08:09,925 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1306, mean_prob2=0.1405\n",
      "2026-02-19 17:08:25,449 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1450, mean_prob2=0.1533\n",
      "2026-02-19 17:08:40,937 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1470, mean_prob2=0.1611\n",
      "2026-02-19 17:08:56,426 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1771, mean_prob2=0.1697\n",
      "2026-02-19 17:09:11,917 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1823, mean_prob2=0.1817\n",
      "2026-02-19 17:09:11,919 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1799, range=[0.0676, 0.3918]\n",
      "2026-02-19 17:09:11,919 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1190, range=[0.0099, 0.2751]\n",
      "2026-02-19 17:09:11,920 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1140.28it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1174.04it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 17:10:07,060 - lg_cotrain - INFO - Phase 2 epoch 1/10: loss1=0.1539, loss2=0.2285, dev_macro_f1=0.6122, dev_err=25.67%\n",
      "2026-02-19 17:11:01,071 - lg_cotrain - INFO - Phase 2 epoch 2/10: loss1=0.0399, loss2=0.1211, dev_macro_f1=0.6370, dev_err=23.16%\n",
      "2026-02-19 17:11:55,011 - lg_cotrain - INFO - Phase 2 epoch 3/10: loss1=0.0168, loss2=0.0854, dev_macro_f1=0.6303, dev_err=24.57%\n",
      "2026-02-19 17:12:48,973 - lg_cotrain - INFO - Phase 2 epoch 4/10: loss1=0.0089, loss2=0.0648, dev_macro_f1=0.6435, dev_err=24.10%\n",
      "2026-02-19 17:13:42,958 - lg_cotrain - INFO - Phase 2 epoch 5/10: loss1=0.0062, loss2=0.0537, dev_macro_f1=0.6404, dev_err=23.32%\n",
      "2026-02-19 17:14:36,953 - lg_cotrain - INFO - Phase 2 epoch 6/10: loss1=0.0050, loss2=0.0294, dev_macro_f1=0.6550, dev_err=22.85%\n",
      "2026-02-19 17:15:30,933 - lg_cotrain - INFO - Phase 2 epoch 7/10: loss1=0.0048, loss2=0.0208, dev_macro_f1=0.6486, dev_err=23.94%\n",
      "2026-02-19 17:16:24,945 - lg_cotrain - INFO - Phase 2 epoch 8/10: loss1=0.0074, loss2=0.0269, dev_macro_f1=0.6474, dev_err=22.38%\n",
      "2026-02-19 17:17:18,954 - lg_cotrain - INFO - Phase 2 epoch 9/10: loss1=0.0098, loss2=0.0296, dev_macro_f1=0.6307, dev_err=24.41%\n",
      "2026-02-19 17:18:12,953 - lg_cotrain - INFO - Phase 2 epoch 10/10: loss1=0.0086, loss2=0.0283, dev_macro_f1=0.6454, dev_err=23.32%\n",
      "2026-02-19 17:18:12,954 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-19 17:18:17,160 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.6327, dev_err=30.36%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:18:21,364 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.7088, dev_err=23.79%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:18:25,570 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.7167, dev_err=20.50%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:18:29,752 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6967, dev_err=23.32%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 17:18:33,956 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.7000, dev_err=23.16%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 17:18:38,146 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.7084, dev_err=23.32%, es_counter1=3, es_counter2=3\n",
      "2026-02-19 17:18:42,339 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.7156, dev_err=22.69%, es_counter1=4, es_counter2=4\n",
      "2026-02-19 17:18:46,548 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.7204, dev_err=22.54%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:18:50,744 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.7198, dev_err=22.85%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 17:18:54,945 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.7133, dev_err=23.47%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 17:18:59,137 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.7090, dev_err=24.10%, es_counter1=3, es_counter2=3\n",
      "2026-02-19 17:19:03,332 - lg_cotrain - INFO - Phase 3 epoch 12: dev_macro_f1=0.7061, dev_err=24.10%, es_counter1=4, es_counter2=4\n",
      "2026-02-19 17:19:07,523 - lg_cotrain - INFO - Phase 3 epoch 13: dev_macro_f1=0.7086, dev_err=23.79%, es_counter1=5, es_counter2=5\n",
      "2026-02-19 17:19:07,524 - lg_cotrain - INFO - Early stopping at epoch 13\n",
      "2026-02-19 17:19:07,532 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-19 17:19:13,680 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-baseline\\hurricane_florence_2018\\50_set1\\metrics.json\n",
      "2026-02-19 17:19:13,681 - lg_cotrain - INFO - Test error rate: 24.25%, Test macro-F1: 0.6995, Test ECE: 0.0740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] budget=50, seed=1 -- done (macro_f1=0.6995)\n",
      "  [PROGRESS] 5/60 (8.3%)  |  Elapsed: 0.86h  |  ETA: 9.41h  |  done\n",
      "\n",
      "Batch complete: 1 ran, 0 skipped, 0 failed (712.3s total)\n",
      "[1/1] budget=50, seed=1 -- starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 17:19:14,295 - lg_cotrain - INFO - Starting LG-CoTrain: event=hurricane_harvey_2017, budget=50, seed_set=1\n",
      "2026-02-19 17:19:14,331 - lg_cotrain - INFO - Detected 9 classes for event hurricane_harvey_2017: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-19 17:19:14,341 - lg_cotrain - INFO - D_l1: 225, D_l2: 225, D_LG: 5928\n",
      "2026-02-19 17:19:14,342 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1222.80it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1153.28it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 17:19:37,795 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1204, mean_prob2=0.1221\n",
      "2026-02-19 17:20:00,047 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1169, mean_prob2=0.1314\n",
      "2026-02-19 17:20:22,222 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1302, mean_prob2=0.1416\n",
      "2026-02-19 17:20:44,408 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1331, mean_prob2=0.1455\n",
      "2026-02-19 17:21:06,616 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1387, mean_prob2=0.1488\n",
      "2026-02-19 17:21:28,783 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1406, mean_prob2=0.1501\n",
      "2026-02-19 17:21:50,972 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1531, mean_prob2=0.1518\n",
      "2026-02-19 17:21:50,973 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1524, range=[0.0452, 0.2681]\n",
      "2026-02-19 17:21:50,974 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1163, range=[0.0127, 0.2597]\n",
      "2026-02-19 17:21:50,974 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1126.84it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1157.88it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 17:23:12,747 - lg_cotrain - INFO - Phase 2 epoch 1/10: loss1=0.1276, loss2=0.1716, dev_macro_f1=0.5869, dev_err=35.63%\n",
      "2026-02-19 17:24:33,296 - lg_cotrain - INFO - Phase 2 epoch 2/10: loss1=0.0340, loss2=0.1196, dev_macro_f1=0.5856, dev_err=35.41%\n",
      "2026-02-19 17:25:53,778 - lg_cotrain - INFO - Phase 2 epoch 3/10: loss1=0.0127, loss2=0.0931, dev_macro_f1=0.6020, dev_err=34.23%\n",
      "2026-02-19 17:27:16,768 - lg_cotrain - INFO - Phase 2 epoch 4/10: loss1=0.0062, loss2=0.0732, dev_macro_f1=0.6070, dev_err=34.55%\n",
      "2026-02-19 17:28:38,233 - lg_cotrain - INFO - Phase 2 epoch 5/10: loss1=0.0050, loss2=0.0545, dev_macro_f1=0.6163, dev_err=33.91%\n",
      "2026-02-19 17:30:00,130 - lg_cotrain - INFO - Phase 2 epoch 6/10: loss1=0.0045, loss2=0.0380, dev_macro_f1=0.6087, dev_err=34.23%\n",
      "2026-02-19 17:31:23,674 - lg_cotrain - INFO - Phase 2 epoch 7/10: loss1=0.0056, loss2=0.0297, dev_macro_f1=0.5974, dev_err=35.74%\n",
      "2026-02-19 17:32:47,039 - lg_cotrain - INFO - Phase 2 epoch 8/10: loss1=0.0082, loss2=0.0352, dev_macro_f1=0.6032, dev_err=34.12%\n",
      "2026-02-19 17:34:09,434 - lg_cotrain - INFO - Phase 2 epoch 9/10: loss1=0.0073, loss2=0.0227, dev_macro_f1=0.6188, dev_err=32.62%\n",
      "2026-02-19 17:35:31,673 - lg_cotrain - INFO - Phase 2 epoch 10/10: loss1=0.0105, loss2=0.0254, dev_macro_f1=0.6128, dev_err=34.23%\n",
      "2026-02-19 17:35:31,674 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-19 17:35:36,992 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.6334, dev_err=33.48%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:35:42,345 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6516, dev_err=31.32%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:35:47,668 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6632, dev_err=31.11%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:35:52,938 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6630, dev_err=31.86%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 17:35:58,236 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.6641, dev_err=30.57%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:36:03,576 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.6598, dev_err=30.79%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 17:36:08,889 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6599, dev_err=30.89%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 17:36:14,192 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6536, dev_err=31.54%, es_counter1=3, es_counter2=3\n",
      "2026-02-19 17:36:19,482 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6598, dev_err=31.00%, es_counter1=4, es_counter2=4\n",
      "2026-02-19 17:36:24,768 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6638, dev_err=30.79%, es_counter1=5, es_counter2=5\n",
      "2026-02-19 17:36:24,770 - lg_cotrain - INFO - Early stopping at epoch 10\n",
      "2026-02-19 17:36:24,777 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-19 17:36:33,761 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-baseline\\hurricane_harvey_2017\\50_set1\\metrics.json\n",
      "2026-02-19 17:36:33,763 - lg_cotrain - INFO - Test error rate: 28.25%, Test macro-F1: 0.6787, Test ECE: 0.0886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] budget=50, seed=1 -- done (macro_f1=0.6787)\n",
      "  [PROGRESS] 6/60 (10.0%)  |  Elapsed: 1.14h  |  ETA: 10.30h  |  done\n",
      "\n",
      "Batch complete: 1 ran, 0 skipped, 0 failed (1040.1s total)\n",
      "[1/1] budget=50, seed=1 -- starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 17:36:34,425 - lg_cotrain - INFO - Starting LG-CoTrain: event=hurricane_irma_2017, budget=50, seed_set=1\n",
      "2026-02-19 17:36:34,463 - lg_cotrain - INFO - Detected 9 classes for event hurricane_irma_2017: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-19 17:36:34,474 - lg_cotrain - INFO - D_l1: 225, D_l2: 225, D_LG: 6129\n",
      "2026-02-19 17:36:34,476 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1124.00it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1104.08it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 17:36:58,662 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1219, mean_prob2=0.1217\n",
      "2026-02-19 17:37:21,670 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1249, mean_prob2=0.1295\n",
      "2026-02-19 17:37:45,735 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1296, mean_prob2=0.1368\n",
      "2026-02-19 17:38:10,019 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1395, mean_prob2=0.1491\n",
      "2026-02-19 17:38:34,292 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1457, mean_prob2=0.1408\n",
      "2026-02-19 17:38:58,607 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1555, mean_prob2=0.1540\n",
      "2026-02-19 17:39:22,967 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1667, mean_prob2=0.1601\n",
      "2026-02-19 17:39:22,968 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1634, range=[0.0631, 0.3256]\n",
      "2026-02-19 17:39:22,969 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1141, range=[0.0078, 0.3031]\n",
      "2026-02-19 17:39:22,969 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|████████████████| 199/199 [00:00<00:00, 949.64it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1075.54it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 17:40:54,528 - lg_cotrain - INFO - Phase 2 epoch 1/10: loss1=0.1339, loss2=0.1921, dev_macro_f1=0.5405, dev_err=39.67%\n",
      "2026-02-19 17:42:23,508 - lg_cotrain - INFO - Phase 2 epoch 2/10: loss1=0.0360, loss2=0.1294, dev_macro_f1=0.5972, dev_err=37.47%\n",
      "2026-02-19 17:43:52,002 - lg_cotrain - INFO - Phase 2 epoch 3/10: loss1=0.0147, loss2=0.0978, dev_macro_f1=0.5922, dev_err=38.52%\n",
      "2026-02-19 17:45:20,677 - lg_cotrain - INFO - Phase 2 epoch 4/10: loss1=0.0068, loss2=0.0708, dev_macro_f1=0.6043, dev_err=38.41%\n",
      "2026-02-19 17:46:45,360 - lg_cotrain - INFO - Phase 2 epoch 5/10: loss1=0.0046, loss2=0.0514, dev_macro_f1=0.6202, dev_err=36.43%\n",
      "2026-02-19 17:48:09,360 - lg_cotrain - INFO - Phase 2 epoch 6/10: loss1=0.0038, loss2=0.0395, dev_macro_f1=0.5999, dev_err=38.10%\n",
      "2026-02-19 17:49:33,639 - lg_cotrain - INFO - Phase 2 epoch 7/10: loss1=0.0055, loss2=0.0256, dev_macro_f1=0.5979, dev_err=37.79%\n",
      "2026-02-19 17:50:58,898 - lg_cotrain - INFO - Phase 2 epoch 8/10: loss1=0.0081, loss2=0.0202, dev_macro_f1=0.5877, dev_err=38.62%\n",
      "2026-02-19 17:52:26,300 - lg_cotrain - INFO - Phase 2 epoch 9/10: loss1=0.0103, loss2=0.0297, dev_macro_f1=0.5879, dev_err=38.62%\n",
      "2026-02-19 17:53:51,186 - lg_cotrain - INFO - Phase 2 epoch 10/10: loss1=0.0097, loss2=0.0233, dev_macro_f1=0.5869, dev_err=38.31%\n",
      "2026-02-19 17:53:51,187 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-19 17:53:56,778 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.6091, dev_err=38.20%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:54:02,523 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6295, dev_err=35.28%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:54:08,034 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6068, dev_err=37.47%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 17:54:13,618 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6397, dev_err=33.30%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:54:19,014 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.6339, dev_err=33.51%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 17:54:24,429 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.6353, dev_err=33.61%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 17:54:29,813 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6419, dev_err=33.40%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:54:35,279 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6430, dev_err=33.30%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:54:40,668 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6434, dev_err=33.19%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:54:46,092 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6461, dev_err=32.67%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 17:54:51,540 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.6433, dev_err=32.88%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 17:54:56,999 - lg_cotrain - INFO - Phase 3 epoch 12: dev_macro_f1=0.6444, dev_err=32.88%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 17:55:02,481 - lg_cotrain - INFO - Phase 3 epoch 13: dev_macro_f1=0.6364, dev_err=33.61%, es_counter1=3, es_counter2=3\n",
      "2026-02-19 17:55:07,840 - lg_cotrain - INFO - Phase 3 epoch 14: dev_macro_f1=0.6294, dev_err=34.34%, es_counter1=4, es_counter2=4\n",
      "2026-02-19 17:55:13,362 - lg_cotrain - INFO - Phase 3 epoch 15: dev_macro_f1=0.6342, dev_err=34.13%, es_counter1=5, es_counter2=5\n",
      "2026-02-19 17:55:13,362 - lg_cotrain - INFO - Early stopping at epoch 15\n",
      "2026-02-19 17:55:13,371 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-19 17:55:22,725 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-baseline\\hurricane_irma_2017\\50_set1\\metrics.json\n",
      "2026-02-19 17:55:22,725 - lg_cotrain - INFO - Test error rate: 33.73%, Test macro-F1: 0.6391, Test ECE: 0.1787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] budget=50, seed=1 -- done (macro_f1=0.6391)\n",
      "  [PROGRESS] 7/60 (11.7%)  |  Elapsed: 1.46h  |  ETA: 11.04h  |  done\n",
      "\n",
      "Batch complete: 1 ran, 0 skipped, 0 failed (1129.0s total)\n",
      "[1/1] budget=50, seed=1 -- starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 17:55:23,368 - lg_cotrain - INFO - Starting LG-CoTrain: event=hurricane_maria_2017, budget=50, seed_set=1\n",
      "2026-02-19 17:55:23,397 - lg_cotrain - INFO - Detected 9 classes for event hurricane_maria_2017: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-19 17:55:23,406 - lg_cotrain - INFO - D_l1: 225, D_l2: 225, D_LG: 4644\n",
      "2026-02-19 17:55:23,408 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1215.67it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1132.21it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 17:55:42,753 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1183, mean_prob2=0.1190\n",
      "2026-02-19 17:56:00,926 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1313, mean_prob2=0.1353\n",
      "2026-02-19 17:56:19,035 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1496, mean_prob2=0.1502\n",
      "2026-02-19 17:56:37,316 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1486, mean_prob2=0.1557\n",
      "2026-02-19 17:56:55,774 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1642, mean_prob2=0.1657\n",
      "2026-02-19 17:57:13,978 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1706, mean_prob2=0.1728\n",
      "2026-02-19 17:57:32,086 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1949, mean_prob2=0.1787\n",
      "2026-02-19 17:57:32,087 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1862, range=[0.0547, 0.3462]\n",
      "2026-02-19 17:57:32,088 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1198, range=[0.0144, 0.2616]\n",
      "2026-02-19 17:57:32,088 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1165.55it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1137.91it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 17:58:37,444 - lg_cotrain - INFO - Phase 2 epoch 1/10: loss1=0.1354, loss2=0.2104, dev_macro_f1=0.4885, dev_err=36.52%\n",
      "2026-02-19 17:59:41,052 - lg_cotrain - INFO - Phase 2 epoch 2/10: loss1=0.0457, loss2=0.1312, dev_macro_f1=0.6097, dev_err=32.75%\n",
      "2026-02-19 18:00:44,799 - lg_cotrain - INFO - Phase 2 epoch 3/10: loss1=0.0210, loss2=0.0945, dev_macro_f1=0.6437, dev_err=32.61%\n",
      "2026-02-19 18:01:48,502 - lg_cotrain - INFO - Phase 2 epoch 4/10: loss1=0.0107, loss2=0.0767, dev_macro_f1=0.6137, dev_err=33.02%\n",
      "2026-02-19 18:02:52,190 - lg_cotrain - INFO - Phase 2 epoch 5/10: loss1=0.0068, loss2=0.0494, dev_macro_f1=0.6240, dev_err=33.02%\n",
      "2026-02-19 18:03:55,867 - lg_cotrain - INFO - Phase 2 epoch 6/10: loss1=0.0051, loss2=0.0505, dev_macro_f1=0.6310, dev_err=32.48%\n",
      "2026-02-19 18:04:59,538 - lg_cotrain - INFO - Phase 2 epoch 7/10: loss1=0.0063, loss2=0.0526, dev_macro_f1=0.6171, dev_err=33.29%\n",
      "2026-02-19 18:06:03,226 - lg_cotrain - INFO - Phase 2 epoch 8/10: loss1=0.0062, loss2=0.0244, dev_macro_f1=0.6112, dev_err=33.96%\n",
      "2026-02-19 18:07:07,180 - lg_cotrain - INFO - Phase 2 epoch 9/10: loss1=0.0070, loss2=0.0144, dev_macro_f1=0.6191, dev_err=33.69%\n",
      "2026-02-19 18:08:11,478 - lg_cotrain - INFO - Phase 2 epoch 10/10: loss1=0.0102, loss2=0.0191, dev_macro_f1=0.6194, dev_err=33.56%\n",
      "2026-02-19 18:08:11,479 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-19 18:08:16,196 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.6334, dev_err=33.56%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 18:08:21,024 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6563, dev_err=31.13%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 18:08:25,839 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6501, dev_err=31.00%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 18:08:30,535 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6602, dev_err=28.71%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 18:08:35,222 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.6741, dev_err=27.76%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 18:08:39,985 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.6568, dev_err=29.25%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 18:08:44,656 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6587, dev_err=29.11%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 18:08:49,446 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6733, dev_err=28.17%, es_counter1=3, es_counter2=3\n",
      "2026-02-19 18:08:54,209 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6684, dev_err=28.03%, es_counter1=4, es_counter2=4\n",
      "2026-02-19 18:08:58,926 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6578, dev_err=28.84%, es_counter1=5, es_counter2=5\n",
      "2026-02-19 18:08:58,926 - lg_cotrain - INFO - Early stopping at epoch 10\n",
      "2026-02-19 18:08:58,939 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-19 18:09:06,283 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-baseline\\hurricane_maria_2017\\50_set1\\metrics.json\n",
      "2026-02-19 18:09:06,285 - lg_cotrain - INFO - Test error rate: 27.39%, Test macro-F1: 0.6801, Test ECE: 0.0742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] budget=50, seed=1 -- done (macro_f1=0.6801)\n",
      "  [PROGRESS] 8/60 (13.3%)  |  Elapsed: 1.69h  |  ETA: 10.96h  |  done\n",
      "\n",
      "Batch complete: 1 ran, 0 skipped, 0 failed (823.5s total)\n",
      "[1/1] budget=50, seed=1 -- starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 18:09:06,970 - lg_cotrain - INFO - Starting LG-CoTrain: event=kaikoura_earthquake_2016, budget=50, seed_set=1\n",
      "2026-02-19 18:09:06,981 - lg_cotrain - INFO - Detected 9 classes for event kaikoura_earthquake_2016: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-19 18:09:06,986 - lg_cotrain - INFO - D_l1: 209, D_l2: 208, D_LG: 1119\n",
      "2026-02-19 18:09:06,987 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1117.98it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1145.92it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 18:09:14,180 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1232, mean_prob2=0.1112\n",
      "2026-02-19 18:09:20,036 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1285, mean_prob2=0.1183\n",
      "2026-02-19 18:09:25,879 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1384, mean_prob2=0.1296\n",
      "2026-02-19 18:09:31,719 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1529, mean_prob2=0.1521\n",
      "2026-02-19 18:09:37,557 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1707, mean_prob2=0.1697\n",
      "2026-02-19 18:09:43,418 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1768, mean_prob2=0.1911\n",
      "2026-02-19 18:09:49,239 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.2155, mean_prob2=0.2162\n",
      "2026-02-19 18:09:49,240 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1960, range=[0.0581, 0.3656]\n",
      "2026-02-19 18:09:49,241 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1091, range=[0.0183, 0.2556]\n",
      "2026-02-19 18:09:49,242 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1133.89it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1139.20it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 18:10:06,010 - lg_cotrain - INFO - Phase 2 epoch 1/10: loss1=0.1984, loss2=0.3542, dev_macro_f1=0.2030, dev_err=51.79%\n",
      "2026-02-19 18:10:21,704 - lg_cotrain - INFO - Phase 2 epoch 2/10: loss1=0.1272, loss2=0.2661, dev_macro_f1=0.3821, dev_err=40.18%\n",
      "2026-02-19 18:10:37,451 - lg_cotrain - INFO - Phase 2 epoch 3/10: loss1=0.0705, loss2=0.1934, dev_macro_f1=0.5187, dev_err=33.04%\n",
      "2026-02-19 18:10:53,321 - lg_cotrain - INFO - Phase 2 epoch 4/10: loss1=0.0349, loss2=0.1357, dev_macro_f1=0.5217, dev_err=33.04%\n",
      "2026-02-19 18:11:09,152 - lg_cotrain - INFO - Phase 2 epoch 5/10: loss1=0.0194, loss2=0.0912, dev_macro_f1=0.5160, dev_err=34.38%\n",
      "2026-02-19 18:11:24,863 - lg_cotrain - INFO - Phase 2 epoch 6/10: loss1=0.0136, loss2=0.0558, dev_macro_f1=0.5608, dev_err=30.80%\n",
      "2026-02-19 18:11:40,606 - lg_cotrain - INFO - Phase 2 epoch 7/10: loss1=0.0096, loss2=0.0376, dev_macro_f1=0.6067, dev_err=31.25%\n",
      "2026-02-19 18:11:56,272 - lg_cotrain - INFO - Phase 2 epoch 8/10: loss1=0.0081, loss2=0.0298, dev_macro_f1=0.6351, dev_err=28.57%\n",
      "2026-02-19 18:12:11,942 - lg_cotrain - INFO - Phase 2 epoch 9/10: loss1=0.0073, loss2=0.0258, dev_macro_f1=0.6238, dev_err=29.91%\n",
      "2026-02-19 18:12:27,629 - lg_cotrain - INFO - Phase 2 epoch 10/10: loss1=0.0062, loss2=0.0301, dev_macro_f1=0.5889, dev_err=32.59%\n",
      "2026-02-19 18:12:27,629 - lg_cotrain - INFO - === Phase 3: Fine-Tuning ===\n",
      "2026-02-19 18:12:30,462 - lg_cotrain - INFO - Phase 3 epoch 1: dev_macro_f1=0.5958, dev_err=33.04%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 18:12:33,305 - lg_cotrain - INFO - Phase 3 epoch 2: dev_macro_f1=0.6463, dev_err=27.23%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 18:12:40,052 - lg_cotrain - INFO - Phase 3 epoch 3: dev_macro_f1=0.6543, dev_err=27.23%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 18:12:42,856 - lg_cotrain - INFO - Phase 3 epoch 4: dev_macro_f1=0.6823, dev_err=24.11%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 18:12:45,671 - lg_cotrain - INFO - Phase 3 epoch 5: dev_macro_f1=0.6644, dev_err=25.45%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 18:12:48,461 - lg_cotrain - INFO - Phase 3 epoch 6: dev_macro_f1=0.6751, dev_err=25.00%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 18:12:51,295 - lg_cotrain - INFO - Phase 3 epoch 7: dev_macro_f1=0.6924, dev_err=23.66%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 18:12:54,100 - lg_cotrain - INFO - Phase 3 epoch 8: dev_macro_f1=0.6796, dev_err=25.00%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 18:12:56,922 - lg_cotrain - INFO - Phase 3 epoch 9: dev_macro_f1=0.6876, dev_err=24.11%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 18:12:59,970 - lg_cotrain - INFO - Phase 3 epoch 10: dev_macro_f1=0.6929, dev_err=23.21%, es_counter1=0, es_counter2=0\n",
      "2026-02-19 18:13:02,970 - lg_cotrain - INFO - Phase 3 epoch 11: dev_macro_f1=0.6718, dev_err=25.45%, es_counter1=1, es_counter2=1\n",
      "2026-02-19 18:13:06,050 - lg_cotrain - INFO - Phase 3 epoch 12: dev_macro_f1=0.6666, dev_err=25.89%, es_counter1=2, es_counter2=2\n",
      "2026-02-19 18:13:09,048 - lg_cotrain - INFO - Phase 3 epoch 13: dev_macro_f1=0.6666, dev_err=25.89%, es_counter1=3, es_counter2=3\n",
      "2026-02-19 18:13:12,045 - lg_cotrain - INFO - Phase 3 epoch 14: dev_macro_f1=0.6699, dev_err=25.45%, es_counter1=4, es_counter2=4\n",
      "2026-02-19 18:13:15,037 - lg_cotrain - INFO - Phase 3 epoch 15: dev_macro_f1=0.6666, dev_err=25.89%, es_counter1=5, es_counter2=5\n",
      "2026-02-19 18:13:15,038 - lg_cotrain - INFO - Early stopping at epoch 15\n",
      "2026-02-19 18:13:15,047 - lg_cotrain - INFO - === Final Evaluation ===\n",
      "2026-02-19 18:13:17,386 - lg_cotrain - INFO - Results saved to D:\\Workspace\\Co-Training\\results\\gpt-4o-quick-stop-baseline\\kaikoura_earthquake_2016\\50_set1\\metrics.json\n",
      "2026-02-19 18:13:17,386 - lg_cotrain - INFO - Test error rate: 22.53%, Test macro-F1: 0.7705, Test ECE: 0.0976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] budget=50, seed=1 -- done (macro_f1=0.7705)\n",
      "  [PROGRESS] 9/60 (15.0%)  |  Elapsed: 1.76h  |  ETA: 9.95h  |  done\n",
      "\n",
      "Batch complete: 1 ran, 0 skipped, 0 failed (251.1s total)\n",
      "[1/1] budget=50, seed=1 -- starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 18:13:18,091 - lg_cotrain - INFO - Starting LG-CoTrain: event=kerala_floods_2018, budget=50, seed_set=1\n",
      "2026-02-19 18:13:18,129 - lg_cotrain - INFO - Detected 9 classes for event kerala_floods_2018: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
      "2026-02-19 18:13:18,141 - lg_cotrain - INFO - D_l1: 220, D_l2: 219, D_LG: 5149\n",
      "2026-02-19 18:13:18,144 - lg_cotrain - INFO - === Phase 1: Weight Generation ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1114.11it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1056.03it/s, Materializing param=bert.pooler.dense.weight]\n",
      "2026-02-19 18:13:40,911 - lg_cotrain - INFO - Phase 1 epoch 1/7: mean_prob1=0.1190, mean_prob2=0.1323\n",
      "2026-02-19 18:14:01,661 - lg_cotrain - INFO - Phase 1 epoch 2/7: mean_prob1=0.1234, mean_prob2=0.1444\n",
      "2026-02-19 18:14:21,465 - lg_cotrain - INFO - Phase 1 epoch 3/7: mean_prob1=0.1368, mean_prob2=0.1360\n",
      "2026-02-19 18:14:41,498 - lg_cotrain - INFO - Phase 1 epoch 4/7: mean_prob1=0.1518, mean_prob2=0.1419\n",
      "2026-02-19 18:15:01,323 - lg_cotrain - INFO - Phase 1 epoch 5/7: mean_prob1=0.1474, mean_prob2=0.1572\n",
      "2026-02-19 18:15:21,215 - lg_cotrain - INFO - Phase 1 epoch 6/7: mean_prob1=0.1721, mean_prob2=0.1695\n",
      "2026-02-19 18:15:41,017 - lg_cotrain - INFO - Phase 1 epoch 7/7: mean_prob1=0.1897, mean_prob2=0.1843\n",
      "2026-02-19 18:15:41,018 - lg_cotrain - INFO - Phase 1 done. lambda1: mean=0.1801, range=[0.0654, 0.3537]\n",
      "2026-02-19 18:15:41,019 - lg_cotrain - INFO - Phase 1 done. lambda2: mean=0.1231, range=[0.0208, 0.2788]\n",
      "2026-02-19 18:15:41,019 - lg_cotrain - INFO - === Phase 2: Co-Training ===\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1153.54it/s, Materializing param=bert.pooler.dense.weight]\n",
      "Loading weights: 100%|███████████████| 199/199 [00:00<00:00, 1078.80it/s, Materializing param=bert.pooler.dense.weight]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m all_strategy_results[strategy] = {}\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m TARGET_EVENTS:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     results = \u001b[43mrun_all_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbudgets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRUN_BUDGETS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRUN_SEEDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpseudo_label_source\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPSEUDO_LABEL_SOURCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDATA_ROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresults_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_on_experiment_done\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtracker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     all_strategy_results[strategy][event] = results\n\u001b[32m     45\u001b[39m strat_elapsed = time.time() - strat_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\lg_cotrain\\run_all.py:107\u001b[39m, in \u001b[36mrun_all_experiments\u001b[39m\u001b[34m(event, budgets, seed_sets, pseudo_label_source, model_name, weight_gen_epochs, cotrain_epochs, finetune_max_epochs, finetune_patience, stopping_strategy, batch_size, lr, max_seq_length, data_root, results_root, _trainer_cls, _on_experiment_done)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    106\u001b[39m     trainer = _trainer_cls(config)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     all_results.append(result)\n\u001b[32m    109\u001b[39m     completed += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\lg_cotrain\\trainer.py:247\u001b[39m, in \u001b[36mLGCoTrainer.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    242\u001b[39m w1 = torch.tensor(\n\u001b[32m    243\u001b[39m     lambda1[sample_idx], dtype=torch.float32, device=\u001b[38;5;28mself\u001b[39m.device\n\u001b[32m    244\u001b[39m )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Model 1 loss (uses theta2's weights = lambda2)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m logits1 = \u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m per_sample_loss1 = F.cross_entropy(logits1, labels, reduction=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    249\u001b[39m loss1 = (w2 * per_sample_loss1).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\lg_cotrain\\model.py:26\u001b[39m, in \u001b[36mBertClassifier.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, labels)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, labels=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     25\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass returning logits.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We compute loss ourselves for weighted CE\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs.logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\utils\\generic.py:841\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    840\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    843\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1139\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, **kwargs)\u001b[39m\n\u001b[32m   1121\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m   1122\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1131\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m   1132\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor] | SequenceClassifierOutput:\n\u001b[32m   1133\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1135\u001b[39m \u001b[33;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[33;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[33;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1149\u001b[39m     pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1151\u001b[39m     pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\utils\\generic.py:915\u001b[39m, in \u001b[36mmerge_with_config_defaults.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    913\u001b[39m             output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[38;5;66;03m# Restore original config value\u001b[39;00m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_causal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\utils\\output_capturing.py:253\u001b[39m, in \u001b[36mcapture_outputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Run the forward\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Reset the states\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    256\u001b[39m     _active_collector.reset(output_token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:697\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    681\u001b[39m     input_ids=input_ids,\n\u001b[32m    682\u001b[39m     position_ids=position_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    685\u001b[39m     past_key_values_length=past_key_values_length,\n\u001b[32m    686\u001b[39m )\n\u001b[32m    688\u001b[39m attention_mask, encoder_attention_mask = \u001b[38;5;28mself\u001b[39m._create_attention_masks(\n\u001b[32m    689\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    690\u001b[39m     encoder_attention_mask=encoder_attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    694\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    695\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m sequence_output = encoder_outputs.last_hidden_state\n\u001b[32m    709\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:452\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    441\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    442\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    450\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m         hidden_states = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    456\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPastAndCrossAttentions(\n\u001b[32m    463\u001b[39m         last_hidden_state=hidden_states,\n\u001b[32m    464\u001b[39m         past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    465\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning_once(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:397\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    396\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     self_attention_output, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m     attention_output = self_attention_output\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:334\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m attention_mask = attention_mask \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_cross_attention \u001b[38;5;28;01melse\u001b[39;00m encoder_attention_mask\n\u001b[32m    326\u001b[39m attention_output, attn_weights = \u001b[38;5;28mself\u001b[39m.self(\n\u001b[32m    327\u001b[39m     hidden_states,\n\u001b[32m    328\u001b[39m     encoder_hidden_states=encoder_hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    332\u001b[39m     **kwargs,\n\u001b[32m    333\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attention_output, attn_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:301\u001b[39m, in \u001b[36mBertSelfOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    303\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Workspace\\Co-Training\\co-training-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Count already-completed experiments across all strategies (for accurate ETA from the start)\n",
    "already_done = sum(\n",
    "    1\n",
    "    for strategy in STRATEGIES\n",
    "    for event in TARGET_EVENTS\n",
    "    for budget in RUN_BUDGETS\n",
    "    for seed_set in RUN_SEEDS\n",
    "    if (\n",
    "        Path(STRATEGY_RESULTS_ROOTS[strategy])\n",
    "        / event / f\"{budget}_set{seed_set}\" / \"metrics.json\"\n",
    "    ).exists()\n",
    ")\n",
    "total_experiments = len(STRATEGIES) * len(TARGET_EVENTS) * len(RUN_BUDGETS) * len(RUN_SEEDS)\n",
    "\n",
    "print(f\"Total experiments : {total_experiments}\")\n",
    "print(f\"Already completed : {already_done}\")\n",
    "print(f\"Remaining         : {total_experiments - already_done}\")\n",
    "print()\n",
    "\n",
    "overall_start = time.time()\n",
    "tracker = ProgressTracker(total_experiments, already_done, overall_start)\n",
    "all_strategy_results = {}  # strategy -> event -> list[result_dict]\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    results_root = STRATEGY_RESULTS_ROOTS[strategy]\n",
    "    strat_start = time.time()\n",
    "    print(f\"\\n{'=' * 65}\")\n",
    "    print(f\"Strategy: {strategy}\")\n",
    "    print(f\"{'=' * 65}\")\n",
    "    all_strategy_results[strategy] = {}\n",
    "\n",
    "    for event in TARGET_EVENTS:\n",
    "        results = run_all_experiments(\n",
    "            event,\n",
    "            budgets=RUN_BUDGETS,\n",
    "            seed_sets=RUN_SEEDS,\n",
    "            pseudo_label_source=PSEUDO_LABEL_SOURCE,\n",
    "            stopping_strategy=strategy,\n",
    "            data_root=DATA_ROOT,\n",
    "            results_root=results_root,\n",
    "            _on_experiment_done=tracker.update,\n",
    "        )\n",
    "        all_strategy_results[strategy][event] = results\n",
    "\n",
    "    strat_elapsed = time.time() - strat_start\n",
    "    print(f\"\\n  Strategy '{strategy}' done in {strat_elapsed / 3600:.2f}h ({strat_elapsed / 60:.1f}min)\")\n",
    "\n",
    "total_elapsed = time.time() - overall_start\n",
    "print(f\"\\n{'=' * 65}\")\n",
    "print(f\"All experiments complete in {total_elapsed / 3600:.2f}h ({total_elapsed / 60:.1f}min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load any results that already existed (re-run safe)\n",
    "for strategy in STRATEGIES:\n",
    "    results_root = Path(STRATEGY_RESULTS_ROOTS[strategy])\n",
    "    if strategy not in all_strategy_results:\n",
    "        all_strategy_results[strategy] = {}\n",
    "\n",
    "    for event in TARGET_EVENTS:\n",
    "        if event in all_strategy_results[strategy]:\n",
    "            continue\n",
    "        results = []\n",
    "        for budget in RUN_BUDGETS:\n",
    "            for seed_set in RUN_SEEDS:\n",
    "                path = results_root / event / f\"{budget}_set{seed_set}\" / \"metrics.json\"\n",
    "                if path.exists():\n",
    "                    with open(path) as f:\n",
    "                        results.append(json.load(f))\n",
    "        if results:\n",
    "            all_strategy_results[strategy][event] = results\n",
    "\n",
    "# Build flat lookup: lookup[strategy][event] -> result dict (or None)\n",
    "# Since budget=50, seed=1 only, each (strategy, event) has at most one result.\n",
    "lookup = {}\n",
    "for strategy in STRATEGIES:\n",
    "    lookup[strategy] = {}\n",
    "    for event in TARGET_EVENTS:\n",
    "        results = all_strategy_results.get(strategy, {}).get(event, [])\n",
    "        # Pick the single result for budget=50, seed=1\n",
    "        match = next(\n",
    "            (r for r in results if r and r.get(\"budget\") == 50 and r.get(\"seed_set\") == 1),\n",
    "            None,\n",
    "        )\n",
    "        lookup[strategy][event] = match\n",
    "\n",
    "print(\"Results available (budget=50, seed=1):\")\n",
    "print(f\"{'Strategy':<26}\" + \"\".join(f\" {e[:12]:<13}\" for e in TARGET_EVENTS))\n",
    "print(\"-\" * (26 + 14 * len(TARGET_EVENTS)))\n",
    "for strategy in STRATEGIES:\n",
    "    row = f\"{strategy:<26}\"\n",
    "    for event in TARGET_EVENTS:\n",
    "        row += \" OK          \" if lookup[strategy][event] else \" --          \"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table: strategies (rows) × events (columns), value = macro-F1\n",
    "# Plus a delta-from-baseline table.\n",
    "\n",
    "col_w = 10\n",
    "event_labels = [e.replace(\"_\", \" \") for e in TARGET_EVENTS]\n",
    "short_labels  = [\" \".join(w[:4] for w in e.split(\"_\")) for e in TARGET_EVENTS]\n",
    "\n",
    "print(\"Macro-F1  (budget=50, seed=1)\")\n",
    "print(f\"{'Strategy':<26}\" + \"\".join(f\" {sl:<{col_w}}\" for sl in short_labels) + \"  Mean\")\n",
    "print(\"-\" * (26 + (col_w + 1) * len(TARGET_EVENTS) + 6))\n",
    "\n",
    "baseline_row = {}\n",
    "for strategy in STRATEGIES:\n",
    "    row = f\"{strategy:<26}\"\n",
    "    vals = []\n",
    "    for event in TARGET_EVENTS:\n",
    "        r = lookup[strategy][event]\n",
    "        f1 = r[\"test_macro_f1\"] if r else None\n",
    "        vals.append(f1)\n",
    "        row += f\" {f1:.4f}   \" if f1 is not None else f\" {'N/A':<{col_w}}\"\n",
    "    valid = [v for v in vals if v is not None]\n",
    "    row += f\"  {sum(valid)/len(valid):.4f}\" if valid else \"  N/A\"\n",
    "    print(row)\n",
    "    if strategy == \"baseline\":\n",
    "        baseline_row = {e: v for e, v in zip(TARGET_EVENTS, vals)}\n",
    "\n",
    "print()\n",
    "print(\"Delta vs baseline  (+) = better:\")\n",
    "print(f\"{'Strategy':<26}\" + \"\".join(f\" {sl:<{col_w}}\" for sl in short_labels) + \"  Mean Δ\")\n",
    "print(\"-\" * (26 + (col_w + 1) * len(TARGET_EVENTS) + 8))\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    if strategy == \"baseline\":\n",
    "        continue\n",
    "    row = f\"{strategy:<26}\"\n",
    "    deltas = []\n",
    "    for event in TARGET_EVENTS:\n",
    "        r = lookup[strategy][event]\n",
    "        f1   = r[\"test_macro_f1\"] if r else None\n",
    "        base = baseline_row.get(event)\n",
    "        if f1 is not None and base is not None:\n",
    "            d = f1 - base\n",
    "            deltas.append(d)\n",
    "            sign = \"+\" if d >= 0 else \"\"\n",
    "            row += f\" {sign}{d:.4f}  \"\n",
    "        else:\n",
    "            row += f\" {'N/A':<{col_w}}\"\n",
    "    row += f\"  {'+' if sum(deltas)/len(deltas)>=0 else ''}{sum(deltas)/len(deltas):.4f}\" if deltas else \"  N/A\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart: one group per event, one bar per strategy\n",
    "# All at budget=50, seed=1\n",
    "\n",
    "n_events     = len(TARGET_EVENTS)\n",
    "n_strategies = len(STRATEGIES)\n",
    "bar_width    = 0.8 / n_strategies\n",
    "colors       = plt.cm.tab10(np.linspace(0, 1, n_strategies))\n",
    "x            = np.arange(n_events)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(14, n_events * 1.4), 5))\n",
    "\n",
    "for i, (strategy, color) in enumerate(zip(STRATEGIES, colors)):\n",
    "    f1s = [\n",
    "        lookup[strategy][event][\"test_macro_f1\"]\n",
    "        if lookup[strategy][event] else 0\n",
    "        for event in TARGET_EVENTS\n",
    "    ]\n",
    "    offset = (i - n_strategies / 2 + 0.5) * bar_width\n",
    "    ax.bar(x + offset, f1s, bar_width * 0.9, label=strategy, color=color, alpha=0.85)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(\n",
    "    [e.replace(\"_\", \"\\n\") for e in TARGET_EVENTS],\n",
    "    fontsize=8,\n",
    ")\n",
    "ax.set_ylabel(\"Test Macro-F1\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title(\n",
    "    f\"Stopping Strategy Comparison — Budget=50, Seed=1\\n(pseudo-labels: {PSEUDO_LABEL_SOURCE})\",\n",
    "    fontsize=11,\n",
    ")\n",
    "ax.legend(loc=\"upper right\", fontsize=8, framealpha=0.8)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class F1 heatmap for each event\n",
    "# Rows = strategies, Columns = classes, Value = F1 at budget=50, seed=1\n",
    "\n",
    "from lg_cotrain.data_loading import CLASS_LABELS\n",
    "\n",
    "for event in TARGET_EVENTS:\n",
    "    strategies_with_data = [\n",
    "        s for s in STRATEGIES\n",
    "        if lookup[s][event] and \"test_per_class_f1\" in lookup[s][event]\n",
    "    ]\n",
    "    if not strategies_with_data:\n",
    "        print(f\"No per-class data for {event}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    data = np.array([\n",
    "        lookup[s][event][\"test_per_class_f1\"]\n",
    "        for s in strategies_with_data\n",
    "    ])  # shape: (n_strategies, n_classes)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(max(9, len(CLASS_LABELS) * 0.75), len(strategies_with_data) * 0.65 + 1.8)\n",
    "    )\n",
    "    im = ax.imshow(data, cmap=\"RdYlGn\", aspect=\"auto\", vmin=0, vmax=1)\n",
    "\n",
    "    ax.set_xticks(range(len(CLASS_LABELS)))\n",
    "    ax.set_xticklabels(CLASS_LABELS, rotation=45, ha=\"right\", fontsize=8)\n",
    "    ax.set_yticks(range(len(strategies_with_data)))\n",
    "    ax.set_yticklabels(strategies_with_data, fontsize=9)\n",
    "    ax.set_title(\n",
    "        f\"{event}  |  Budget=50, Seed=1  |  Per-class F1 by strategy\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "    for i in range(len(strategies_with_data)):\n",
    "        for j in range(len(CLASS_LABELS)):\n",
    "            val = data[i, j]\n",
    "            color = \"black\" if 0.25 < val < 0.75 else \"white\"\n",
    "            ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", fontsize=7, color=color)\n",
    "\n",
    "    fig.colorbar(im, ax=ax, label=\"F1 Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild multi-tab dashboard so all quick-run result sets appear as tabs.\n",
    "\n",
    "from lg_cotrain.dashboard import discover_result_sets, generate_html_multi\n",
    "\n",
    "TOP_RESULTS_ROOT = str(repo_root / \"results\")\n",
    "\n",
    "result_sets = discover_result_sets(TOP_RESULTS_ROOT)\n",
    "print(f\"Discovered {len(result_sets)} result set(s):\")\n",
    "for name, path in result_sets.items():\n",
    "    print(f\"  {name:<35} → {path}\")\n",
    "\n",
    "html = generate_html_multi(result_sets, data_root=DATA_ROOT)\n",
    "dashboard_path = Path(TOP_RESULTS_ROOT) / \"dashboard.html\"\n",
    "dashboard_path.write_text(html)\n",
    "print(f\"\\nDashboard written to: {dashboard_path}\")\n",
    "print(\"Open in a browser to compare strategies side-by-side.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
